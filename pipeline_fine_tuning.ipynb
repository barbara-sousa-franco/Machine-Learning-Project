{
 "cells": [
  {
   "cell_type": "code",
   "id": "334eb522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:05.349409Z",
     "start_time": "2025-12-07T17:41:05.293886Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from math import ceil\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, median_absolute_error, root_mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from functions_MARISA import *\n",
    "from Classes import Categorical_Correction, Outlier_Treatment, Missing_Value_Treatment, Typecasting, Feature_Engineering, Encoder, Scaler, Feature_Selection"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "c5d1ecd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:07.245163Z",
     "start_time": "2025-12-07T17:41:07.101258Z"
    }
   },
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "665d8ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:08.938950Z",
     "start_time": "2025-12-07T17:41:08.933756Z"
    }
   },
   "source": [
    "df_train.set_index('carID', inplace=True)\n",
    "df_test.set_index('carID', inplace=True)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "2441abb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:15.716543Z",
     "start_time": "2025-12-07T17:41:15.712018Z"
    }
   },
   "source": [
    "random_state = 42"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "88d69aa7",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9ac63",
   "metadata": {},
   "source": [
    "We start by defining the inconsistent values discussed in the EDA as NA:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca0536e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:17.080814Z",
     "start_time": "2025-12-07T17:41:17.065905Z"
    }
   },
   "source": [
    "df_train.loc[df_train['year']>2020, 'year'] = np.nan\n",
    "df_test.loc[df_test['year']>2020, 'year'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mileage']<0, 'mileage'] = np.nan\n",
    "df_test.loc[df_test['mileage']<0, 'mileage'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['tax']<0, 'tax'] = np.nan\n",
    "df_test.loc[df_test['tax']<0,'tax'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mpg']<=0, 'mpg'] = np.nan\n",
    "df_test.loc[df_test['mpg']<=0, 'mpg'] = np.nan\n",
    "\n",
    "\n",
    "df_train.loc[df_train['previousOwners']< 0, 'previousOwners'] = np.nan\n",
    "df_test.loc[df_test['previousOwners']< 0, 'previousOwners'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['engineSize']<= 0, 'engineSize'] = np.nan\n",
    "df_test.loc[df_test['engineSize']<= 0, 'engineSize'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mpg'] < 40, 'mpg'] = np.nan\n",
    "df_test.loc[df_test['mpg'] < 40, 'mpg'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['engineSize'] < 1, 'engineSize'] = np.nan\n",
    "df_test.loc[df_test['engineSize'] < 1, 'engineSize'] = np.nan"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "ec345109",
   "metadata": {},
   "source": [
    "We proceed to round 'year' and 'previousOwners' to whole numbers using the floor function. Other numerical features are rounded to 2 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "id": "80085750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:19.769571Z",
     "start_time": "2025-12-07T17:41:19.758487Z"
    }
   },
   "source": [
    "df_train['year'] = np.floor(df_train['year'])\n",
    "df_train['previousOwners'] = np.floor(df_train['previousOwners'])\n",
    "\n",
    "df_test['year'] = np.floor(df_test['year'])\n",
    "df_test['previousOwners'] = np.floor(df_test['previousOwners'])\n",
    "\n",
    "for feat in ['mileage', 'tax', 'mpg', 'engineSize']:\n",
    "    df_train[feat] = df_train[feat].round(2)\n",
    "    df_test[feat] = df_test[feat].round(2)"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "52bf0b21",
   "metadata": {},
   "source": [
    "We also pre-process the categorical variables in order to have a uniform format for later treatment (inside k-fold CV). We remove leeading and trailing spaces and uppercase all letters."
   ]
  },
  {
   "cell_type": "code",
   "id": "454801c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:21.895291Z",
     "start_time": "2025-12-07T17:41:21.790698Z"
    }
   },
   "source": [
    "# Pre processing the categorical variables to be easier to find clusters in typos:\n",
    "    # remove spaces (at the beginning and end) and uppercase all letters\n",
    "    # does not replace NaN's\n",
    "df_train['Brand'] = df_train['Brand'].where(df_train['Brand'].isna(), df_train['Brand'].astype(str).str.strip().str.upper())\n",
    "df_test['Brand']  = df_test['Brand'].where(df_test['Brand'].isna(), df_test['Brand'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['model'] = df_train['model'].where(df_train['model'].isna(), df_train['model'].astype(str).str.strip().str.upper())\n",
    "df_test['model']  = df_test['model'].where(df_test['model'].isna(), df_test['model'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['fuelType'] = df_train['fuelType'].where(df_train['fuelType'].isna(), df_train['fuelType'].astype(str).str.strip().str.upper())\n",
    "df_test['fuelType']  = df_test['fuelType'].where(df_test['fuelType'].isna(), df_test['fuelType'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['transmission'] = df_train['transmission'].where(df_train['transmission'].isna(), df_train['transmission'].astype(str).str.strip().str.upper())\n",
    "df_test['transmission']  = df_test['transmission'].where(df_test['transmission'].isna(), df_test['transmission'].astype(str).str.strip().str.upper())"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "1da0ba87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:25.531815Z",
     "start_time": "2025-12-07T17:41:25.527032Z"
    }
   },
   "source": [
    "(df_train[\"price\"] == 0).any()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "a3225b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:27.283377Z",
     "start_time": "2025-12-07T17:41:27.269568Z"
    }
   },
   "source": [
    "y = df_train['price']\n",
    "X = df_train.drop('price', axis=1)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "69f9c951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:28.943746Z",
     "start_time": "2025-12-07T17:41:28.930249Z"
    }
   },
   "source": [
    "X.drop('paintQuality%', axis=1, inplace=True)\n",
    "df_test.drop('paintQuality%', axis=1, inplace=True)"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "10af4260",
   "metadata": {},
   "source": [
    "Notas sobre as classes:\n",
    "\n",
    "- variáveis criadas na inicialização não acabam em _; as ue são criadas dentro dos métodos acabam em _!\n",
    "- criando uma var nos métodos, se ela não começar em self. não será reconhecida por toda a classe, será apenas local!\n",
    "- logo, iniciar com self. para criar novos atributos gerais (assim transform() cconsegue aceder ao atributo criado em fit() por exemplo)\n",
    "- cuidado com data leakage! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543515b8",
   "metadata": {},
   "source": [
    "### Categorical_Correction Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732ce2e",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "id": "8fdfa392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:34.606521Z",
     "start_time": "2025-12-07T17:41:34.597468Z"
    }
   },
   "source": [
    "pipeline_huber = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), #NO NEED FOR RANDOM FOREST REG\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', HuberRegressor())\n",
    "])\n",
    "\n",
    "pipeline_huber_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), \n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=HuberRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), \n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "pipeline_mlp_robust = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(RobustScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_robust_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(RobustScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax2 = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler( feature_range=(-1,1)))),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax2_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler( feature_range=(-1,1)))),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "5e820757",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "id": "ada50733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:38.206163Z",
     "start_time": "2025-12-07T17:41:38.200439Z"
    }
   },
   "source": [
    "# Making an adjusted R2 function:\n",
    "\n",
    "def adjusted_r2_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    n, p = X.shape\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1) # erros\n",
    "\n",
    "adj_r2 = make_scorer(adjusted_r2_scorer, greater_is_better=True) #erros\n",
    "\n",
    "\n",
    "scoring = { 'R2': 'r2', #'AdjR2': adj_r2 -> esta função é difícil de implementar, erros\n",
    "    'MAE': 'neg_mean_absolute_error',\n",
    "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "    'MedAE': 'neg_median_absolute_error',\n",
    "    'RMSE': 'neg_root_mean_squared_error'}\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "2e2b52ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:43.099792Z",
     "start_time": "2025-12-07T17:41:43.090213Z"
    }
   },
   "source": [
    "param_distributions_huber = {\n",
    "    'regressor__epsilon': [1.1, 1.2, 1.35, 1.5, 2.0, 2.5, 3.0],\n",
    "    'regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0],\n",
    "    'regressor__fit_intercept': [True],\n",
    "    'regressor__max_iter': [500, 1000, 2000]\n",
    "\n",
    "}\n",
    "\n",
    "param_distributions_huber_logprice = {\n",
    "    'regressor__regressor__epsilon': [1.1, 1.2, 1.35, 1.5, 2.0, 2.5, 3.0],\n",
    "    'regressor__regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0],\n",
    "    'regressor__regressor__fit_intercept': [True],\n",
    "    'regressor__regressor__max_iter': [500, 1000, 2000]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "param_distributions_mlp_adam = {\n",
    "    'regressor__regressor__solver' : ['adam'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16), (200, 100), (600, 200), (200,100,50), (100,50,25)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.001, 0.01, 0.1]\n",
    "    \n",
    "}\n",
    "\n",
    "param_distributions_mlp_sgd = { \n",
    "    'regressor__regressor__solver' : ['sgd'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16), (200,100), (100,50,25)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate' :  ['constant','invscaling','adaptive'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.01, 0.001, 0.0001, 0.00001],\n",
    "    'regressor__regressor__batch_size' : [100, 200, 500],\n",
    "    'regressor__regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "param_distributions_mlp_adam_logprice = {\n",
    "    'regressor__regressor__solver' : ['adam'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16), (200, 100), (600, 200), (300,200,100), (200,100,50), (100,50,25)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.001, 0.01, 0.1]\n",
    "    \n",
    "}\n",
    "\n",
    "param_distributions_mlp_sgd_logprice = { \n",
    "    'regressor__regressor__solver' : ['sgd'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16), (200,100), (100,50,25)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate' :  ['constant','invscaling','adaptive'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'regressor__regressor__batch_size' : [100, 200, 500],\n",
    "    'regressor__regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def random_search (pipeline, param_distributions, n_iter=10) :\n",
    "    return RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,             # 10 random combinations of parameters -> reduced to 2 just for testing\n",
    "        scoring=scoring, # evaluation metrics\n",
    "        refit = 'MAE',\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=random_state),                 # 10-fold CV -> mudei para 5 \n",
    "        verbose=3, # to show iterations\n",
    "        return_train_score=True, # to return train metric results in cv_scores_\n",
    "        random_state=random_state, # defined on top of the nb\n",
    "        n_jobs=-1 \n",
    "    )\n",
    "\n",
    "rs_huber = random_search(\n",
    "    pipeline=pipeline_huber,\n",
    "    param_distributions=param_distributions_huber)\n",
    "\n",
    "rs_huber_logprice = random_search(\n",
    "    pipeline=pipeline_huber_logprice,\n",
    "    param_distributions=param_distributions_huber_logprice)\n",
    "\n",
    "rs_mlp_adam = random_search(pipeline = pipeline_mlp, param_distributions=param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_logprice = random_search(pipeline = pipeline_mlp_logprice, param_distributions=param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "rs_mlp_adam_robust = random_search(pipeline = pipeline_mlp_robust, param_distributions=param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_robust_logprice = random_search(pipeline = pipeline_mlp_robust_logprice, param_distributions=param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "rs_mlp_adam_minmax = random_search(pipeline = pipeline_mlp_minmax, param_distributions=param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_minmax_logprice = random_search(pipeline = pipeline_mlp_minmax_logprice, param_distributions=param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "rs_mlp_adam_minmax2 = random_search(pipeline = pipeline_mlp_minmax2, param_distributions = param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_minmax2_logprice = random_search(pipeline = pipeline_mlp_minmax2_logprice, param_distributions = param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "\n",
    "rs_mlp_sgd = random_search(pipeline = pipeline_mlp, param_distributions=param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_logprice = random_search(pipeline = pipeline_mlp_logprice, param_distributions=param_distributions_mlp_sgd_logprice, n_iter=30)\n",
    "rs_mlp_sgd_robust = random_search(pipeline = pipeline_mlp_robust, param_distributions=param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_robust_logprice = random_search(pipeline = pipeline_mlp_robust_logprice, param_distributions=param_distributions_mlp_sgd_logprice, n_iter=30)\n",
    "rs_mlp_sgd_minmax = random_search(pipeline = pipeline_mlp_minmax, param_distributions=param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_minmax_logprice = random_search(pipeline = pipeline_mlp_minmax_logprice, param_distributions=param_distributions_mlp_sgd_logprice, n_iter=30)\n",
    "rs_mlp_sgd_minmax2 = random_search(pipeline = pipeline_mlp_minmax2, param_distributions = param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_minmax2_logprice = random_search(pipeline = pipeline_mlp_minmax2_logprice, param_distributions = param_distributions_mlp_sgd_logprice, n_iter=30)\n"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "d1055e41",
   "metadata": {},
   "source": [
    "### Running RnadomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "id": "93c57346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T00:54:43.787256Z",
     "start_time": "2025-12-07T00:54:43.782647Z"
    }
   },
   "source": [
    "'''\n",
    "print(\"Running RandomizedSearchCV with Pipeline with Huber...\")\n",
    "rs_huber_ss.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_huber_ss.best_params_}\")\n",
    "print(f\"Best CV score: {rs_huber_ss.best_score_:.4f}\")\n",
    "idx = rs_huber_ss.best_index_\n",
    "train_mae = rs_huber_ss.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_huber_ss.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_huber_ss.best_estimator_.named_steps[\"regressor\"].n_iter_}')\n",
    "\n",
    "\n",
    "print(\"Running RandomizedSearchCV with Pipeline with Huber with Log Price...\")\n",
    "rs_huber_ss.fit(X, log_y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_huber_ss.best_params_}\")\n",
    "print(f\"Best CV score: {rs_huber_ss.best_score_:.4f}\")\n",
    "idx = rs_huber_ss.best_index_\n",
    "train_mae = rs_huber_ss.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_huber_ss.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_huber_ss.best_estimator_.named_steps[\"regressor\"].n_iter_}')\n",
    "\n",
    "\n",
    "random_searches = [rs_mlp_adam, rs_mlp_sgd, rs_mlp_adam_with_outliers, rs_mlp_sgd_with_outliers]\n",
    "\n",
    "for rs in random_searches:\n",
    "    print(\"Running RandomizedSearchCV with Pipeline with MLP...\")\n",
    "    rs.fit(X, y)\n",
    "\n",
    "    print(\"\\nRandomizedSearchCV Results:\")\n",
    "    print(f\"Best parameters: {rs.best_params_}\")\n",
    "    print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "    idx = rs.best_index_\n",
    "    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\n",
    "    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "    print(f' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')\n",
    "\n",
    "'''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Running RandomizedSearchCV with Pipeline with Huber...\")\\nrs_huber_ss.fit(X, y)\\n\\nprint(\"\\nRandomizedSearchCV Results:\")\\nprint(f\"Best parameters: {rs_huber_ss.best_params_}\")\\nprint(f\"Best CV score: {rs_huber_ss.best_score_:.4f}\")\\nidx = rs_huber_ss.best_index_\\ntrain_mae = rs_huber_ss.cv_results_[\"mean_train_MAE\"][idx]\\noverfit = (rs_huber_ss.best_score_ - train_mae) / abs(train_mae) * 100\\nprint(f\"Overfit: {overfit:.2f}%\\n\\n\")\\nprint(f\\' Number of iterations until convergence : {rs_huber_ss.best_estimator_.named_steps[\"regressor\"].n_iter_}\\')\\n\\n\\nprint(\"Running RandomizedSearchCV with Pipeline with Huber with Log Price...\")\\nrs_huber_ss.fit(X, log_y)\\n\\nprint(\"\\nRandomizedSearchCV Results:\")\\nprint(f\"Best parameters: {rs_huber_ss.best_params_}\")\\nprint(f\"Best CV score: {rs_huber_ss.best_score_:.4f}\")\\nidx = rs_huber_ss.best_index_\\ntrain_mae = rs_huber_ss.cv_results_[\"mean_train_MAE\"][idx]\\noverfit = (rs_huber_ss.best_score_ - train_mae) / abs(train_mae) * 100\\nprint(f\"Overfit: {overfit:.2f}%\\n\\n\")\\nprint(f\\' Number of iterations until convergence : {rs_huber_ss.best_estimator_.named_steps[\"regressor\"].n_iter_}\\')\\n\\n\\nrandom_searches = [rs_mlp_adam, rs_mlp_sgd, rs_mlp_adam_with_outliers, rs_mlp_sgd_with_outliers]\\n\\nfor rs in random_searches:\\n    print(\"Running RandomizedSearchCV with Pipeline with MLP...\")\\n    rs.fit(X, y)\\n\\n    print(\"\\nRandomizedSearchCV Results:\")\\n    print(f\"Best parameters: {rs.best_params_}\")\\n    print(f\"Best CV score: {rs.best_score_:.4f}\")\\n    idx = rs.best_index_\\n    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\\n    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\\n    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\\n    print(f\\' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}\\')\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "029b2875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T00:54:43.808665Z",
     "start_time": "2025-12-07T00:54:43.804562Z"
    }
   },
   "source": [
    "'''\n",
    "for rs in [rs_mlp_adam_logprice, rs_mlp_sgd_logprice]:\n",
    "    print(\"Running RandomizedSearchCV with Pipeline with MLP and Log Price...\")\n",
    "    rs.fit(X, y)\n",
    "\n",
    "    print(\"\\nRandomizedSearchCV Results:\")\n",
    "    print(f\"Best parameters: {rs.best_params_}\")\n",
    "    print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "    idx = rs.best_index_\n",
    "    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\n",
    "    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "    print(f' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')\n",
    "\n",
    "'''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor rs in [rs_mlp_adam_logprice, rs_mlp_sgd_logprice]:\\n    print(\"Running RandomizedSearchCV with Pipeline with MLP and Log Price...\")\\n    rs.fit(X, y)\\n\\n    print(\"\\nRandomizedSearchCV Results:\")\\n    print(f\"Best parameters: {rs.best_params_}\")\\n    print(f\"Best CV score: {rs.best_score_:.4f}\")\\n    idx = rs.best_index_\\n    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\\n    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\\n    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\\n    print(f\\' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}\\')\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "15fb6eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T00:54:43.829831Z",
     "start_time": "2025-12-07T00:54:43.825592Z"
    }
   },
   "source": [
    "'''\n",
    "for rs in [rs_mlp_adam, rs_mlp_sgd] :\n",
    "    print(\"Running RandomizedSearchCV with Pipeline with MLP...\")\n",
    "    rs.fit(X, y)\n",
    "\n",
    "    print(\"\\nRandomizedSearchCV Results:\")\n",
    "    print(f\"Best parameters: {rs.best_params_}\")\n",
    "    print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "    idx = rs.best_index_\n",
    "    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\n",
    "    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "    print(f' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')\n",
    "'''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor rs in [rs_mlp_adam, rs_mlp_sgd] :\\n    print(\"Running RandomizedSearchCV with Pipeline with MLP...\")\\n    rs.fit(X, y)\\n\\n    print(\"\\nRandomizedSearchCV Results:\")\\n    print(f\"Best parameters: {rs.best_params_}\")\\n    print(f\"Best CV score: {rs.best_score_:.4f}\")\\n    idx = rs.best_index_\\n    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\\n    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\\n    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\\n    print(f\\' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}\\')\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T06:42:12.295332Z",
     "start_time": "2025-12-07T00:54:43.847709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for rs in [rs_mlp_adam, rs_mlp_sgd, rs_mlp_adam_robust, rs_mlp_sgd_robust, rs_mlp_adam_minmax, rs_mlp_sgd_minmax, rs_mlp_adam_minmax2, rs_mlp_sgd_minmax2]:\n",
    "    print(\"Running RandomizedSearchCV with Pipeline with MLP...\")\n",
    "    rs.fit(X, y)\n",
    "\n",
    "    print(\"\\nRandomizedSearchCV Results:\")\n",
    "    print(f\"Best parameters: {rs.best_params_}\")\n",
    "    print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "    idx = rs.best_index_\n",
    "    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\n",
    "    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "    print(f' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ],
   "id": "a21c8d5b7814d95f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (600, 200), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1615.5530\n",
      "Overfit: -12.47%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 144\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1770.2732\n",
      "Overfit: -2.20%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 163\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1637.6231\n",
      "Overfit: -10.51%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 151\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1790.5498\n",
      "Overfit: -1.85%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 165\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1652.2762\n",
      "Overfit: -7.39%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 121\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1649.9947\n",
      "Overfit: -6.85%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 150\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1639.3994\n",
      "Overfit: -8.28%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 61\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1822.7295\n",
      "Overfit: -1.41%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 195\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T09:40:11.011888Z",
     "start_time": "2025-12-07T06:42:12.361400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for rs in [rs_mlp_adam_logprice, rs_mlp_sgd_logprice, rs_mlp_adam_robust_logprice, rs_mlp_sgd_robust_logprice, rs_mlp_adam_minmax_logprice, rs_mlp_sgd_minmax_logprice, rs_mlp_adam_minmax2_logprice, rs_mlp_sgd_minmax2_logprice]:\n",
    "    print(\"Running RandomizedSearchCV with Pipeline with MLP and Log Price...\")\n",
    "    rs.fit(X, y)\n",
    "\n",
    "    print(\"\\nRandomizedSearchCV Results:\")\n",
    "    print(f\"Best parameters: {rs.best_params_}\")\n",
    "    print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "    idx = rs.best_index_\n",
    "    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\n",
    "    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "    print(f' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')\n"
   ],
   "id": "7b011a186fe5dc0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1692.3330\n",
      "Overfit: -3.02%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 68\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 0.001, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1862.1595\n",
      "Overfit: -0.59%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 90\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1724.9555\n",
      "Overfit: -1.77%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 67\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.1, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (200, 100), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1820.8326\n",
      "Overfit: -1.49%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 113\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1771.9289\n",
      "Overfit: -0.96%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 57\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 0.001, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1985.0701\n",
      "Overfit: -0.41%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 101\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1741.4855\n",
      "Overfit: -1.83%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 60\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 0.001, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1965.8694\n",
      "Overfit: -1.07%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 89\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Melhores parametros para adam sem log price:  (200,100,50), 0.001, relu\n",
    "\n",
    "Excecao Standarscaler() que é com (600,200) mas dá overfit -> ver se ele testou a combinacao acima\n",
    "\n",
    "Melhores parametros para adam com log price: (200,100,50), 0.001, tanh . Sempre sem overfit\n",
    "\n",
    "\n",
    "\n",
    "Melhores parametros para sgd sem log price : (32,16), LR = 0.01, relu, 200, 1e-06 . Nao dá overfit\n",
    "\n",
    "Melhores parametros para sgd com log price : (32,16), LR = 0.01, adaptive, 100, alpha = 0.001, relu . Nao dá overfit.\n",
    "excecao minmax (200,100), LR = 0.1, adaptive,  200, alpha = 1e-06, relu\n",
    "\n"
   ],
   "id": "259aded33650c85f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:21:31.257897Z",
     "start_time": "2025-12-07T17:08:22.620388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_distributions_mlp_adam_logprice = {\n",
    "    'regressor__regressor__solver' : ['adam'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(600,300,150)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['tanh'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.001]}\n",
    "\n",
    "rs_teste = random_search(pipeline = pipeline_mlp_logprice, param_distributions= param_distributions_mlp_adam_logprice, n_iter=1)\n",
    "\n",
    "print(\"Running RandomizedSearchCV with Pipeline with MLP and Log Price...\")\n",
    "rs_teste.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_teste.best_params_}\")\n",
    "print(f\"Best CV score: {rs_teste.best_score_:.4f}\")\n",
    "idx = rs_teste.best_index_\n",
    "train_mae = rs_teste.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_teste.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_teste.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')\n"
   ],
   "id": "efc64ab1a3c8599f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (600, 300, 150), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1592.2060\n",
      "Overfit: -7.54%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 67\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:22:27.796736Z",
     "start_time": "2025-12-07T17:22:27.751721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rs_teste.best_estimator_, \"modelo_mlp_teste.pkl\")"
   ],
   "id": "49c915c0b1112c48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelo_mlp_teste.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T18:54:08.289700Z",
     "start_time": "2025-12-07T18:54:08.281413Z"
    }
   },
   "cell_type": "code",
   "source": "X.columns",
   "id": "f94969d629f4e99a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Brand', 'model', 'year', 'transmission', 'mileage', 'fuelType', 'tax',\n",
       "       'mpg', 'engineSize', 'previousOwners', 'hasDamage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "47dfca5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T11:40:51.306111Z",
     "start_time": "2025-12-07T11:36:31.120155Z"
    }
   },
   "source": [
    "print(\"Running RandomizedSearchCV with Pipeline with Huber...\")\n",
    "rs_huber.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_huber.best_params_}\")\n",
    "print(f\"Best CV score: {rs_huber.best_score_:.4f}\")\n",
    "idx = rs_huber.best_index_\n",
    "train_mae = rs_huber.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_huber.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_huber.best_estimator_.named_steps[\"regressor\"].n_iter_}')\n",
    "\n",
    "\n",
    "print(\"Running RandomizedSearchCV with Pipeline with Huber with Log Price...\")\n",
    "rs_huber_logprice.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_huber_logprice.best_params_}\")\n",
    "print(f\"Best CV score: {rs_huber_logprice.best_score_:.4f}\")\n",
    "idx = rs_huber_logprice.best_index_\n",
    "train_mae = rs_huber_logprice.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_huber_logprice.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_huber_logprice.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with Huber...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__max_iter': 500, 'regressor__fit_intercept': True, 'regressor__epsilon': 1.1, 'regressor__alpha': 0.1}\n",
      "Best CV score: -2532.3889\n",
      "Overfit: -0.24%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 57\n",
      "Running RandomizedSearchCV with Pipeline with Huber with Log Price...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__max_iter': 500, 'regressor__fit_intercept': True, 'regressor__epsilon': 1.1, 'regressor__alpha': 0.1}\n",
      "Best CV score: -2532.3889\n",
      "Overfit: -0.24%\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HuberRegressor' object has no attribute 'regressor_'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     22\u001B[39m overfit = (rs_huber.best_score_ - train_mae) / \u001B[38;5;28mabs\u001B[39m(train_mae) * \u001B[32m100\u001B[39m\n\u001B[32m     23\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mOverfit: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moverfit\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m%\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m Number of iterations until convergence : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mrs_huber\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbest_estimator_\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnamed_steps\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mregressor\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mregressor_\u001B[49m.n_iter_\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'HuberRegressor' object has no attribute 'regressor_'"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f094e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': ['year',\n",
       "  'mileage',\n",
       "  'tax',\n",
       "  'engineSize',\n",
       "  'carAge',\n",
       "  'AvgUsage',\n",
       "  'carSegment',\n",
       "  'model_cleaned_encoded',\n",
       "  'Brand_cleaned_encoded',\n",
       "  'fuelType_cleaned_DIESEL',\n",
       "  'fuelType_cleaned_PETROL',\n",
       "  'transmission_cleaned_AUTOMATIC',\n",
       "  'transmission_cleaned_MANUAL'],\n",
       " 'mae': 2623.6985387785553}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline = rs_mlp_adam.best_estimator_\n",
    "selector = best_pipeline.named_steps['feature selection']\n",
    "\n",
    "# Máscara de features selecionadas pelo fit no treino completo\n",
    "selected_mask = selector.best_\n",
    "selected_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb663b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 1000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <td>2551.6071</td>\n",
       "      <td>2530.1854</td>\n",
       "      <td>2541.2493</td>\n",
       "      <td>2528.0639</td>\n",
       "      <td>2568.8685</td>\n",
       "      <td>2930.3284</td>\n",
       "      <td>2921.043</td>\n",
       "      <td>2530.1849</td>\n",
       "      <td>2527.2113</td>\n",
       "      <td>2568.1785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <td>2556.4507</td>\n",
       "      <td>2535.5577</td>\n",
       "      <td>2546.6487</td>\n",
       "      <td>2533.3778</td>\n",
       "      <td>2574.0082</td>\n",
       "      <td>2933.3653</td>\n",
       "      <td>2924.4678</td>\n",
       "      <td>2535.5573</td>\n",
       "      <td>2532.4386</td>\n",
       "      <td>2574.1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_R2</th>\n",
       "      <td>0.789378</td>\n",
       "      <td>0.796883</td>\n",
       "      <td>0.800425</td>\n",
       "      <td>0.795421</td>\n",
       "      <td>0.7979</td>\n",
       "      <td>0.716265</td>\n",
       "      <td>0.725378</td>\n",
       "      <td>0.796882</td>\n",
       "      <td>0.792719</td>\n",
       "      <td>0.805038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_R2</th>\n",
       "      <td>0.788012</td>\n",
       "      <td>0.795319</td>\n",
       "      <td>0.798866</td>\n",
       "      <td>0.793867</td>\n",
       "      <td>0.796481</td>\n",
       "      <td>0.715462</td>\n",
       "      <td>0.72454</td>\n",
       "      <td>0.795318</td>\n",
       "      <td>0.791204</td>\n",
       "      <td>0.803422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAPE</th>\n",
       "      <td>0.1702</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>0.1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAPE</th>\n",
       "      <td>0.1702</td>\n",
       "      <td>0.1722</td>\n",
       "      <td>0.1757</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.1813</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.1722</td>\n",
       "      <td>0.1687</td>\n",
       "      <td>0.1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MedAE</th>\n",
       "      <td>1662.3472</td>\n",
       "      <td>1662.0865</td>\n",
       "      <td>1699.9093</td>\n",
       "      <td>1649.5007</td>\n",
       "      <td>1734.4398</td>\n",
       "      <td>1927.5141</td>\n",
       "      <td>1963.5399</td>\n",
       "      <td>1662.0101</td>\n",
       "      <td>1632.2679</td>\n",
       "      <td>1749.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MedAE</th>\n",
       "      <td>1664.4162</td>\n",
       "      <td>1661.8458</td>\n",
       "      <td>1699.8727</td>\n",
       "      <td>1650.2333</td>\n",
       "      <td>1737.7184</td>\n",
       "      <td>1930.4911</td>\n",
       "      <td>1967.0569</td>\n",
       "      <td>1661.8527</td>\n",
       "      <td>1632.2834</td>\n",
       "      <td>1752.9066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <td>4468.5152</td>\n",
       "      <td>4388.1669</td>\n",
       "      <td>4349.7284</td>\n",
       "      <td>4403.9355</td>\n",
       "      <td>4377.1677</td>\n",
       "      <td>5186.4866</td>\n",
       "      <td>5102.5168</td>\n",
       "      <td>4388.1715</td>\n",
       "      <td>4432.9214</td>\n",
       "      <td>4299.1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <td>4482.3739</td>\n",
       "      <td>4404.3162</td>\n",
       "      <td>4365.9043</td>\n",
       "      <td>4419.9336</td>\n",
       "      <td>4391.7751</td>\n",
       "      <td>5193.5176</td>\n",
       "      <td>5109.9795</td>\n",
       "      <td>4404.321</td>\n",
       "      <td>4448.4204</td>\n",
       "      <td>4315.9651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  0  ...                                                  9\n",
       "params            {'regressor__max_iter': 2000, 'regressor__fit_...  ...  {'regressor__max_iter': 500, 'regressor__fit_i...\n",
       "mean_train_MAE                                            2551.6071  ...                                          2568.1785\n",
       "mean_test_MAE                                             2556.4507  ...                                          2574.1165\n",
       "mean_train_R2                                              0.789378  ...                                           0.805038\n",
       "mean_test_R2                                               0.788012  ...                                           0.803422\n",
       "mean_train_MAPE                                              0.1702  ...                                             0.1814\n",
       "mean_test_MAPE                                               0.1702  ...                                             0.1813\n",
       "mean_train_MedAE                                          1662.3472  ...                                          1749.0065\n",
       "mean_test_MedAE                                           1664.4162  ...                                          1752.9066\n",
       "mean_train_RMSE                                           4468.5152  ...                                          4299.1549\n",
       "mean_test_RMSE                                            4482.3739  ...                                          4315.9651\n",
       "\n",
       "[11 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = {}\n",
    "df_metrics = {}\n",
    "\n",
    "for rs in random_searches:\n",
    "    results_df[f'results_{rs}'] = pd.DataFrame(rs.cv_results_)\n",
    "\n",
    "\n",
    "    metric_cols_train_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_R2\")]\n",
    "    metric_cols_test_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_R2\")]\n",
    "\n",
    "    metric_cols_train_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_MAE\")]\n",
    "    metric_cols_test_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_MAE\")]\n",
    "    metric_cols_train_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_MAPE\")]\n",
    "    metric_cols_test_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_MAPE\")]\n",
    "    metric_cols_train_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_MedAE\")]\n",
    "    metric_cols_test_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_MedAE\")]\n",
    "    metric_cols_train_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_RMSE\")]\n",
    "    metric_cols_test_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_RMSE\")]\n",
    "\n",
    "    std_cols_train_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_R2\")]\n",
    "    std_cols_test_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_R2\")]\n",
    "\n",
    "    std_cols_train_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_MAE\")]\n",
    "    std_cols_test_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_MAE\")]\n",
    "    std_cols_train_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_MAPE\")]\n",
    "    std_cols_test_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_MAPE\")]\n",
    "    std_cols_train_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_MedAE\")]\n",
    "    std_cols_test_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_MedAE\")]\n",
    "    std_cols_train_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_RMSE\")]\n",
    "    std_cols_test_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_RMSE\")]\n",
    "\n",
    "\n",
    "    df_metrics[f'{rs}'] = results_df[f'results_{rs}'][[\"params\"]+ metric_cols_train_MAE + metric_cols_test_MAE +\n",
    "                        metric_cols_train_R2 + metric_cols_test_R2 + \n",
    "                        metric_cols_train_MAPE + metric_cols_test_MAPE + \n",
    "                        metric_cols_train_MedAE + metric_cols_test_MedAE + \n",
    "                        metric_cols_train_RMSE + metric_cols_test_RMSE ]\n",
    "    df_metrics[f'{rs}'] = df_metrics[f'{rs}'].loc[:,['mean_train_MAE', 'mean_test_MAE',\n",
    "                        'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                        'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                        'mean_train_RMSE', 'mean_test_RMSE' ]] = df_metrics[f'{rs}'].loc[:,\n",
    "                                                                                          ['mean_train_MAE', 'mean_test_MAE',\n",
    "                        'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                        'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                        'mean_train_RMSE', 'mean_test_RMSE' ]] .round(4) * -1\n",
    "                        \n",
    "df_metrics[random_search_huber_standardscaler].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca59d90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <td>2079.6791</td>\n",
       "      <td>1963.3101</td>\n",
       "      <td>1964.0531</td>\n",
       "      <td>2149.1718</td>\n",
       "      <td>7794.1649</td>\n",
       "      <td>2364.6067</td>\n",
       "      <td>2136.3948</td>\n",
       "      <td>2273.3245</td>\n",
       "      <td>2222.3959</td>\n",
       "      <td>2277.5946</td>\n",
       "      <td>2230.2857</td>\n",
       "      <td>3039.7676</td>\n",
       "      <td>2369.0044</td>\n",
       "      <td>2869.7785</td>\n",
       "      <td>7007.7978</td>\n",
       "      <td>1737.1833</td>\n",
       "      <td>2411.2841</td>\n",
       "      <td>2685.4600</td>\n",
       "      <td>1775.2054</td>\n",
       "      <td>2458.9122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <td>2087.4618</td>\n",
       "      <td>1973.2623</td>\n",
       "      <td>1970.4834</td>\n",
       "      <td>2154.5462</td>\n",
       "      <td>7780.8247</td>\n",
       "      <td>2370.8310</td>\n",
       "      <td>2144.6654</td>\n",
       "      <td>2278.0469</td>\n",
       "      <td>2227.0507</td>\n",
       "      <td>2289.4033</td>\n",
       "      <td>2236.2657</td>\n",
       "      <td>3040.9516</td>\n",
       "      <td>2375.0590</td>\n",
       "      <td>2870.3143</td>\n",
       "      <td>7007.9053</td>\n",
       "      <td>1772.5296</td>\n",
       "      <td>2420.0104</td>\n",
       "      <td>2685.3512</td>\n",
       "      <td>1794.1628</td>\n",
       "      <td>2460.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAPE</th>\n",
       "      <td>0.1329</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.1376</td>\n",
       "      <td>0.6744</td>\n",
       "      <td>0.1569</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.2019</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>0.5594</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.1546</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAPE</th>\n",
       "      <td>0.1334</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.6739</td>\n",
       "      <td>0.1569</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1762</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.1634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MedAE</th>\n",
       "      <td>1408.1102</td>\n",
       "      <td>1295.1288</td>\n",
       "      <td>1300.6553</td>\n",
       "      <td>1458.2160</td>\n",
       "      <td>7081.9171</td>\n",
       "      <td>1623.4914</td>\n",
       "      <td>1446.8812</td>\n",
       "      <td>1597.3952</td>\n",
       "      <td>1501.2109</td>\n",
       "      <td>1606.1462</td>\n",
       "      <td>1553.3995</td>\n",
       "      <td>2182.1724</td>\n",
       "      <td>1657.3299</td>\n",
       "      <td>2034.7487</td>\n",
       "      <td>5879.2173</td>\n",
       "      <td>1124.1663</td>\n",
       "      <td>1693.1318</td>\n",
       "      <td>1889.7226</td>\n",
       "      <td>1153.2974</td>\n",
       "      <td>1762.9708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MedAE</th>\n",
       "      <td>1413.4873</td>\n",
       "      <td>1293.9959</td>\n",
       "      <td>1301.2434</td>\n",
       "      <td>1456.5420</td>\n",
       "      <td>7059.7745</td>\n",
       "      <td>1626.0657</td>\n",
       "      <td>1452.2277</td>\n",
       "      <td>1593.0016</td>\n",
       "      <td>1502.8436</td>\n",
       "      <td>1608.2543</td>\n",
       "      <td>1554.8086</td>\n",
       "      <td>2183.4487</td>\n",
       "      <td>1658.3161</td>\n",
       "      <td>2032.9938</td>\n",
       "      <td>5879.6406</td>\n",
       "      <td>1141.3689</td>\n",
       "      <td>1699.3119</td>\n",
       "      <td>1884.8615</td>\n",
       "      <td>1160.2152</td>\n",
       "      <td>1769.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <td>3568.5671</td>\n",
       "      <td>3407.0603</td>\n",
       "      <td>3403.1448</td>\n",
       "      <td>3648.1394</td>\n",
       "      <td>10174.9308</td>\n",
       "      <td>3973.4080</td>\n",
       "      <td>3663.6794</td>\n",
       "      <td>3830.7770</td>\n",
       "      <td>3822.0037</td>\n",
       "      <td>3735.6462</td>\n",
       "      <td>3777.8817</td>\n",
       "      <td>4982.2623</td>\n",
       "      <td>4008.4188</td>\n",
       "      <td>4829.9549</td>\n",
       "      <td>9717.7719</td>\n",
       "      <td>3019.7121</td>\n",
       "      <td>3986.6194</td>\n",
       "      <td>4536.1180</td>\n",
       "      <td>3077.8644</td>\n",
       "      <td>3944.9332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <td>3589.7293</td>\n",
       "      <td>3437.0938</td>\n",
       "      <td>3424.4586</td>\n",
       "      <td>3669.9601</td>\n",
       "      <td>10164.1989</td>\n",
       "      <td>3993.0938</td>\n",
       "      <td>3681.1151</td>\n",
       "      <td>3842.3295</td>\n",
       "      <td>3834.5201</td>\n",
       "      <td>3759.9846</td>\n",
       "      <td>3792.1205</td>\n",
       "      <td>4985.3482</td>\n",
       "      <td>4018.5090</td>\n",
       "      <td>4822.8579</td>\n",
       "      <td>9716.1640</td>\n",
       "      <td>3140.5348</td>\n",
       "      <td>4000.7682</td>\n",
       "      <td>4537.6439</td>\n",
       "      <td>3154.3823</td>\n",
       "      <td>3955.3592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0          1          2          3           4   \\\n",
       "mean_train_MAE    2079.6791  1963.3101  1964.0531  2149.1718   7794.1649   \n",
       "mean_test_MAE     2087.4618  1973.2623  1970.4834  2154.5462   7780.8247   \n",
       "mean_train_MAPE      0.1329     0.1217     0.1220     0.1376      0.6744   \n",
       "mean_test_MAPE       0.1334     0.1223     0.1224     0.1379      0.6739   \n",
       "mean_train_MedAE  1408.1102  1295.1288  1300.6553  1458.2160   7081.9171   \n",
       "mean_test_MedAE   1413.4873  1293.9959  1301.2434  1456.5420   7059.7745   \n",
       "mean_train_RMSE   3568.5671  3407.0603  3403.1448  3648.1394  10174.9308   \n",
       "mean_test_RMSE    3589.7293  3437.0938  3424.4586  3669.9601  10164.1989   \n",
       "\n",
       "                         5          6          7          8          9   \\\n",
       "mean_train_MAE    2364.6067  2136.3948  2273.3245  2222.3959  2277.5946   \n",
       "mean_test_MAE     2370.8310  2144.6654  2278.0469  2227.0507  2289.4033   \n",
       "mean_train_MAPE      0.1569     0.1372     0.1480     0.1411     0.1495   \n",
       "mean_test_MAPE       0.1569     0.1378     0.1483     0.1413     0.1505   \n",
       "mean_train_MedAE  1623.4914  1446.8812  1597.3952  1501.2109  1606.1462   \n",
       "mean_test_MedAE   1626.0657  1452.2277  1593.0016  1502.8436  1608.2543   \n",
       "mean_train_RMSE   3973.4080  3663.6794  3830.7770  3822.0037  3735.6462   \n",
       "mean_test_RMSE    3993.0938  3681.1151  3842.3295  3834.5201  3759.9846   \n",
       "\n",
       "                         10         11         12         13         14  \\\n",
       "mean_train_MAE    2230.2857  3039.7676  2369.0044  2869.7785  7007.7978   \n",
       "mean_test_MAE     2236.2657  3040.9516  2375.0590  2870.3143  7007.9053   \n",
       "mean_train_MAPE      0.1448     0.2019     0.1551     0.1871     0.5594   \n",
       "mean_test_MAPE       0.1452     0.2017     0.1551     0.1875     0.5595   \n",
       "mean_train_MedAE  1553.3995  2182.1724  1657.3299  2034.7487  5879.2173   \n",
       "mean_test_MedAE   1554.8086  2183.4487  1658.3161  2032.9938  5879.6406   \n",
       "mean_train_RMSE   3777.8817  4982.2623  4008.4188  4829.9549  9717.7719   \n",
       "mean_test_RMSE    3792.1205  4985.3482  4018.5090  4822.8579  9716.1640   \n",
       "\n",
       "                         15         16         17         18         19  \n",
       "mean_train_MAE    1737.1833  2411.2841  2685.4600  1775.2054  2458.9122  \n",
       "mean_test_MAE     1772.5296  2420.0104  2685.3512  1794.1628  2460.8917  \n",
       "mean_train_MAPE      0.1075     0.1546     0.1760     0.1094     0.1632  \n",
       "mean_test_MAPE       0.1094     0.1551     0.1762     0.1106     0.1634  \n",
       "mean_train_MedAE  1124.1663  1693.1318  1889.7226  1153.2974  1762.9708  \n",
       "mean_test_MedAE   1141.3689  1699.3119  1884.8615  1160.2152  1769.0095  \n",
       "mean_train_RMSE   3019.7121  3986.6194  4536.1180  3077.8644  3944.9332  \n",
       "mean_test_RMSE    3140.5348  4000.7682  4537.6439  3154.3823  3955.3592  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results_df_mlp_sgd = pd.DataFrame(rs_mlp_sgd.cv_results_)\n",
    "\n",
    "\n",
    "metric_cols_train_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_R2\")]\n",
    "metric_cols_test_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_R2\")]\n",
    "metric_cols_train_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_MAE\")]\n",
    "metric_cols_test_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_MAE\")]\n",
    "metric_cols_train_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_MAPE\")]\n",
    "metric_cols_test_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_MAPE\")]\n",
    "metric_cols_train_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_MedAE\")]\n",
    "metric_cols_test_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_MedAE\")]\n",
    "metric_cols_train_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_RMSE\")]\n",
    "metric_cols_test_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_RMSE\")]\n",
    "\n",
    "std_cols_train_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_R2\")]\n",
    "std_cols_test_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_R2\")]\n",
    "\n",
    "std_cols_train_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_MAE\")]\n",
    "std_cols_test_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_MAE\")]\n",
    "std_cols_train_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_MAPE\")]\n",
    "std_cols_test_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_MAPE\")]\n",
    "std_cols_train_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_MedAE\")]\n",
    "std_cols_test_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_MedAE\")]\n",
    "std_cols_train_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_RMSE\")]\n",
    "std_cols_test_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_RMSE\")]\n",
    "\n",
    "df_metrics_mlp_sgd = results_df_mlp_sgd[[\"params\"]+ metric_cols_train_MAE + metric_cols_test_MAE +\n",
    "                    metric_cols_train_R2 + metric_cols_test_R2 + \n",
    "                    metric_cols_train_MAPE + metric_cols_test_MAPE + \n",
    "                    metric_cols_train_MedAE + metric_cols_test_MedAE + \n",
    "                    metric_cols_train_RMSE + metric_cols_test_RMSE ]\n",
    "df_metrics_mlp_sgd = df_metrics_mlp_sgd.loc[:,['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] = df_metrics_mlp_sgd.loc[:,\n",
    "                                                                                        ['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] .round(4) * -1\n",
    "                    \n",
    "df_metrics_mlp_sgd.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cad0e2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__solver</th>\n",
       "      <th>param_regressor__regressor__max_iter</th>\n",
       "      <th>param_regressor__regressor__learning_rate_init</th>\n",
       "      <th>param_regressor__regressor__learning_rate</th>\n",
       "      <th>param_regressor__regressor__hidden_layer_sizes</th>\n",
       "      <th>param_regressor__regressor__batch_size</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <th>std_test_RMSE</th>\n",
       "      <th>rank_test_RMSE</th>\n",
       "      <th>split0_train_RMSE</th>\n",
       "      <th>split1_train_RMSE</th>\n",
       "      <th>split2_train_RMSE</th>\n",
       "      <th>split3_train_RMSE</th>\n",
       "      <th>split4_train_RMSE</th>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <th>std_train_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>558.028797</td>\n",
       "      <td>18.976123</td>\n",
       "      <td>16.200441</td>\n",
       "      <td>4.609916</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3589.729294</td>\n",
       "      <td>278.795294</td>\n",
       "      <td>5</td>\n",
       "      <td>-3623.567015</td>\n",
       "      <td>-3573.751681</td>\n",
       "      <td>-3454.721581</td>\n",
       "      <td>-3579.404931</td>\n",
       "      <td>-3611.390420</td>\n",
       "      <td>-3568.567126</td>\n",
       "      <td>59.939917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>377.705706</td>\n",
       "      <td>27.098480</td>\n",
       "      <td>17.470885</td>\n",
       "      <td>2.636715</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3437.093756</td>\n",
       "      <td>259.384596</td>\n",
       "      <td>4</td>\n",
       "      <td>-3469.404762</td>\n",
       "      <td>-3397.131378</td>\n",
       "      <td>-3312.293040</td>\n",
       "      <td>-3394.606579</td>\n",
       "      <td>-3461.865643</td>\n",
       "      <td>-3407.060280</td>\n",
       "      <td>56.789030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.362577</td>\n",
       "      <td>11.492277</td>\n",
       "      <td>14.026476</td>\n",
       "      <td>5.449640</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>constant</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3424.458603</td>\n",
       "      <td>275.950211</td>\n",
       "      <td>3</td>\n",
       "      <td>-3479.374764</td>\n",
       "      <td>-3390.819076</td>\n",
       "      <td>-3327.562300</td>\n",
       "      <td>-3381.370649</td>\n",
       "      <td>-3436.597421</td>\n",
       "      <td>-3403.144842</td>\n",
       "      <td>51.514729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>562.888881</td>\n",
       "      <td>32.192474</td>\n",
       "      <td>16.794674</td>\n",
       "      <td>5.258475</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3669.960146</td>\n",
       "      <td>254.503716</td>\n",
       "      <td>6</td>\n",
       "      <td>-3688.875532</td>\n",
       "      <td>-3667.719420</td>\n",
       "      <td>-3555.317770</td>\n",
       "      <td>-3626.307189</td>\n",
       "      <td>-3702.477223</td>\n",
       "      <td>-3648.139427</td>\n",
       "      <td>53.080899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.484686</td>\n",
       "      <td>14.066121</td>\n",
       "      <td>14.574779</td>\n",
       "      <td>5.498719</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-10164.198898</td>\n",
       "      <td>480.968476</td>\n",
       "      <td>20</td>\n",
       "      <td>-9828.447428</td>\n",
       "      <td>-10983.769140</td>\n",
       "      <td>-10127.229664</td>\n",
       "      <td>-9780.513070</td>\n",
       "      <td>-10154.694848</td>\n",
       "      <td>-10174.930830</td>\n",
       "      <td>431.861187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>742.741857</td>\n",
       "      <td>31.765148</td>\n",
       "      <td>15.924535</td>\n",
       "      <td>5.860255</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>constant</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3993.093789</td>\n",
       "      <td>271.414837</td>\n",
       "      <td>13</td>\n",
       "      <td>-3956.519959</td>\n",
       "      <td>-4006.918559</td>\n",
       "      <td>-3866.622497</td>\n",
       "      <td>-4029.580728</td>\n",
       "      <td>-4007.398314</td>\n",
       "      <td>-3973.408011</td>\n",
       "      <td>58.518414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>195.364304</td>\n",
       "      <td>14.925049</td>\n",
       "      <td>11.659679</td>\n",
       "      <td>1.476357</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3681.115114</td>\n",
       "      <td>272.993292</td>\n",
       "      <td>7</td>\n",
       "      <td>-3745.038658</td>\n",
       "      <td>-3671.663767</td>\n",
       "      <td>-3572.373330</td>\n",
       "      <td>-3612.991916</td>\n",
       "      <td>-3716.329262</td>\n",
       "      <td>-3663.679387</td>\n",
       "      <td>63.813958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>117.630982</td>\n",
       "      <td>11.805049</td>\n",
       "      <td>15.068488</td>\n",
       "      <td>4.435113</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3842.329474</td>\n",
       "      <td>304.426670</td>\n",
       "      <td>11</td>\n",
       "      <td>-3874.723954</td>\n",
       "      <td>-3810.474618</td>\n",
       "      <td>-3801.319241</td>\n",
       "      <td>-3835.471624</td>\n",
       "      <td>-3831.895370</td>\n",
       "      <td>-3830.776961</td>\n",
       "      <td>25.434509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>502.835241</td>\n",
       "      <td>21.121225</td>\n",
       "      <td>12.237389</td>\n",
       "      <td>2.824670</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>constant</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3834.520098</td>\n",
       "      <td>264.962000</td>\n",
       "      <td>10</td>\n",
       "      <td>-3894.702757</td>\n",
       "      <td>-3809.171791</td>\n",
       "      <td>-3751.647028</td>\n",
       "      <td>-3782.561085</td>\n",
       "      <td>-3871.936021</td>\n",
       "      <td>-3822.003737</td>\n",
       "      <td>53.756541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70.559527</td>\n",
       "      <td>8.421112</td>\n",
       "      <td>14.510177</td>\n",
       "      <td>5.434039</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3759.984618</td>\n",
       "      <td>217.862472</td>\n",
       "      <td>8</td>\n",
       "      <td>-3853.443162</td>\n",
       "      <td>-3758.214891</td>\n",
       "      <td>-3592.847103</td>\n",
       "      <td>-3690.804405</td>\n",
       "      <td>-3782.921461</td>\n",
       "      <td>-3735.646204</td>\n",
       "      <td>88.343408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>119.007178</td>\n",
       "      <td>14.859804</td>\n",
       "      <td>15.567739</td>\n",
       "      <td>5.415075</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3792.120532</td>\n",
       "      <td>241.681687</td>\n",
       "      <td>9</td>\n",
       "      <td>-3919.490306</td>\n",
       "      <td>-3777.277536</td>\n",
       "      <td>-3683.860172</td>\n",
       "      <td>-3692.595831</td>\n",
       "      <td>-3816.184893</td>\n",
       "      <td>-3777.881747</td>\n",
       "      <td>86.757391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2062.089798</td>\n",
       "      <td>66.676741</td>\n",
       "      <td>12.017694</td>\n",
       "      <td>1.858810</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>constant</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-4985.348199</td>\n",
       "      <td>250.840935</td>\n",
       "      <td>18</td>\n",
       "      <td>-5021.883327</td>\n",
       "      <td>-4989.153576</td>\n",
       "      <td>-4926.294135</td>\n",
       "      <td>-4928.484502</td>\n",
       "      <td>-5045.496121</td>\n",
       "      <td>-4982.262332</td>\n",
       "      <td>48.250013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>76.815181</td>\n",
       "      <td>2.112804</td>\n",
       "      <td>13.010936</td>\n",
       "      <td>5.664640</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-4018.508965</td>\n",
       "      <td>308.762894</td>\n",
       "      <td>15</td>\n",
       "      <td>-4112.329323</td>\n",
       "      <td>-3873.059847</td>\n",
       "      <td>-4019.545043</td>\n",
       "      <td>-4017.510703</td>\n",
       "      <td>-4019.648940</td>\n",
       "      <td>-4008.418771</td>\n",
       "      <td>76.748961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>102.682580</td>\n",
       "      <td>5.330166</td>\n",
       "      <td>13.010984</td>\n",
       "      <td>5.228511</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-4822.857883</td>\n",
       "      <td>407.744154</td>\n",
       "      <td>17</td>\n",
       "      <td>-4535.058602</td>\n",
       "      <td>-4695.274182</td>\n",
       "      <td>-4840.506602</td>\n",
       "      <td>-4936.863420</td>\n",
       "      <td>-5142.071763</td>\n",
       "      <td>-4829.954914</td>\n",
       "      <td>206.910816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>92.377069</td>\n",
       "      <td>6.062701</td>\n",
       "      <td>14.577364</td>\n",
       "      <td>4.299658</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-9716.164033</td>\n",
       "      <td>200.087059</td>\n",
       "      <td>19</td>\n",
       "      <td>-9764.540348</td>\n",
       "      <td>-9723.633636</td>\n",
       "      <td>-9641.518478</td>\n",
       "      <td>-9688.663497</td>\n",
       "      <td>-9770.503693</td>\n",
       "      <td>-9717.771930</td>\n",
       "      <td>48.299427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>187.055395</td>\n",
       "      <td>18.112049</td>\n",
       "      <td>13.829041</td>\n",
       "      <td>5.512543</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3140.534828</td>\n",
       "      <td>256.483755</td>\n",
       "      <td>1</td>\n",
       "      <td>-3034.162892</td>\n",
       "      <td>-2988.991477</td>\n",
       "      <td>-2972.533508</td>\n",
       "      <td>-3040.844582</td>\n",
       "      <td>-3062.027954</td>\n",
       "      <td>-3019.712083</td>\n",
       "      <td>33.513027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>88.325405</td>\n",
       "      <td>2.962579</td>\n",
       "      <td>16.692462</td>\n",
       "      <td>3.844482</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-4000.768172</td>\n",
       "      <td>266.618100</td>\n",
       "      <td>14</td>\n",
       "      <td>-4026.215663</td>\n",
       "      <td>-4181.243002</td>\n",
       "      <td>-3840.481320</td>\n",
       "      <td>-3849.992309</td>\n",
       "      <td>-4035.164729</td>\n",
       "      <td>-3986.619404</td>\n",
       "      <td>127.926666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>238.861156</td>\n",
       "      <td>22.098575</td>\n",
       "      <td>16.470852</td>\n",
       "      <td>3.634199</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-4537.643874</td>\n",
       "      <td>343.910177</td>\n",
       "      <td>16</td>\n",
       "      <td>-4576.559998</td>\n",
       "      <td>-4635.115021</td>\n",
       "      <td>-4521.681841</td>\n",
       "      <td>-4508.241726</td>\n",
       "      <td>-4438.991334</td>\n",
       "      <td>-4536.117984</td>\n",
       "      <td>66.113605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>459.071625</td>\n",
       "      <td>41.116077</td>\n",
       "      <td>15.115410</td>\n",
       "      <td>6.413855</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>constant</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3154.382267</td>\n",
       "      <td>239.660370</td>\n",
       "      <td>2</td>\n",
       "      <td>-3109.561561</td>\n",
       "      <td>-3058.253402</td>\n",
       "      <td>-3017.594393</td>\n",
       "      <td>-3075.214638</td>\n",
       "      <td>-3128.698123</td>\n",
       "      <td>-3077.864424</td>\n",
       "      <td>39.020158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>160.759178</td>\n",
       "      <td>13.574618</td>\n",
       "      <td>14.555457</td>\n",
       "      <td>6.235444</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>constant</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3955.359209</td>\n",
       "      <td>200.895114</td>\n",
       "      <td>12</td>\n",
       "      <td>-4118.835587</td>\n",
       "      <td>-3887.341703</td>\n",
       "      <td>-3795.147922</td>\n",
       "      <td>-3936.758375</td>\n",
       "      <td>-3986.582641</td>\n",
       "      <td>-3944.933245</td>\n",
       "      <td>107.513677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      558.028797     18.976123        16.200441        4.609916   \n",
       "1      377.705706     27.098480        17.470885        2.636715   \n",
       "2      195.362577     11.492277        14.026476        5.449640   \n",
       "3      562.888881     32.192474        16.794674        5.258475   \n",
       "4       80.484686     14.066121        14.574779        5.498719   \n",
       "5      742.741857     31.765148        15.924535        5.860255   \n",
       "6      195.364304     14.925049        11.659679        1.476357   \n",
       "7      117.630982     11.805049        15.068488        4.435113   \n",
       "8      502.835241     21.121225        12.237389        2.824670   \n",
       "9       70.559527      8.421112        14.510177        5.434039   \n",
       "10     119.007178     14.859804        15.567739        5.415075   \n",
       "11    2062.089798     66.676741        12.017694        1.858810   \n",
       "12      76.815181      2.112804        13.010936        5.664640   \n",
       "13     102.682580      5.330166        13.010984        5.228511   \n",
       "14      92.377069      6.062701        14.577364        4.299658   \n",
       "15     187.055395     18.112049        13.829041        5.512543   \n",
       "16      88.325405      2.962579        16.692462        3.844482   \n",
       "17     238.861156     22.098575        16.470852        3.634199   \n",
       "18     459.071625     41.116077        15.115410        6.413855   \n",
       "19     160.759178     13.574618        14.555457        6.235444   \n",
       "\n",
       "   param_regressor__regressor__solver  param_regressor__regressor__max_iter  \\\n",
       "0                                 sgd                                   700   \n",
       "1                                 sgd                                   700   \n",
       "2                                 sgd                                   700   \n",
       "3                                 sgd                                   700   \n",
       "4                                 sgd                                   700   \n",
       "5                                 sgd                                   700   \n",
       "6                                 sgd                                   700   \n",
       "7                                 sgd                                   700   \n",
       "8                                 sgd                                   700   \n",
       "9                                 sgd                                   700   \n",
       "10                                sgd                                   700   \n",
       "11                                sgd                                   700   \n",
       "12                                sgd                                   700   \n",
       "13                                sgd                                   700   \n",
       "14                                sgd                                   700   \n",
       "15                                sgd                                   700   \n",
       "16                                sgd                                   700   \n",
       "17                                sgd                                   700   \n",
       "18                                sgd                                   700   \n",
       "19                                sgd                                   700   \n",
       "\n",
       "    param_regressor__regressor__learning_rate_init  \\\n",
       "0                                          0.00010   \n",
       "1                                          0.00010   \n",
       "2                                          0.00100   \n",
       "3                                          0.00100   \n",
       "4                                          0.00010   \n",
       "5                                          0.00010   \n",
       "6                                          0.00010   \n",
       "7                                          0.01000   \n",
       "8                                          0.00010   \n",
       "9                                          0.01000   \n",
       "10                                         0.01000   \n",
       "11                                         0.00001   \n",
       "12                                         0.01000   \n",
       "13                                         0.00010   \n",
       "14                                         0.01000   \n",
       "15                                         0.01000   \n",
       "16                                         0.00100   \n",
       "17                                         0.00001   \n",
       "18                                         0.00100   \n",
       "19                                         0.00001   \n",
       "\n",
       "   param_regressor__regressor__learning_rate  \\\n",
       "0                                   adaptive   \n",
       "1                                   adaptive   \n",
       "2                                   constant   \n",
       "3                                   adaptive   \n",
       "4                                 invscaling   \n",
       "5                                   constant   \n",
       "6                                   adaptive   \n",
       "7                                 invscaling   \n",
       "8                                   constant   \n",
       "9                                 invscaling   \n",
       "10                                invscaling   \n",
       "11                                  constant   \n",
       "12                                invscaling   \n",
       "13                                invscaling   \n",
       "14                                invscaling   \n",
       "15                                  adaptive   \n",
       "16                                invscaling   \n",
       "17                                  adaptive   \n",
       "18                                  constant   \n",
       "19                                  constant   \n",
       "\n",
       "   param_regressor__regressor__hidden_layer_sizes  \\\n",
       "0                                      (200, 100)   \n",
       "1                                      (200, 100)   \n",
       "2                                   (100, 50, 25)   \n",
       "3                                      (200, 100)   \n",
       "4                                        (32, 16)   \n",
       "5                                      (200, 100)   \n",
       "6                                        (32, 16)   \n",
       "7                                      (200, 100)   \n",
       "8                                   (100, 50, 25)   \n",
       "9                                        (32, 16)   \n",
       "10                                     (200, 100)   \n",
       "11                                     (200, 100)   \n",
       "12                                       (32, 16)   \n",
       "13                                  (100, 50, 25)   \n",
       "14                                  (100, 50, 25)   \n",
       "15                                       (32, 16)   \n",
       "16                                  (100, 50, 25)   \n",
       "17                                       (32, 16)   \n",
       "18                                  (100, 50, 25)   \n",
       "19                                       (32, 16)   \n",
       "\n",
       "    param_regressor__regressor__batch_size  ...  mean_test_RMSE std_test_RMSE  \\\n",
       "0                                      200  ...    -3589.729294    278.795294   \n",
       "1                                      100  ...    -3437.093756    259.384596   \n",
       "2                                      500  ...    -3424.458603    275.950211   \n",
       "3                                      200  ...    -3669.960146    254.503716   \n",
       "4                                      500  ...   -10164.198898    480.968476   \n",
       "5                                      100  ...    -3993.093789    271.414837   \n",
       "6                                      200  ...    -3681.115114    272.993292   \n",
       "7                                      500  ...    -3842.329474    304.426670   \n",
       "8                                      100  ...    -3834.520098    264.962000   \n",
       "9                                      500  ...    -3759.984618    217.862472   \n",
       "10                                     500  ...    -3792.120532    241.681687   \n",
       "11                                     200  ...    -4985.348199    250.840935   \n",
       "12                                     500  ...    -4018.508965    308.762894   \n",
       "13                                     100  ...    -4822.857883    407.744154   \n",
       "14                                     500  ...    -9716.164033    200.087059   \n",
       "15                                     200  ...    -3140.534828    256.483755   \n",
       "16                                     200  ...    -4000.768172    266.618100   \n",
       "17                                     500  ...    -4537.643874    343.910177   \n",
       "18                                     100  ...    -3154.382267    239.660370   \n",
       "19                                     200  ...    -3955.359209    200.895114   \n",
       "\n",
       "   rank_test_RMSE  split0_train_RMSE  split1_train_RMSE  split2_train_RMSE  \\\n",
       "0               5       -3623.567015       -3573.751681       -3454.721581   \n",
       "1               4       -3469.404762       -3397.131378       -3312.293040   \n",
       "2               3       -3479.374764       -3390.819076       -3327.562300   \n",
       "3               6       -3688.875532       -3667.719420       -3555.317770   \n",
       "4              20       -9828.447428      -10983.769140      -10127.229664   \n",
       "5              13       -3956.519959       -4006.918559       -3866.622497   \n",
       "6               7       -3745.038658       -3671.663767       -3572.373330   \n",
       "7              11       -3874.723954       -3810.474618       -3801.319241   \n",
       "8              10       -3894.702757       -3809.171791       -3751.647028   \n",
       "9               8       -3853.443162       -3758.214891       -3592.847103   \n",
       "10              9       -3919.490306       -3777.277536       -3683.860172   \n",
       "11             18       -5021.883327       -4989.153576       -4926.294135   \n",
       "12             15       -4112.329323       -3873.059847       -4019.545043   \n",
       "13             17       -4535.058602       -4695.274182       -4840.506602   \n",
       "14             19       -9764.540348       -9723.633636       -9641.518478   \n",
       "15              1       -3034.162892       -2988.991477       -2972.533508   \n",
       "16             14       -4026.215663       -4181.243002       -3840.481320   \n",
       "17             16       -4576.559998       -4635.115021       -4521.681841   \n",
       "18              2       -3109.561561       -3058.253402       -3017.594393   \n",
       "19             12       -4118.835587       -3887.341703       -3795.147922   \n",
       "\n",
       "    split3_train_RMSE  split4_train_RMSE  mean_train_RMSE  std_train_RMSE  \n",
       "0        -3579.404931       -3611.390420     -3568.567126       59.939917  \n",
       "1        -3394.606579       -3461.865643     -3407.060280       56.789030  \n",
       "2        -3381.370649       -3436.597421     -3403.144842       51.514729  \n",
       "3        -3626.307189       -3702.477223     -3648.139427       53.080899  \n",
       "4        -9780.513070      -10154.694848    -10174.930830      431.861187  \n",
       "5        -4029.580728       -4007.398314     -3973.408011       58.518414  \n",
       "6        -3612.991916       -3716.329262     -3663.679387       63.813958  \n",
       "7        -3835.471624       -3831.895370     -3830.776961       25.434509  \n",
       "8        -3782.561085       -3871.936021     -3822.003737       53.756541  \n",
       "9        -3690.804405       -3782.921461     -3735.646204       88.343408  \n",
       "10       -3692.595831       -3816.184893     -3777.881747       86.757391  \n",
       "11       -4928.484502       -5045.496121     -4982.262332       48.250013  \n",
       "12       -4017.510703       -4019.648940     -4008.418771       76.748961  \n",
       "13       -4936.863420       -5142.071763     -4829.954914      206.910816  \n",
       "14       -9688.663497       -9770.503693     -9717.771930       48.299427  \n",
       "15       -3040.844582       -3062.027954     -3019.712083       33.513027  \n",
       "16       -3849.992309       -4035.164729     -3986.619404      127.926666  \n",
       "17       -4508.241726       -4438.991334     -4536.117984       66.113605  \n",
       "18       -3075.214638       -3128.698123     -3077.864424       39.020158  \n",
       "19       -3936.758375       -3986.582641     -3944.933245      107.513677  \n",
       "\n",
       "[20 rows x 88 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_mlp_sgd"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab0c5d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T15:23:31.343097Z",
     "start_time": "2025-12-07T15:23:31.173341Z"
    }
   },
   "source": [
    "\n",
    "results_df_mlp_adam_logprice = pd.DataFrame(rs_mlp_adam_logprice.cv_results_)\n",
    "\n",
    "\n",
    "metric_cols_train_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_R2\")]\n",
    "metric_cols_test_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_R2\")]\n",
    "metric_cols_train_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_MAE\")]\n",
    "metric_cols_test_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_MAE\")]\n",
    "metric_cols_train_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_MAPE\")]\n",
    "metric_cols_test_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_MAPE\")]\n",
    "metric_cols_train_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_MedAE\")]\n",
    "metric_cols_test_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_MedAE\")]\n",
    "metric_cols_train_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_RMSE\")]\n",
    "metric_cols_test_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_RMSE\")]\n",
    "\n",
    "std_cols_train_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_R2\")]\n",
    "std_cols_test_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_R2\")]\n",
    "\n",
    "std_cols_train_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_MAE\")]\n",
    "std_cols_test_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_MAE\")]\n",
    "std_cols_train_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_MAPE\")]\n",
    "std_cols_test_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_MAPE\")]\n",
    "std_cols_train_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_MedAE\")]\n",
    "std_cols_test_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_MedAE\")]\n",
    "std_cols_train_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_RMSE\")]\n",
    "std_cols_test_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_RMSE\")]\n",
    "\n",
    "df_metrics_mlp_adam_logprice = results_df_mlp_adam_logprice[[\"params\"]+ metric_cols_train_MAE + metric_cols_test_MAE +\n",
    "                    metric_cols_train_R2 + metric_cols_test_R2 + \n",
    "                    metric_cols_train_MAPE + metric_cols_test_MAPE + \n",
    "                    metric_cols_train_MedAE + metric_cols_test_MedAE + \n",
    "                    metric_cols_train_RMSE + metric_cols_test_RMSE ]\n",
    "df_metrics_mlp_adam_logprice = df_metrics_mlp_adam_logprice.loc[:,['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] = df_metrics_mlp_adam_logprice.loc[:,\n",
    "                                                                                        ['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] .round(4) * -1\n",
    "                    \n",
    "df_metrics_mlp_adam_logprice.T"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                         0          1          2          3          4   \\\n",
       "mean_train_MAE    1823.7927  4696.3662  1871.7151  1769.0096  6347.8533   \n",
       "mean_test_MAE     1840.5369  4718.1586  1881.9895  1820.2303  6326.2337   \n",
       "mean_train_MAPE      0.1081     0.3192     0.1108     0.1037     0.3635   \n",
       "mean_test_MAPE       0.1096     0.3192     0.1116     0.1070     0.3632   \n",
       "mean_train_MedAE  1144.5458  3350.0481  1193.7210  1090.6357  4840.1933   \n",
       "mean_test_MedAE   1148.7134  3363.6835  1204.3398  1107.5904  4814.1270   \n",
       "mean_train_RMSE   3284.4875  7186.6966  3353.9174  3182.6087  8898.1134   \n",
       "mean_test_RMSE    3323.2788  7215.6888  3377.3394  3892.2349  8915.9897   \n",
       "\n",
       "                         5          6          7          8          9   ...  \\\n",
       "mean_train_MAE    3091.9805  2189.1859  1763.2101  1778.1183  4146.4143  ...   \n",
       "mean_test_MAE     3094.6215  2203.5017  1794.4552  1830.5257  4162.8335  ...   \n",
       "mean_train_MAPE      0.1939     0.1331     0.1034     0.1064     0.2970  ...   \n",
       "mean_test_MAPE       0.1946     0.1337     0.1056     0.1100     0.2980  ...   \n",
       "mean_train_MedAE  2046.4653  1439.1585  1092.8137  1140.7243  3125.2564  ...   \n",
       "mean_test_MedAE   2045.1535  1444.8061  1104.3100  1162.4855  3149.1714  ...   \n",
       "mean_train_RMSE   5013.8162  3659.7234  3215.4182  3156.5098  6303.6779  ...   \n",
       "mean_test_RMSE    5027.2430  3671.0129  3297.4937  3463.2618  6317.7407  ...   \n",
       "\n",
       "                          20         21         22         23         24  \\\n",
       "mean_train_MAE     6913.4567  1778.3537  1769.6238  1927.6070  1642.7558   \n",
       "mean_test_MAE      6916.2295  1821.6808  1820.2453  1950.7856  1692.3330   \n",
       "mean_train_MAPE       0.4941     0.1086     0.1057     0.1116     0.0953   \n",
       "mean_test_MAPE        0.4940     0.1118     0.1094     0.1128     0.0994   \n",
       "mean_train_MedAE   5265.5308  1155.7387  1133.1250  1204.5307  1005.4943   \n",
       "mean_test_MedAE    5257.7348  1170.7079  1152.7171  1208.9873  1027.5927   \n",
       "mean_train_RMSE   10022.5280  3120.3785  3126.0205  3435.6506  2995.4454   \n",
       "mean_test_RMSE    10025.4002  3444.7066  3230.6217  3538.4065  3126.9732   \n",
       "\n",
       "                         25         26         27         28          29  \n",
       "mean_train_MAE    1922.6469  2003.6206  6075.9071  1908.0995   8848.0690  \n",
       "mean_test_MAE     1930.3365  2017.2177  6090.1588  1926.9268   8879.1705  \n",
       "mean_train_MAPE      0.1153     0.1203     0.4379     0.1130      0.7163  \n",
       "mean_test_MAPE       0.1159     0.1214     0.4388     0.1143      0.7180  \n",
       "mean_train_MedAE  1247.1476  1307.5095  4631.4735  1204.8594   7719.7619  \n",
       "mean_test_MedAE   1253.4162  1311.0224  4678.2650  1217.3762   7746.6992  \n",
       "mean_train_RMSE   3434.9845  3511.5516  8865.0315  3340.8180  11587.8090  \n",
       "mean_test_RMSE    3447.8044  3547.4701  8853.9947  3484.1455  11655.8953  \n",
       "\n",
       "[8 rows x 30 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <td>1823.7927</td>\n",
       "      <td>4696.3662</td>\n",
       "      <td>1871.7151</td>\n",
       "      <td>1769.0096</td>\n",
       "      <td>6347.8533</td>\n",
       "      <td>3091.9805</td>\n",
       "      <td>2189.1859</td>\n",
       "      <td>1763.2101</td>\n",
       "      <td>1778.1183</td>\n",
       "      <td>4146.4143</td>\n",
       "      <td>...</td>\n",
       "      <td>6913.4567</td>\n",
       "      <td>1778.3537</td>\n",
       "      <td>1769.6238</td>\n",
       "      <td>1927.6070</td>\n",
       "      <td>1642.7558</td>\n",
       "      <td>1922.6469</td>\n",
       "      <td>2003.6206</td>\n",
       "      <td>6075.9071</td>\n",
       "      <td>1908.0995</td>\n",
       "      <td>8848.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <td>1840.5369</td>\n",
       "      <td>4718.1586</td>\n",
       "      <td>1881.9895</td>\n",
       "      <td>1820.2303</td>\n",
       "      <td>6326.2337</td>\n",
       "      <td>3094.6215</td>\n",
       "      <td>2203.5017</td>\n",
       "      <td>1794.4552</td>\n",
       "      <td>1830.5257</td>\n",
       "      <td>4162.8335</td>\n",
       "      <td>...</td>\n",
       "      <td>6916.2295</td>\n",
       "      <td>1821.6808</td>\n",
       "      <td>1820.2453</td>\n",
       "      <td>1950.7856</td>\n",
       "      <td>1692.3330</td>\n",
       "      <td>1930.3365</td>\n",
       "      <td>2017.2177</td>\n",
       "      <td>6090.1588</td>\n",
       "      <td>1926.9268</td>\n",
       "      <td>8879.1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAPE</th>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.3635</td>\n",
       "      <td>0.1939</td>\n",
       "      <td>0.1331</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4941</td>\n",
       "      <td>0.1086</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.1153</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.7163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAPE</th>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4940</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>0.1159</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.4388</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.7180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MedAE</th>\n",
       "      <td>1144.5458</td>\n",
       "      <td>3350.0481</td>\n",
       "      <td>1193.7210</td>\n",
       "      <td>1090.6357</td>\n",
       "      <td>4840.1933</td>\n",
       "      <td>2046.4653</td>\n",
       "      <td>1439.1585</td>\n",
       "      <td>1092.8137</td>\n",
       "      <td>1140.7243</td>\n",
       "      <td>3125.2564</td>\n",
       "      <td>...</td>\n",
       "      <td>5265.5308</td>\n",
       "      <td>1155.7387</td>\n",
       "      <td>1133.1250</td>\n",
       "      <td>1204.5307</td>\n",
       "      <td>1005.4943</td>\n",
       "      <td>1247.1476</td>\n",
       "      <td>1307.5095</td>\n",
       "      <td>4631.4735</td>\n",
       "      <td>1204.8594</td>\n",
       "      <td>7719.7619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MedAE</th>\n",
       "      <td>1148.7134</td>\n",
       "      <td>3363.6835</td>\n",
       "      <td>1204.3398</td>\n",
       "      <td>1107.5904</td>\n",
       "      <td>4814.1270</td>\n",
       "      <td>2045.1535</td>\n",
       "      <td>1444.8061</td>\n",
       "      <td>1104.3100</td>\n",
       "      <td>1162.4855</td>\n",
       "      <td>3149.1714</td>\n",
       "      <td>...</td>\n",
       "      <td>5257.7348</td>\n",
       "      <td>1170.7079</td>\n",
       "      <td>1152.7171</td>\n",
       "      <td>1208.9873</td>\n",
       "      <td>1027.5927</td>\n",
       "      <td>1253.4162</td>\n",
       "      <td>1311.0224</td>\n",
       "      <td>4678.2650</td>\n",
       "      <td>1217.3762</td>\n",
       "      <td>7746.6992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <td>3284.4875</td>\n",
       "      <td>7186.6966</td>\n",
       "      <td>3353.9174</td>\n",
       "      <td>3182.6087</td>\n",
       "      <td>8898.1134</td>\n",
       "      <td>5013.8162</td>\n",
       "      <td>3659.7234</td>\n",
       "      <td>3215.4182</td>\n",
       "      <td>3156.5098</td>\n",
       "      <td>6303.6779</td>\n",
       "      <td>...</td>\n",
       "      <td>10022.5280</td>\n",
       "      <td>3120.3785</td>\n",
       "      <td>3126.0205</td>\n",
       "      <td>3435.6506</td>\n",
       "      <td>2995.4454</td>\n",
       "      <td>3434.9845</td>\n",
       "      <td>3511.5516</td>\n",
       "      <td>8865.0315</td>\n",
       "      <td>3340.8180</td>\n",
       "      <td>11587.8090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <td>3323.2788</td>\n",
       "      <td>7215.6888</td>\n",
       "      <td>3377.3394</td>\n",
       "      <td>3892.2349</td>\n",
       "      <td>8915.9897</td>\n",
       "      <td>5027.2430</td>\n",
       "      <td>3671.0129</td>\n",
       "      <td>3297.4937</td>\n",
       "      <td>3463.2618</td>\n",
       "      <td>6317.7407</td>\n",
       "      <td>...</td>\n",
       "      <td>10025.4002</td>\n",
       "      <td>3444.7066</td>\n",
       "      <td>3230.6217</td>\n",
       "      <td>3538.4065</td>\n",
       "      <td>3126.9732</td>\n",
       "      <td>3447.8044</td>\n",
       "      <td>3547.4701</td>\n",
       "      <td>8853.9947</td>\n",
       "      <td>3484.1455</td>\n",
       "      <td>11655.8953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "ef90043a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T15:23:47.510893Z",
     "start_time": "2025-12-07T15:23:47.475719Z"
    }
   },
   "source": "results_df_mlp_adam_logprice",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       31.478377      3.563553         3.370773        0.250992   \n",
       "1       98.546206     49.720939         4.240581        0.648203   \n",
       "2      138.268552     22.021792         4.548737        0.735356   \n",
       "3       94.671406     18.220213         3.659305        0.567268   \n",
       "4      189.485212     61.559806         5.410172        1.076697   \n",
       "5       50.476873     11.027237         3.741984        0.481592   \n",
       "6       30.912102      3.394773         3.716965        0.381253   \n",
       "7       77.462347      3.177266         4.221598        0.594457   \n",
       "8       68.146701     15.476163         3.938739        0.340778   \n",
       "9       43.031234      3.072215         4.061826        0.488856   \n",
       "10      51.066626     30.518761         3.832821        0.466952   \n",
       "11     398.275573    180.681305         4.210492        0.664153   \n",
       "12     129.968417      3.958523         4.634185        0.629188   \n",
       "13     188.123398     53.785138         4.609593        0.383518   \n",
       "14      56.668848      6.360419         4.162617        0.506973   \n",
       "15      57.983543     21.104738         4.203888        0.724551   \n",
       "16      38.067512      7.177378         3.923891        0.371266   \n",
       "17     338.172058     71.689651         4.026541        0.730399   \n",
       "18     330.601102     43.192999         5.315227        0.825389   \n",
       "19      56.859680      4.934298         3.779324        0.612667   \n",
       "20      62.143703      5.826298         4.841043        0.736918   \n",
       "21      59.376954     19.064104         3.856312        0.600644   \n",
       "22     247.313128     92.515911         2.938709        0.892753   \n",
       "23      44.058560     13.889678         3.951998        0.369083   \n",
       "24     117.731767      4.249347         3.593629        0.123232   \n",
       "25      39.294637      2.146576         3.680499        0.514347   \n",
       "26      98.426086      6.094100         4.230798        0.705783   \n",
       "27      72.223433      4.244474         3.690974        0.562158   \n",
       "28      33.481158      1.444735         3.922804        0.274149   \n",
       "29      62.356761      9.304282         1.951683        0.063265   \n",
       "\n",
       "   param_regressor__regressor__solver  param_regressor__regressor__max_iter  \\\n",
       "0                                adam                                   700   \n",
       "1                                adam                                   700   \n",
       "2                                adam                                   700   \n",
       "3                                adam                                   700   \n",
       "4                                adam                                   700   \n",
       "5                                adam                                   700   \n",
       "6                                adam                                   700   \n",
       "7                                adam                                   700   \n",
       "8                                adam                                   700   \n",
       "9                                adam                                   700   \n",
       "10                               adam                                   700   \n",
       "11                               adam                                   700   \n",
       "12                               adam                                   700   \n",
       "13                               adam                                   700   \n",
       "14                               adam                                   700   \n",
       "15                               adam                                   700   \n",
       "16                               adam                                   700   \n",
       "17                               adam                                   700   \n",
       "18                               adam                                   700   \n",
       "19                               adam                                   700   \n",
       "20                               adam                                   700   \n",
       "21                               adam                                   700   \n",
       "22                               adam                                   700   \n",
       "23                               adam                                   700   \n",
       "24                               adam                                   700   \n",
       "25                               adam                                   700   \n",
       "26                               adam                                   700   \n",
       "27                               adam                                   700   \n",
       "28                               adam                                   700   \n",
       "29                               adam                                   700   \n",
       "\n",
       "    param_regressor__regressor__learning_rate_init  \\\n",
       "0                                            0.010   \n",
       "1                                            0.010   \n",
       "2                                            0.001   \n",
       "3                                            0.001   \n",
       "4                                            0.100   \n",
       "5                                            0.100   \n",
       "6                                            0.100   \n",
       "7                                            0.010   \n",
       "8                                            0.001   \n",
       "9                                            0.100   \n",
       "10                                           0.010   \n",
       "11                                           0.100   \n",
       "12                                           0.100   \n",
       "13                                           0.001   \n",
       "14                                           0.010   \n",
       "15                                           0.010   \n",
       "16                                           0.010   \n",
       "17                                           0.001   \n",
       "18                                           0.001   \n",
       "19                                           0.001   \n",
       "20                                           0.100   \n",
       "21                                           0.001   \n",
       "22                                           0.001   \n",
       "23                                           0.010   \n",
       "24                                           0.001   \n",
       "25                                           0.001   \n",
       "26                                           0.010   \n",
       "27                                           0.100   \n",
       "28                                           0.001   \n",
       "29                                           0.100   \n",
       "\n",
       "   param_regressor__regressor__hidden_layer_sizes  \\\n",
       "0                                        (32, 16)   \n",
       "1                                  (200, 100, 50)   \n",
       "2                                  (200, 100, 50)   \n",
       "3                                  (200, 100, 50)   \n",
       "4                                      (600, 200)   \n",
       "5                                      (200, 100)   \n",
       "6                                   (100, 50, 25)   \n",
       "7                                   (100, 50, 25)   \n",
       "8                                      (200, 100)   \n",
       "9                                  (200, 100, 50)   \n",
       "10                                 (200, 100, 50)   \n",
       "11                                     (600, 200)   \n",
       "12                                     (600, 200)   \n",
       "13                                     (600, 200)   \n",
       "14                                  (100, 50, 25)   \n",
       "15                                     (200, 100)   \n",
       "16                                       (32, 16)   \n",
       "17                                     (600, 200)   \n",
       "18                                (300, 200, 100)   \n",
       "19                                  (100, 50, 25)   \n",
       "20                                 (200, 100, 50)   \n",
       "21                                  (100, 50, 25)   \n",
       "22                                (300, 200, 100)   \n",
       "23                                  (100, 50, 25)   \n",
       "24                                 (200, 100, 50)   \n",
       "25                                       (32, 16)   \n",
       "26                                     (600, 200)   \n",
       "27                                (300, 200, 100)   \n",
       "28                                       (32, 16)   \n",
       "29                                (300, 200, 100)   \n",
       "\n",
       "   param_regressor__regressor__activation  \\\n",
       "0                                    tanh   \n",
       "1                                logistic   \n",
       "2                                logistic   \n",
       "3                                    relu   \n",
       "4                                logistic   \n",
       "5                                    relu   \n",
       "6                                    relu   \n",
       "7                                logistic   \n",
       "8                                    relu   \n",
       "9                                    tanh   \n",
       "10                                   relu   \n",
       "11                                   relu   \n",
       "12                                   tanh   \n",
       "13                                   relu   \n",
       "14                                   tanh   \n",
       "15                                   relu   \n",
       "16                               logistic   \n",
       "17                                   tanh   \n",
       "18                               logistic   \n",
       "19                                   tanh   \n",
       "20                               logistic   \n",
       "21                                   relu   \n",
       "22                                   relu   \n",
       "23                                   relu   \n",
       "24                                   tanh   \n",
       "25                               logistic   \n",
       "26                                   tanh   \n",
       "27                                   relu   \n",
       "28                                   relu   \n",
       "29                               logistic   \n",
       "\n",
       "                                               params  ...  mean_test_RMSE  \\\n",
       "0   {'regressor__regressor__solver': 'adam', 'regr...  ...    -3323.278840   \n",
       "1   {'regressor__regressor__solver': 'adam', 'regr...  ...    -7215.688829   \n",
       "2   {'regressor__regressor__solver': 'adam', 'regr...  ...    -3377.339351   \n",
       "3   {'regressor__regressor__solver': 'adam', 'regr...  ...    -3892.234888   \n",
       "4   {'regressor__regressor__solver': 'adam', 'regr...  ...    -8915.989702   \n",
       "5   {'regressor__regressor__solver': 'adam', 'regr...  ...    -5027.243036   \n",
       "6   {'regressor__regressor__solver': 'adam', 'regr...  ...    -3671.012884   \n",
       "7   {'regressor__regressor__solver': 'adam', 'regr...  ...    -3297.493673   \n",
       "8   {'regressor__regressor__solver': 'adam', 'regr...  ...    -3463.261803   \n",
       "9   {'regressor__regressor__solver': 'adam', 'regr...  ...    -6317.740656   \n",
       "10  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3771.530955   \n",
       "11  {'regressor__regressor__solver': 'adam', 'regr...  ...    -5349.971594   \n",
       "12  {'regressor__regressor__solver': 'adam', 'regr...  ...    -6674.091355   \n",
       "13  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3576.139629   \n",
       "14  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3428.887658   \n",
       "15  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3443.515572   \n",
       "16  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3325.851283   \n",
       "17  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3217.919548   \n",
       "18  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3378.296496   \n",
       "19  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3211.190701   \n",
       "20  {'regressor__regressor__solver': 'adam', 'regr...  ...   -10025.400153   \n",
       "21  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3444.706610   \n",
       "22  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3230.621679   \n",
       "23  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3538.406497   \n",
       "24  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3126.973191   \n",
       "25  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3447.804360   \n",
       "26  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3547.470095   \n",
       "27  {'regressor__regressor__solver': 'adam', 'regr...  ...    -8853.994713   \n",
       "28  {'regressor__regressor__solver': 'adam', 'regr...  ...    -3484.145493   \n",
       "29  {'regressor__regressor__solver': 'adam', 'regr...  ...   -11655.895338   \n",
       "\n",
       "    std_test_RMSE  rank_test_RMSE  split0_train_RMSE  split1_train_RMSE  \\\n",
       "0      230.369908               6       -3529.089624       -3176.851216   \n",
       "1     3340.030063              26       -3287.823920       -3070.427441   \n",
       "2      250.789020               8       -3370.715157       -3331.720219   \n",
       "3     1448.825318              21       -3158.232483       -3051.190950   \n",
       "4     2116.483464              28      -11034.657071       -8039.582787   \n",
       "5     2434.264395              22       -3494.455159       -3944.188860   \n",
       "6      357.591138              19       -3358.924825       -3840.500874   \n",
       "7      281.535401               5       -3252.083369       -3097.899381   \n",
       "8      550.631034              14       -3097.774023       -3047.943829   \n",
       "9     1804.650538              24       -5836.411986       -9758.598367   \n",
       "10     376.967815              20       -3921.764143       -4230.770141   \n",
       "11    2421.275566              23       -3890.214013       -3669.607277   \n",
       "12    2184.833422              25      -10470.005680       -4126.730020   \n",
       "13     711.024679              18       -3439.047407       -3266.950681   \n",
       "14     333.368804              10       -3398.328486       -3544.705048   \n",
       "15     253.404043              11       -3555.222543       -3367.519879   \n",
       "16     264.032600               7       -3305.338032       -3133.823651   \n",
       "17     283.083573               3       -3168.902313       -3138.974097   \n",
       "18     256.158671               9       -3448.592657       -3364.602909   \n",
       "19     328.953224               2       -3137.119703       -3140.160475   \n",
       "20     369.855233              29       -9787.415898      -10532.246537   \n",
       "21     560.718681              12       -3081.762380       -3196.474398   \n",
       "22     308.492452               4       -3197.276097       -3271.676985   \n",
       "23     490.995042              16       -3180.059712       -3173.442109   \n",
       "24     266.624578               1       -2967.616405       -2907.857951   \n",
       "25     271.674586              13       -3535.523803       -3391.347322   \n",
       "26     149.811704              17       -3658.331844       -3407.686387   \n",
       "27    2279.290450              27       -4648.975305       -9852.364449   \n",
       "28     353.527209              15       -3403.075242       -3464.764897   \n",
       "29    1705.117218              30      -12002.794680      -10832.234948   \n",
       "\n",
       "    split2_train_RMSE  split3_train_RMSE  split4_train_RMSE  mean_train_RMSE  \\\n",
       "0        -3205.293231       -3226.072908       -3285.130635     -3284.487523   \n",
       "1        -9794.650898       -9910.775870       -9869.804961     -7186.696618   \n",
       "2        -3228.475554       -3332.262768       -3506.413088     -3353.917357   \n",
       "3        -2952.947017       -3567.351141       -3183.321847     -3182.608688   \n",
       "4        -4691.317049      -10304.823392      -10420.186877     -8898.113435   \n",
       "5        -3679.940664       -3793.555523      -10156.940583     -5013.816158   \n",
       "6        -3471.768013       -3591.020194       -4036.403039     -3659.723389   \n",
       "7        -3113.445671       -3337.116701       -3276.545700     -3215.418164   \n",
       "8        -3099.890995       -3233.957024       -3302.983372     -3156.509849   \n",
       "9        -5929.446357       -4337.615903       -5656.316788     -6303.677880   \n",
       "10       -3407.145190       -3407.127248       -3376.688330     -3668.699010   \n",
       "11       -4192.240642       -9993.262143       -4793.839644     -5307.832744   \n",
       "12       -4673.859039       -5663.579505       -8037.251585     -6594.285166   \n",
       "13       -4544.216846       -3255.634229       -3226.186655     -3546.407164   \n",
       "14       -3241.793986       -3398.199145       -3297.309060     -3376.067145   \n",
       "15       -3401.589832       -3362.141910       -3387.171680     -3414.729169   \n",
       "16       -3181.885248       -3275.812118       -3458.865913     -3271.144992   \n",
       "17       -3115.948731       -3121.698629       -3068.183368     -3122.741427   \n",
       "18       -3261.183055       -3314.474341       -3352.922968     -3348.355186   \n",
       "19       -3119.684649       -3253.152587       -3102.352738     -3150.494030   \n",
       "20       -9952.755087       -9729.568408      -10110.654080    -10022.528002   \n",
       "21       -3042.012821       -3072.381831       -3209.260992     -3120.378485   \n",
       "22       -3163.216014       -2837.831220       -3160.102029     -3126.020469   \n",
       "23       -3551.963630       -3865.693221       -3407.094455     -3435.650625   \n",
       "24       -2871.646424       -3174.105219       -3056.001236     -2995.445447   \n",
       "25       -3353.236348       -3395.516292       -3499.298866     -3434.984526   \n",
       "26       -3241.840379       -3401.492289       -3848.407135     -3511.551607   \n",
       "27       -9919.577523       -9995.714987       -9908.525282     -8865.031509   \n",
       "28       -3218.232953       -3305.670733       -3312.346421     -3340.818049   \n",
       "29       -9905.020820      -10240.838208      -14958.156478    -11587.809027   \n",
       "\n",
       "    std_train_RMSE  \n",
       "0       127.357904  \n",
       "1      3273.102013  \n",
       "2        89.704397  \n",
       "3       209.110769  \n",
       "4      2336.431890  \n",
       "5      2575.754224  \n",
       "6       247.020318  \n",
       "7        93.915388  \n",
       "8        95.806817  \n",
       "9      1820.952405  \n",
       "10      347.006150  \n",
       "11     2372.990048  \n",
       "12     2355.692659  \n",
       "13      504.447476  \n",
       "14      103.591749  \n",
       "15       71.644375  \n",
       "16      112.476258  \n",
       "17       32.918496  \n",
       "18       61.772641  \n",
       "19       53.087167  \n",
       "20      287.593629  \n",
       "21       68.741178  \n",
       "22      149.593643  \n",
       "23      258.209537  \n",
       "24      108.957369  \n",
       "25       69.841524  \n",
       "26      214.784203  \n",
       "27     2108.523081  \n",
       "28       85.219543  \n",
       "29     1830.216039  \n",
       "\n",
       "[30 rows x 85 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__solver</th>\n",
       "      <th>param_regressor__regressor__max_iter</th>\n",
       "      <th>param_regressor__regressor__learning_rate_init</th>\n",
       "      <th>param_regressor__regressor__hidden_layer_sizes</th>\n",
       "      <th>param_regressor__regressor__activation</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <th>std_test_RMSE</th>\n",
       "      <th>rank_test_RMSE</th>\n",
       "      <th>split0_train_RMSE</th>\n",
       "      <th>split1_train_RMSE</th>\n",
       "      <th>split2_train_RMSE</th>\n",
       "      <th>split3_train_RMSE</th>\n",
       "      <th>split4_train_RMSE</th>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <th>std_train_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.478377</td>\n",
       "      <td>3.563553</td>\n",
       "      <td>3.370773</td>\n",
       "      <td>0.250992</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3323.278840</td>\n",
       "      <td>230.369908</td>\n",
       "      <td>6</td>\n",
       "      <td>-3529.089624</td>\n",
       "      <td>-3176.851216</td>\n",
       "      <td>-3205.293231</td>\n",
       "      <td>-3226.072908</td>\n",
       "      <td>-3285.130635</td>\n",
       "      <td>-3284.487523</td>\n",
       "      <td>127.357904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.546206</td>\n",
       "      <td>49.720939</td>\n",
       "      <td>4.240581</td>\n",
       "      <td>0.648203</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-7215.688829</td>\n",
       "      <td>3340.030063</td>\n",
       "      <td>26</td>\n",
       "      <td>-3287.823920</td>\n",
       "      <td>-3070.427441</td>\n",
       "      <td>-9794.650898</td>\n",
       "      <td>-9910.775870</td>\n",
       "      <td>-9869.804961</td>\n",
       "      <td>-7186.696618</td>\n",
       "      <td>3273.102013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138.268552</td>\n",
       "      <td>22.021792</td>\n",
       "      <td>4.548737</td>\n",
       "      <td>0.735356</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3377.339351</td>\n",
       "      <td>250.789020</td>\n",
       "      <td>8</td>\n",
       "      <td>-3370.715157</td>\n",
       "      <td>-3331.720219</td>\n",
       "      <td>-3228.475554</td>\n",
       "      <td>-3332.262768</td>\n",
       "      <td>-3506.413088</td>\n",
       "      <td>-3353.917357</td>\n",
       "      <td>89.704397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94.671406</td>\n",
       "      <td>18.220213</td>\n",
       "      <td>3.659305</td>\n",
       "      <td>0.567268</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3892.234888</td>\n",
       "      <td>1448.825318</td>\n",
       "      <td>21</td>\n",
       "      <td>-3158.232483</td>\n",
       "      <td>-3051.190950</td>\n",
       "      <td>-2952.947017</td>\n",
       "      <td>-3567.351141</td>\n",
       "      <td>-3183.321847</td>\n",
       "      <td>-3182.608688</td>\n",
       "      <td>209.110769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189.485212</td>\n",
       "      <td>61.559806</td>\n",
       "      <td>5.410172</td>\n",
       "      <td>1.076697</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-8915.989702</td>\n",
       "      <td>2116.483464</td>\n",
       "      <td>28</td>\n",
       "      <td>-11034.657071</td>\n",
       "      <td>-8039.582787</td>\n",
       "      <td>-4691.317049</td>\n",
       "      <td>-10304.823392</td>\n",
       "      <td>-10420.186877</td>\n",
       "      <td>-8898.113435</td>\n",
       "      <td>2336.431890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50.476873</td>\n",
       "      <td>11.027237</td>\n",
       "      <td>3.741984</td>\n",
       "      <td>0.481592</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-5027.243036</td>\n",
       "      <td>2434.264395</td>\n",
       "      <td>22</td>\n",
       "      <td>-3494.455159</td>\n",
       "      <td>-3944.188860</td>\n",
       "      <td>-3679.940664</td>\n",
       "      <td>-3793.555523</td>\n",
       "      <td>-10156.940583</td>\n",
       "      <td>-5013.816158</td>\n",
       "      <td>2575.754224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30.912102</td>\n",
       "      <td>3.394773</td>\n",
       "      <td>3.716965</td>\n",
       "      <td>0.381253</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3671.012884</td>\n",
       "      <td>357.591138</td>\n",
       "      <td>19</td>\n",
       "      <td>-3358.924825</td>\n",
       "      <td>-3840.500874</td>\n",
       "      <td>-3471.768013</td>\n",
       "      <td>-3591.020194</td>\n",
       "      <td>-4036.403039</td>\n",
       "      <td>-3659.723389</td>\n",
       "      <td>247.020318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77.462347</td>\n",
       "      <td>3.177266</td>\n",
       "      <td>4.221598</td>\n",
       "      <td>0.594457</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3297.493673</td>\n",
       "      <td>281.535401</td>\n",
       "      <td>5</td>\n",
       "      <td>-3252.083369</td>\n",
       "      <td>-3097.899381</td>\n",
       "      <td>-3113.445671</td>\n",
       "      <td>-3337.116701</td>\n",
       "      <td>-3276.545700</td>\n",
       "      <td>-3215.418164</td>\n",
       "      <td>93.915388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68.146701</td>\n",
       "      <td>15.476163</td>\n",
       "      <td>3.938739</td>\n",
       "      <td>0.340778</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3463.261803</td>\n",
       "      <td>550.631034</td>\n",
       "      <td>14</td>\n",
       "      <td>-3097.774023</td>\n",
       "      <td>-3047.943829</td>\n",
       "      <td>-3099.890995</td>\n",
       "      <td>-3233.957024</td>\n",
       "      <td>-3302.983372</td>\n",
       "      <td>-3156.509849</td>\n",
       "      <td>95.806817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.031234</td>\n",
       "      <td>3.072215</td>\n",
       "      <td>4.061826</td>\n",
       "      <td>0.488856</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-6317.740656</td>\n",
       "      <td>1804.650538</td>\n",
       "      <td>24</td>\n",
       "      <td>-5836.411986</td>\n",
       "      <td>-9758.598367</td>\n",
       "      <td>-5929.446357</td>\n",
       "      <td>-4337.615903</td>\n",
       "      <td>-5656.316788</td>\n",
       "      <td>-6303.677880</td>\n",
       "      <td>1820.952405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51.066626</td>\n",
       "      <td>30.518761</td>\n",
       "      <td>3.832821</td>\n",
       "      <td>0.466952</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3771.530955</td>\n",
       "      <td>376.967815</td>\n",
       "      <td>20</td>\n",
       "      <td>-3921.764143</td>\n",
       "      <td>-4230.770141</td>\n",
       "      <td>-3407.145190</td>\n",
       "      <td>-3407.127248</td>\n",
       "      <td>-3376.688330</td>\n",
       "      <td>-3668.699010</td>\n",
       "      <td>347.006150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>398.275573</td>\n",
       "      <td>180.681305</td>\n",
       "      <td>4.210492</td>\n",
       "      <td>0.664153</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-5349.971594</td>\n",
       "      <td>2421.275566</td>\n",
       "      <td>23</td>\n",
       "      <td>-3890.214013</td>\n",
       "      <td>-3669.607277</td>\n",
       "      <td>-4192.240642</td>\n",
       "      <td>-9993.262143</td>\n",
       "      <td>-4793.839644</td>\n",
       "      <td>-5307.832744</td>\n",
       "      <td>2372.990048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>129.968417</td>\n",
       "      <td>3.958523</td>\n",
       "      <td>4.634185</td>\n",
       "      <td>0.629188</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-6674.091355</td>\n",
       "      <td>2184.833422</td>\n",
       "      <td>25</td>\n",
       "      <td>-10470.005680</td>\n",
       "      <td>-4126.730020</td>\n",
       "      <td>-4673.859039</td>\n",
       "      <td>-5663.579505</td>\n",
       "      <td>-8037.251585</td>\n",
       "      <td>-6594.285166</td>\n",
       "      <td>2355.692659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>188.123398</td>\n",
       "      <td>53.785138</td>\n",
       "      <td>4.609593</td>\n",
       "      <td>0.383518</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3576.139629</td>\n",
       "      <td>711.024679</td>\n",
       "      <td>18</td>\n",
       "      <td>-3439.047407</td>\n",
       "      <td>-3266.950681</td>\n",
       "      <td>-4544.216846</td>\n",
       "      <td>-3255.634229</td>\n",
       "      <td>-3226.186655</td>\n",
       "      <td>-3546.407164</td>\n",
       "      <td>504.447476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56.668848</td>\n",
       "      <td>6.360419</td>\n",
       "      <td>4.162617</td>\n",
       "      <td>0.506973</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3428.887658</td>\n",
       "      <td>333.368804</td>\n",
       "      <td>10</td>\n",
       "      <td>-3398.328486</td>\n",
       "      <td>-3544.705048</td>\n",
       "      <td>-3241.793986</td>\n",
       "      <td>-3398.199145</td>\n",
       "      <td>-3297.309060</td>\n",
       "      <td>-3376.067145</td>\n",
       "      <td>103.591749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>57.983543</td>\n",
       "      <td>21.104738</td>\n",
       "      <td>4.203888</td>\n",
       "      <td>0.724551</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3443.515572</td>\n",
       "      <td>253.404043</td>\n",
       "      <td>11</td>\n",
       "      <td>-3555.222543</td>\n",
       "      <td>-3367.519879</td>\n",
       "      <td>-3401.589832</td>\n",
       "      <td>-3362.141910</td>\n",
       "      <td>-3387.171680</td>\n",
       "      <td>-3414.729169</td>\n",
       "      <td>71.644375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>38.067512</td>\n",
       "      <td>7.177378</td>\n",
       "      <td>3.923891</td>\n",
       "      <td>0.371266</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3325.851283</td>\n",
       "      <td>264.032600</td>\n",
       "      <td>7</td>\n",
       "      <td>-3305.338032</td>\n",
       "      <td>-3133.823651</td>\n",
       "      <td>-3181.885248</td>\n",
       "      <td>-3275.812118</td>\n",
       "      <td>-3458.865913</td>\n",
       "      <td>-3271.144992</td>\n",
       "      <td>112.476258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>338.172058</td>\n",
       "      <td>71.689651</td>\n",
       "      <td>4.026541</td>\n",
       "      <td>0.730399</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3217.919548</td>\n",
       "      <td>283.083573</td>\n",
       "      <td>3</td>\n",
       "      <td>-3168.902313</td>\n",
       "      <td>-3138.974097</td>\n",
       "      <td>-3115.948731</td>\n",
       "      <td>-3121.698629</td>\n",
       "      <td>-3068.183368</td>\n",
       "      <td>-3122.741427</td>\n",
       "      <td>32.918496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>330.601102</td>\n",
       "      <td>43.192999</td>\n",
       "      <td>5.315227</td>\n",
       "      <td>0.825389</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(300, 200, 100)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3378.296496</td>\n",
       "      <td>256.158671</td>\n",
       "      <td>9</td>\n",
       "      <td>-3448.592657</td>\n",
       "      <td>-3364.602909</td>\n",
       "      <td>-3261.183055</td>\n",
       "      <td>-3314.474341</td>\n",
       "      <td>-3352.922968</td>\n",
       "      <td>-3348.355186</td>\n",
       "      <td>61.772641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>56.859680</td>\n",
       "      <td>4.934298</td>\n",
       "      <td>3.779324</td>\n",
       "      <td>0.612667</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3211.190701</td>\n",
       "      <td>328.953224</td>\n",
       "      <td>2</td>\n",
       "      <td>-3137.119703</td>\n",
       "      <td>-3140.160475</td>\n",
       "      <td>-3119.684649</td>\n",
       "      <td>-3253.152587</td>\n",
       "      <td>-3102.352738</td>\n",
       "      <td>-3150.494030</td>\n",
       "      <td>53.087167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>62.143703</td>\n",
       "      <td>5.826298</td>\n",
       "      <td>4.841043</td>\n",
       "      <td>0.736918</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-10025.400153</td>\n",
       "      <td>369.855233</td>\n",
       "      <td>29</td>\n",
       "      <td>-9787.415898</td>\n",
       "      <td>-10532.246537</td>\n",
       "      <td>-9952.755087</td>\n",
       "      <td>-9729.568408</td>\n",
       "      <td>-10110.654080</td>\n",
       "      <td>-10022.528002</td>\n",
       "      <td>287.593629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>59.376954</td>\n",
       "      <td>19.064104</td>\n",
       "      <td>3.856312</td>\n",
       "      <td>0.600644</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3444.706610</td>\n",
       "      <td>560.718681</td>\n",
       "      <td>12</td>\n",
       "      <td>-3081.762380</td>\n",
       "      <td>-3196.474398</td>\n",
       "      <td>-3042.012821</td>\n",
       "      <td>-3072.381831</td>\n",
       "      <td>-3209.260992</td>\n",
       "      <td>-3120.378485</td>\n",
       "      <td>68.741178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>247.313128</td>\n",
       "      <td>92.515911</td>\n",
       "      <td>2.938709</td>\n",
       "      <td>0.892753</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(300, 200, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3230.621679</td>\n",
       "      <td>308.492452</td>\n",
       "      <td>4</td>\n",
       "      <td>-3197.276097</td>\n",
       "      <td>-3271.676985</td>\n",
       "      <td>-3163.216014</td>\n",
       "      <td>-2837.831220</td>\n",
       "      <td>-3160.102029</td>\n",
       "      <td>-3126.020469</td>\n",
       "      <td>149.593643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>44.058560</td>\n",
       "      <td>13.889678</td>\n",
       "      <td>3.951998</td>\n",
       "      <td>0.369083</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3538.406497</td>\n",
       "      <td>490.995042</td>\n",
       "      <td>16</td>\n",
       "      <td>-3180.059712</td>\n",
       "      <td>-3173.442109</td>\n",
       "      <td>-3551.963630</td>\n",
       "      <td>-3865.693221</td>\n",
       "      <td>-3407.094455</td>\n",
       "      <td>-3435.650625</td>\n",
       "      <td>258.209537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>117.731767</td>\n",
       "      <td>4.249347</td>\n",
       "      <td>3.593629</td>\n",
       "      <td>0.123232</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3126.973191</td>\n",
       "      <td>266.624578</td>\n",
       "      <td>1</td>\n",
       "      <td>-2967.616405</td>\n",
       "      <td>-2907.857951</td>\n",
       "      <td>-2871.646424</td>\n",
       "      <td>-3174.105219</td>\n",
       "      <td>-3056.001236</td>\n",
       "      <td>-2995.445447</td>\n",
       "      <td>108.957369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>39.294637</td>\n",
       "      <td>2.146576</td>\n",
       "      <td>3.680499</td>\n",
       "      <td>0.514347</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3447.804360</td>\n",
       "      <td>271.674586</td>\n",
       "      <td>13</td>\n",
       "      <td>-3535.523803</td>\n",
       "      <td>-3391.347322</td>\n",
       "      <td>-3353.236348</td>\n",
       "      <td>-3395.516292</td>\n",
       "      <td>-3499.298866</td>\n",
       "      <td>-3434.984526</td>\n",
       "      <td>69.841524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>98.426086</td>\n",
       "      <td>6.094100</td>\n",
       "      <td>4.230798</td>\n",
       "      <td>0.705783</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3547.470095</td>\n",
       "      <td>149.811704</td>\n",
       "      <td>17</td>\n",
       "      <td>-3658.331844</td>\n",
       "      <td>-3407.686387</td>\n",
       "      <td>-3241.840379</td>\n",
       "      <td>-3401.492289</td>\n",
       "      <td>-3848.407135</td>\n",
       "      <td>-3511.551607</td>\n",
       "      <td>214.784203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72.223433</td>\n",
       "      <td>4.244474</td>\n",
       "      <td>3.690974</td>\n",
       "      <td>0.562158</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(300, 200, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-8853.994713</td>\n",
       "      <td>2279.290450</td>\n",
       "      <td>27</td>\n",
       "      <td>-4648.975305</td>\n",
       "      <td>-9852.364449</td>\n",
       "      <td>-9919.577523</td>\n",
       "      <td>-9995.714987</td>\n",
       "      <td>-9908.525282</td>\n",
       "      <td>-8865.031509</td>\n",
       "      <td>2108.523081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33.481158</td>\n",
       "      <td>1.444735</td>\n",
       "      <td>3.922804</td>\n",
       "      <td>0.274149</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-3484.145493</td>\n",
       "      <td>353.527209</td>\n",
       "      <td>15</td>\n",
       "      <td>-3403.075242</td>\n",
       "      <td>-3464.764897</td>\n",
       "      <td>-3218.232953</td>\n",
       "      <td>-3305.670733</td>\n",
       "      <td>-3312.346421</td>\n",
       "      <td>-3340.818049</td>\n",
       "      <td>85.219543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>62.356761</td>\n",
       "      <td>9.304282</td>\n",
       "      <td>1.951683</td>\n",
       "      <td>0.063265</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(300, 200, 100)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'regressor__regressor__solver': 'adam', 'regr...</td>\n",
       "      <td>...</td>\n",
       "      <td>-11655.895338</td>\n",
       "      <td>1705.117218</td>\n",
       "      <td>30</td>\n",
       "      <td>-12002.794680</td>\n",
       "      <td>-10832.234948</td>\n",
       "      <td>-9905.020820</td>\n",
       "      <td>-10240.838208</td>\n",
       "      <td>-14958.156478</td>\n",
       "      <td>-11587.809027</td>\n",
       "      <td>1830.216039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 85 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__max_iter': 2000, 'regressor__fit_intercept': True, 'regressor__epsilon': 2.0, 'regressor__alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# best estimator usa o MAE que foi escolhido no refit da randomized search\n",
    "\n",
    "# y_prev_MAE = random_search.best_estimator_.predict(df_test)\n",
    "\n",
    "#--------------\n",
    "\n",
    "# MAS se quisermos escolher nós próprias o modelo, escolhemos os parâmetros e colocamos na pipeline diretamente:\n",
    "\n",
    "model_params = random_search_huber_standardscaler.cv_results_['params'][1]\n",
    "\n",
    "print(model_params)\n",
    "\n",
    "test_pipeline = pipeline_huber_standardscaler.set_params(**model_params)\n",
    "\n",
    "test_pipeline.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "y_pred = test_pipeline.predict(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "080750b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89856</td>\n",
       "      <td>12395.172688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106581</td>\n",
       "      <td>22889.303863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80886</td>\n",
       "      <td>16059.964318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100174</td>\n",
       "      <td>18685.122790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81376</td>\n",
       "      <td>21292.119165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32562</th>\n",
       "      <td>105775</td>\n",
       "      <td>17533.123908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32563</th>\n",
       "      <td>81363</td>\n",
       "      <td>32517.830227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32564</th>\n",
       "      <td>76833</td>\n",
       "      <td>30390.936661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32565</th>\n",
       "      <td>91768</td>\n",
       "      <td>20886.943567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32566</th>\n",
       "      <td>99627</td>\n",
       "      <td>13234.943995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32567 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        carID         price\n",
       "0       89856  12395.172688\n",
       "1      106581  22889.303863\n",
       "2       80886  16059.964318\n",
       "3      100174  18685.122790\n",
       "4       81376  21292.119165\n",
       "...       ...           ...\n",
       "32562  105775  17533.123908\n",
       "32563   81363  32517.830227\n",
       "32564   76833  30390.936661\n",
       "32565   91768  20886.943567\n",
       "32566   99627  13234.943995\n",
       "\n",
       "[32567 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.DataFrame({'carID' : df_test.index, 'price' : y_pred})\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18a05bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.to_csv(\"finetuning_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b590d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions_mlp = [{\n",
    "    'regressor__hidden_layer_sizes' : [(10,), (50,), (100,), (50,50), (100,50), (100,100)],\n",
    "    'regressor__max_iter' : [200, 500, 1000],\n",
    "    'regressor__activation' : ['relu', 'tanh', 'logistic', 'sigmoid'],\n",
    "    'regressor__solver' : ['adam'],\n",
    "    'regressor__learning_rate_init' : [0.001, 0.01, 0.1],\n",
    "    \n",
    "},\n",
    "{ \n",
    "    'regressor__hidden_layer_sizes' : [(10,), (50,), (100,), (50,50), (100,50), (100,100)],\n",
    "    'regressor__max_iter' : [200, 500, 1000],\n",
    "    'regressor__activation' : ['relu', 'tanh', 'logistic', 'sigmoid'],\n",
    "    'regressor__solver' : ['sgd'],\n",
    "    'regressor__learning_rate' :  ['constant','invscaling','adaptive'],\n",
    "    'regressor__learning_rate_init' : [0.001, 0.01, 0.1],\n",
    "    'regressor__batch_size' : [50, 100, 200]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b59445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a58393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(    \n",
    "    solver='adam',\n",
    "    hidden_layer_sizes=(600,400,200,100),\n",
    "    max_iter=1000,\n",
    "    activation='relu',\n",
    "    learning_rate_init=0.01,\n",
    "    random_state=random_state)\n",
    "\n",
    "\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), #NO NEED FOR RANDOM FOREST REG\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=mlp, transformer = StandardScaler()) )\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "530f70a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 1751.4008\n",
      "Validation MAE: 1753.6437\n",
      "Overfitting: -0.13%\n"
     ]
    }
   ],
   "source": [
    "pipeline_mlp.fit(X_train, y_train)\n",
    "y_train_pred = pipeline_mlp.predict(X_train)\n",
    "y_val_pred = pipeline_mlp.predict(X_val)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "overfit = (train_mae - val_mae) / train_mae * 100\n",
    "print(f\"Train MAE: {train_mae:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Overfitting: {overfit:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5af92",
   "metadata": {},
   "source": [
    "adam, logistic, LR = 0.1, size = (13,)\n",
    "n_iter = 342\n",
    "Train MAE: 1965.7185\n",
    "Validation MAE: 1935.6366\n",
    "\n",
    "\n",
    "adam, logistic, LR =0.01, size = (80,60)\n",
    "n_iter = 846\n",
    "Train MAE: 1516.8650\n",
    "Validation MAE: 1644.8813\n",
    "Overfitting: -8.44%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adam, relu, LR = (10,5)\n",
    "n_iter = 134\n",
    "Train MAE: 1960.8814\n",
    "Validation MAE: 1946.3452\n",
    "Overfitting: 0.74%\n",
    "\n",
    "adam, relu, LR = (400,200)\n",
    "n_iter = 135\n",
    "Train MAE: 1486.1078\n",
    "Validation MAE: 1626.7325\n",
    "Overfitting: -9.46%\n",
    "melhores resultados para lr = 0.01 , (600,400,200) sem overfit\n",
    "\n",
    "\n",
    "\n",
    "adam, tanh, LR = 0.01\n",
    "n_iter = 562\n",
    "Train MAE: 1607.2187\n",
    "Validation MAE: 1725.7870\n",
    "Overfitting: -7.38%\n",
    "\n",
    "\n",
    "\n",
    "sgd, logistic, 400,200, LR = 0.1, adaptive, batch = 500\n",
    "Train MAE: 1709.2573\n",
    "Validation MAE: 1722.8373\n",
    "Overfitting: -0.79%\n",
    "bons resultados para batch = 100, LR = 0.1, (400,200,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f43d0740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_mlp.named_steps['regressor'].regressor_.n_iter_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbe29e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "losses = pipeline_mlp.named_steps['regressor'].regressor_.loss_curve_\n",
    "iterations = range(pipeline_mlp.named_steps['regressor'].regressor_.n_iter_)\n",
    "sns.lineplot(x = iterations, y = losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
