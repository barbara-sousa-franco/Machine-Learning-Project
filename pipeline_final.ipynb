{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b270cdee",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <h1 style=\"background-color: #80dfffff; color: #137a91ff; padding: 10px\">\n",
    "    <strong>Model Development</strong>\n",
    "  </h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b93080",
   "metadata": {},
   "source": [
    "**Student ID's:**\n",
    "\n",
    "Andreea Roica: 20250361\n",
    "\n",
    "Beatriz Varela: 20250367\n",
    "\n",
    "Barbara Franco: 20250388\n",
    "\n",
    "Marisa Esteves: 20250348"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3addf27",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 style=\"background-color: #80dfffff; color: #137a91ff; padding: 5px; margin: 5px;\">\n",
    "<strong>Index</strong>\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020932e",
   "metadata": {},
   "source": [
    "[1. **Pre Processing**](#1st-bullet)<br>\n",
    "\n",
    "[2. **Pipeline**](#2nd-bullet)<br>\n",
    "\n",
    "[3. **Fine Tuning**](#3rd-bullet)<br>\n",
    "\n",
    "[4. **Model Comparison**](#9th-bullet)<br>\n",
    "\n",
    "[5. **Ablation Study**](#10th-bullet)<br>\n",
    "\n",
    "[6. **Feature Importance**](#11th-bullet)<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4df272",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 style=\"background-color: #80dfffff; color: #137a91ff; padding: 5px; margin: 5px;\">\n",
    "<strong>Imports</strong>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edd2f9",
   "metadata": {},
   "source": [
    "Importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334eb522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes import *\n",
    "\n",
    "from functions import *\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import shap\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b058fc6",
   "metadata": {},
   "source": [
    "Uploading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff95aee",
   "metadata": {},
   "source": [
    "Setting carID as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.set_index('carID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b54d6",
   "metadata": {},
   "source": [
    "Setting a seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff0671",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 id=\"1st-bullet\" style=\"background-color: #f7d888ff; color: #da6919ff; padding: 5px; margin: 5px;\">\n",
    "  <strong>  1. Pre Processing</strong>\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3ea9a",
   "metadata": {},
   "source": [
    "Drop informations given by the mechanic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('paintQuality', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9ac63",
   "metadata": {},
   "source": [
    "We start by defining the inconsistent values discussed in the EDA as NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0536e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train['year']>2020, 'year'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mileage']<0, 'mileage'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['tax']<0, 'tax'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mpg'] < 8, 'mpg'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['previousOwners']< 0, 'previousOwners'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['engineSize'] < 1, 'engineSize'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345109",
   "metadata": {},
   "source": [
    "We proceed to round 'year' and 'previousOwners' to whole numbers using the floor function. Other numerical features are rounded to 2 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80085750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['year'] = np.floor(df_train['year'])\n",
    "df_train['previousOwners'] = np.floor(df_train['previousOwners'])\n",
    "\n",
    "for feat in ['mileage', 'tax', 'mpg', 'engineSize', 'paintQuality%']:\n",
    "    df_train[feat] = df_train[feat].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf0b21",
   "metadata": {},
   "source": [
    "We also pre-process the categorical variables in order to have a uniform format for later treatment (inside k-fold CV). We remove leeading and trailing spaces and uppercase all letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454801c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing the categorical variables to be easier to find clusters in typos:\n",
    "    # remove spaces (at the beginning and end) and uppercase all letters\n",
    "    # does not replace NaN's\n",
    "df_train['Brand'] = df_train['Brand'].where(df_train['Brand'].isna(), df_train['Brand'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['model'] = df_train['model'].where(df_train['model'].isna(), df_train['model'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['fuelType'] = df_train['fuelType'].where(df_train['fuelType'].isna(), df_train['fuelType'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['transmission'] = df_train['transmission'].where(df_train['transmission'].isna(), df_train['transmission'].astype(str).str.strip().str.upper())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b7f67",
   "metadata": {},
   "source": [
    "The W's in brand could either mean 'BMW' or 'VW'. We already checked that they are all 'VW', so lets correct the W's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd49ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train['Brand'] == 'W', 'Brand'] = 'VW'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674043e6",
   "metadata": {},
   "source": [
    "Division between input variables (X) and output variable (y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3225b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df_train['price']\n",
    "X = df_train.drop('price', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732ce2e",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 id=\"2nd-bullet\" style=\"background-color: #f7d888ff; color: #da6919ff; padding: 5px; margin: 5px;\">\n",
    "  <strong>  2. Pipeline</strong>\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803cf8f-c0d6-4010-8f81-069e96baa879",
   "metadata": {},
   "source": [
    "We are going to build a pipeline that is shared by all the models.\n",
    "\n",
    "This funtion is created here because it uses the classes, therefore it cannot be include in the functions.py file like the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0391c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(regressor, use_log=False, target_scaler=None, scaler_instance=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Builds a complete machine learning pipeline for regression tasks. \n",
    "    The pipeline allows different regressors, scaling strategies and optional log-transformation or scaling of the target variable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor: \n",
    "        Regression model used as the final estimator.\n",
    "\n",
    "    use_log: boolean \n",
    "        Indicates whether to apply log-transformation to the target variable.\n",
    "\n",
    "    target_scaler: scaler object\n",
    "        Scaler applied to the target variable. Default is None (no target scaling).\n",
    "\n",
    "    scaler_instance: scaler object \n",
    "        Scaler object passed to the scaling step.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        A machine learning pipeline with preprocessing, feature engineering, feature selection, and regression steps.\n",
    "    \"\"\"\n",
    "\n",
    "    if use_log:\n",
    "        final_reg = TransformedTargetRegressor(regressor=regressor,\n",
    "                                               func=np.log,\n",
    "                                               inverse_func=np.exp)\n",
    "\n",
    "    elif target_scaler is not None:\n",
    "        final_reg = TransformedTargetRegressor(\n",
    "            regressor=regressor,\n",
    "            transformer=target_scaler\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        final_reg = regressor\n",
    "\n",
    "    return Pipeline([\n",
    "        ('categorical_treatment', Categorical_Correction()),\n",
    "        ('outlier_treatment', Outlier_Treatment()),\n",
    "        ('missing_value_treatment', Missing_Value_Treatment()),\n",
    "        ('typecasting', Typecasting()),\n",
    "        ('feature_engineering', Feature_Engineering()),\n",
    "        ('encoder', Encoder()),\n",
    "        ('scaler', Scaler(scaler=scaler_instance)),\n",
    "        ('feature_selection', Feature_Selection()),\n",
    "        ('regressor', final_reg)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957eb2a8-1717-4cd4-8327-65ed0fba1ffc",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 id=\"3rd-bullet\" style=\"background-color: #f7d888ff; color: #da6919ff; padding: 5px; margin: 5px;\">\n",
    "  <strong>  3. Fine Tuning</strong>\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5450c-bd05-428a-9b5d-574ca03d18f3",
   "metadata": {},
   "source": [
    "We are going to create a model dictionary that defines the regressor instance and the hyperparameter search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d84b72-72df-4ae0-a9e1-4e87023cbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_testing = {\n",
    "    \"KNN\": {\n",
    "        \"regressor\": KNeighborsRegressor(),\n",
    "        \"use_log\": True,\n",
    "        \"param_distributions\": {\n",
    "            'feature_selection__rfe_k': list(range(1,15)),\n",
    "            'feature_selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "            'regressor__regressor__n_neighbors': np.arange(1, 100),\n",
    "            'regressor__regressor__weights': ['uniform', 'distance'],\n",
    "            'regressor__regressor__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'regressor__regressor__metric': ['minkowski', 'manhattan', 'euclidean', 'chebyshev'],\n",
    "            'regressor__regressor__p': [1, 1.5, 2, 3, 4],\n",
    "            'scaler__scaler': [StandardScaler(), MinMaxScaler(feature_range=(0,1)), MinMaxScaler(feature_range=(-1,1)), RobustScaler()]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"GradientBoosting\": {\n",
    "        \"regressor\": GradientBoostingRegressor(),\n",
    "        \"use_log\": True,\n",
    "        \"param_distributions\": {\n",
    "            'feature_selection__rfe_k': list(range(1,15)),\n",
    "            'feature_selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "            'regressor__regressor__max_features': [0.4, 0.5, 0.6],\n",
    "            'regressor__regressor__loss': ['absolute_error', 'huber'],\n",
    "            'regressor__regressor__n_estimators': [700, 900, 1100],\n",
    "            'regressor__regressor__max_depth': [6, 7, 8, 9], \n",
    "            'regressor__regressor__learning_rate': [0.02, 0.03, 0.04, 0.05],\n",
    "            'regressor__regressor__subsample': [0.75, 0.8],\n",
    "            'regressor__regressor__min_samples_split': [4, 5, 6],\n",
    "            'regressor__regressor__min_samples_leaf': [3, 4],\n",
    "            'regressor__regressor__criterion': ['squared_error'],\n",
    "            'regressor__regressor__min_impurity_decrease': [0.0, 0.0001, 0.0005],\n",
    "            'regressor__regressor__max_leaf_nodes': [None, 30, 40],\n",
    "            'regressor__regressor__n_iter_no_change': [None, 10, 15],\n",
    "            'regressor__regressor__validation_fraction': [0.15, 0.2],\n",
    "            'scaler__scaler': [StandardScaler(), MinMaxScaler(feature_range=(0,1)), MinMaxScaler(feature_range=(-1,1)), RobustScaler()]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"RandomForest\": {\n",
    "        \"regressor\": RandomForestRegressor(),\n",
    "        \"use_log\": False,\n",
    "        \"param_distributions\": {\n",
    "            'feature_selection__rfe_k': list(range(1,15)),\n",
    "            'feature_selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "            'regressor__n_estimators': [400,300],\n",
    "            'regressor__min_samples_split': [24,25,26,30,35],  \n",
    "            'regressor__min_samples_leaf': [2,3,4],   \n",
    "            'regressor__max_features': [0.5, 'sqrt'],\n",
    "            'regressor__max_depth': [18,19,20],         \n",
    "            'regressor__min_impurity_decrease': [0.001, 0.0005],\n",
    "            'regressor__ccp_alpha':[0.00005, 0.0001],\n",
    "            'regressor__max_samples': [0.75, 0.80],\n",
    "            'regressor__criterion': ['squared_error'], #default\n",
    "            'regressor__bootstrap': [True], #default\n",
    "            'scaler__scaler': [StandardScaler(), MinMaxScaler(feature_range=(0,1)), MinMaxScaler(feature_range=(-1,1)), RobustScaler()]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"MLP_sgd\": {\n",
    "        \"regressor\" : MLPRegressor(),\n",
    "        \"use_log\" : False,\n",
    "        \"target_scaler\": StandardScaler(),\n",
    "        \"param_distributions\" : { \n",
    "            'feature_selection__rfe_k': list(range(1,15)),\n",
    "            'feature_selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "            'regressor__regressor__solver' : ['sgd'],\n",
    "            'regressor__regressor__hidden_layer_sizes' : [(32,16),(100,50), (150,70), (200,100), (100,50,25), (200,100,50)],\n",
    "            'regressor__regressor__max_iter' :  [300],\n",
    "            'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "            'regressor__regressor__learning_rate' :  ['constant','invscaling','adaptive'],\n",
    "            'regressor__regressor__learning_rate_init' : [0.01, 0.001, 0.0001, 0.00001],\n",
    "            'regressor__regressor__batch_size' : [100, 200, 300],\n",
    "            'regressor__regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "            'scaler__scaler': [StandardScaler(), MinMaxScaler(feature_range=(0,1)), MinMaxScaler(feature_range=(-1,1)), RobustScaler()]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"MLP_adam\": {\n",
    "            \"regressor\" : MLPRegressor(),\n",
    "            \"use_log\" : False,\n",
    "            \"param_distributions\" : { \n",
    "                'feature_selection__rfe_k': list(range(1,15)),\n",
    "                'feature_selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "                'regressor__solver' : ['adam'],\n",
    "                'regressor__hidden_layer_sizes' : [(32,16), (200, 100), (400, 200), (100,50,25)],\n",
    "                'regressor__max_iter' :  [700],\n",
    "                'regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "                'regressor__learning_rate_init' : [0.001, 0.01, 0.1],\n",
    "                'scaler__scaler': [StandardScaler(), MinMaxScaler(feature_range=(0,1)), MinMaxScaler(feature_range=(-1,1)), RobustScaler()]\n",
    "        }\n",
    "    },\n",
    "\n",
    "\n",
    "    \"Huber\": {\n",
    "            \"regressor\" : HuberRegressor(),\n",
    "            \"use_log\" : False,\n",
    "            \"param_distributions\" : { \n",
    "                    'feature_selection__rfe_k': list(range(1,15)),\n",
    "                    'feature_selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "                    'regressor__epsilon': [1.1, 1.2, 1.35, 1.5, 2.0, 2.5, 3.0],\n",
    "                    'regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0],\n",
    "                    'regressor__fit_intercept': [True],\n",
    "                    'regressor__max_iter': [500, 1000, 2000],\n",
    "                    'scaler__scaler': [StandardScaler(), MinMaxScaler(feature_range=(0,1)), MinMaxScaler(feature_range=(-1,1)), RobustScaler()]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Decision Tree\": {\n",
    "            \"regressor\" : DecisionTreeRegressor(),\n",
    "            \"use_log\" : False,\n",
    "            \"param_distributions\" : { \n",
    "                    'feature_selection__rfe_k': list(range(1,15)),\n",
    "                    'feature_selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "                    'regressor__max_depth': [5, 10, 15, 20, None],\n",
    "                    'regressor__min_samples_split': [5, 6, 7, 8, 9, 10],\n",
    "                    'regressor__min_samples_leaf': [5, 6, 7],\n",
    "                    'regressor__criterion': ['absolute_error'],\n",
    "                    'regressor__min_impurity_decrease': [0.0001, 0.0005, 0.001],\n",
    "                    'regressor__max_features': ['sqrt', 0.5, 0.2],  \n",
    "                    'regressor__max_leaf_nodes': [None, 20, 50, 100],\n",
    "                    'scaler__scaler': [StandardScaler(), MinMaxScaler(feature_range=(0,1)), MinMaxScaler(feature_range=(-1,1)), RobustScaler()]  \n",
    "\n",
    "        }\n",
    "     }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659cac1",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f14a5",
   "metadata": {},
   "source": [
    "This funtion is created here because it uses build_pipeline, therefore it cannot be include in the functions.py file like the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_search(name, cfg, X, y, n_iter=2, cv=5, random_state=42):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs a RandomizedSearchCV for hyperparameter tuning of a regression model pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name of the model being evaluated.\n",
    "\n",
    "    cfg : dict\n",
    "        Configuration dictionary containing:    \n",
    "        - 'regressor': The regression model to be used in the pipeline. \n",
    "        - 'use_log': Boolean indicating whether to apply log transformation to the target variable.\n",
    "        - 'target_scaler': Scaler applied to the target variable. \n",
    "            If not specified, no scaling is applied.\n",
    "        - 'param_distributions': Dictionary of hyperparameter distributions for RandomizedSearchCV.\n",
    "    \n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix used for training the model.\n",
    "\n",
    "    y : pandas.Series\n",
    "        Target variable (price).\n",
    "\n",
    "    n_iter : int, default=2\n",
    "        Number of parameter settings that are sampled in RandomizedSearchCV.\n",
    "\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds.\n",
    "\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing:\n",
    "        - 'Model': The name of the model.\n",
    "        - 'Use_Log': Boolean indicating if log transformation was used.\n",
    "        - 'Target_Scaled': Boolean indicating if target variable was scaled.\n",
    "        - 'Search_Object': The fitted RandomizedSearchCV object.\n",
    "        - 'CV_Results': DataFrame with cross-validation results and additional metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    scoring = { 'R2': 'r2', \n",
    "    'MAE': 'neg_mean_absolute_error',\n",
    "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "    'MedAE': 'neg_median_absolute_error',\n",
    "    'RMSE': 'neg_root_mean_squared_error'}\n",
    "\n",
    "    print(f\"\\n=== Running: {name} (use_log={cfg['use_log']}) ===\")\n",
    "\n",
    "    pipeline = build_pipeline(cfg['regressor'], use_log=cfg['use_log'], target_scaler=cfg.get('target_scaler', None))\n",
    "    params = cfg['param_distributions']\n",
    "\n",
    "    num_config = math.prod(len(v) for v in params.values())\n",
    "    n_iter = min(n_iter, num_config)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=params,\n",
    "        n_iter=n_iter,\n",
    "        scoring=scoring,\n",
    "        refit='MAE',\n",
    "        cv=KFold(n_splits=cv, shuffle=True, random_state=random_state),\n",
    "        verbose=2,\n",
    "        return_train_score=True,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    search.fit(X, y)\n",
    "    t_elapsed = time.time() - t0\n",
    "\n",
    "    cv_results_df = pd.DataFrame(search.cv_results_)\n",
    "    \n",
    "    cv_results_df['overfit_mae'] = (cv_results_df['mean_test_MAE'] - cv_results_df['mean_train_MAE']) / cv_results_df['mean_train_MAE'] * 100\n",
    "    cv_results_df['overfit_R2'] = (cv_results_df['mean_test_R2'] - cv_results_df['mean_train_R2']) / cv_results_df['mean_train_R2'] * 100\n",
    "    cv_results_df['Model'] = name\n",
    "    cv_results_df['Use_Log'] = cfg['use_log']\n",
    "    cv_results_df['Execution_Time_s'] = t_elapsed\n",
    "\n",
    "    result = {\n",
    "        'Model': name,\n",
    "        'Use_Log': cfg['use_log'],\n",
    "        'Target_Scaled': cfg.get('target_scaler', None) is not None,\n",
    "        'Search_Object': search,\n",
    "        'CV_Results': cv_results_df\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1055e41",
   "metadata": {},
   "source": [
    "### Loop through all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791796d-96ac-46b1-8a7e-ea2bef1a0adf",
   "metadata": {},
   "source": [
    "After collecting all CV results from every model, we are going to rank all hyperparameter combinations by lowest test MAE and display the top 50 best-performing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd939715",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for model_name, cfg in models_testing.items():\n",
    "    res = run_model_search(model_name, cfg, X, y, n_iter=2, cv=5, random_state=random_state)\n",
    "    all_results.append(res)\n",
    "\n",
    "all_cv_summary = pd.concat([r['CV_Results'] for r in all_results], ignore_index=True)\n",
    "\n",
    "all_cv_summary['mean_train_MAE_pos'] = -all_cv_summary['mean_train_MAE']\n",
    "all_cv_summary['mean_test_MAE_pos'] = -all_cv_summary['mean_test_MAE']\n",
    "\n",
    "all_cv_summary.to_csv(\"all_cv_summary.csv\", index=True)\n",
    "\n",
    "top_50_cv_summary = all_cv_summary.sort_values('mean_test_MAE_pos', ascending=True).head(50)\n",
    "\n",
    "top_50_cv_summary.to_csv(\"top_50_cv_summary.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== Top 50 hyperparameter combinations with lowest CV MAE ===\")\n",
    "display(top_50_cv_summary[[\n",
    "    'Model', 'Use_Log', 'params',\n",
    "    'mean_train_MAE_pos', 'mean_test_MAE_pos',\n",
    "    'mean_train_R2', 'mean_test_R2',\n",
    "    'mean_train_MAPE', 'mean_test_MAPE',\n",
    "    'mean_train_MedAE', 'mean_test_MedAE',\n",
    "    'mean_train_RMSE', 'mean_test_RMSE',\n",
    "    'overfit_mae', 'overfit_R2'\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47a952",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 id=\"4th-bullet\" style=\"background-color: #f7d888ff; color: #da6919ff; padding: 5px; margin: 5px;\">\n",
    "  <strong>  4. Model Comparison</strong>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d10dec",
   "metadata": {},
   "source": [
    "Fazer talvez uma tabela com os melhores resultados de cada modelo? Analisar em comparação com os melhores MAE'S que temos\n",
    "\n",
    "DEPOIS PODEMOS FAZER UMA TABELA COM O MELHOR RESULTADO PARA CADA MODELO (AQUI NO MARKDOWN MESMO, NÃO É PRECISO CODIFICAR) COM R2, MAE, OVERFIT E ASSIM PARA TRAIN E VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d55017",
   "metadata": {},
   "source": [
    "### **Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2b7f7",
   "metadata": {},
   "source": [
    "justificar a escolhar e a seguir apresentar quais os escolhidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5421530",
   "metadata": {},
   "source": [
    "The best parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_cv_summary['params'][0]) #depois mudar de 0 para o indice certo!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02052769-4d0c-4fe6-bffb-efbf2d187525",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 id=\"5th-bullet\" style=\"background-color: #f7d888ff; color: #da6919ff; padding: 5px; margin: 5px;\">\n",
    "  <strong>  5. Ablation Study</strong>\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fce71",
   "metadata": {},
   "source": [
    "For the ablation study, we'll evaluate the performance of the chosen model with the best parameters removing or simplifying (when removing is not possible) one step of the pipeline at a time. We'll compare these results with the performance of the model using the full pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02c18a",
   "metadata": {},
   "source": [
    "Defining the CV strategy and the scoring metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = KFold(n_splits=2, shuffle=True, random_state=42)  \n",
    "scoring = { 'R2': 'r2', 'MAE': 'neg_mean_absolute_error'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8ec55",
   "metadata": {},
   "source": [
    "Defining the full pipeline and saving the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68938c-693d-4f5f-beb5-b33ca36d9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), \n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', LinearRegression() ) # corrigir para o regressor final!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161369b-18e2-4229-abca-3eeedb2f55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Pipeline with the Final Regressor\n",
    "baseline_train_r2, baseline_test_r2, baseline_train_mae, baseline_test_mae, baseline_time = evaluate(pipeline, X, y, cv=CV, scoring=scoring)\n",
    "baseline_pipeline_results = {\n",
    "    'Step Tested': 'Full Pipeline',\n",
    "    'Train R2': baseline_train_r2,\n",
    "    'Test R2': baseline_test_r2,\n",
    "    'Train MAE': baseline_train_mae,\n",
    "    'Test MAE': baseline_test_mae,\n",
    "    'Execution Time': baseline_time }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4a886",
   "metadata": {},
   "source": [
    "Testing each step individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45c321-cd69-4644-97b7-c52aee6fec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_to_test = ['categorical treatment','outlier treatment','missing value treatment','typecasting', 'feature engineering','encoder','scaler','feature selection']\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append (baseline_pipeline_results)\n",
    "\n",
    "for step in steps_to_test:\n",
    "\n",
    "    simplified_pipeline = Pipeline([\n",
    "        ('categorical treatment', Categorical_Correction() if step != 'categorical treatment' else Simplified_Categorical_Correction()),\n",
    "        ('outlier treatment', Outlier_Treatment() if step != 'outlier treatment' else Identity_Transformer()),                   \n",
    "        ('missing value treatment', Missing_Value_Treatment() if step != 'missing value treatment' else Simplified_Missing_Value_Treatment()),\n",
    "        ('typecasting', Typecasting() if step != 'typecasting' else Identity_Transformer()), \n",
    "        ('feature engineering', Feature_Engineering() if step != 'feature engineering' else Identity_Transformer()), \n",
    "        ('encoder', Encoder() if step != 'encoder' else Simplified_Encode()), \n",
    "        ('scaler', Scaler() if step != 'scaler' else Identity_Transformer()), \n",
    "        ('feature selection', Feature_Selection() if step != 'feature selection' else Identity_Transformer()),\n",
    "        ('regressor', LinearRegression() )\n",
    "    ])\n",
    "    \n",
    "    train_r2, test_r2, train_mae, test_mae, exec_time = evaluate(simplified_pipeline, X, y, cv=CV, scoring=scoring)\n",
    "    \n",
    "    results.append({\n",
    "        'Step Tested': step,\n",
    "        'Train R2': train_r2,\n",
    "        'Test R2': test_r2,\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MAE': test_mae,\n",
    "        'Execution Time': exec_time\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101a654",
   "metadata": {},
   "source": [
    "Finally, concatenate all results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf904f-e2a3-4b16-be2e-12116c2bde9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "\n",
    "results ['Overfit_Mae'] = results ['Test MAE'] / results ['Train MAE']\n",
    "\n",
    "results ['Delta'] = results ['Test MAE'] - results ['Test MAE'][0] \n",
    "\n",
    "results.to_csv(\"ablation_study.csv\", index=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77bf09",
   "metadata": {},
   "source": [
    "### **Conclusions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee444f",
   "metadata": {},
   "source": [
    "BLABLABLABALABALABALA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f03d70-d274-4d83-8462-af033351a41c",
   "metadata": {},
   "source": [
    "#\n",
    "<h2 id=\"6th-bullet\" style=\"background-color: #f7d888ff; color: #da6919ff; padding: 5px; margin: 5px;\">\n",
    "  <strong>  6. Feature Importance</strong>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257094fe",
   "metadata": {},
   "source": [
    "We will now explore the importance of each feature for the final prediction. For this, we will use our best model and the same pipeline, while skipping the feature selection step. We will fit the whole train dataset into said pipeline and use the feature_importances_ attribute from GradientBoostingRegressor as to obtain feature importance values. Corresponding feature names will be obtained from the last transformer's feats_names_ attribute which was defined within the scaler class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f24fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model parameters (w/ gradient boost) MUDAR\n",
    "params = {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 900, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 40, 'regressor__max_features': 0.6, 'regressor__max_depth': 7, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.04, 'regressor__criterion': 'squared_error'}\n",
    "\n",
    "# pipeline\n",
    "importance_pipeline = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler(scaler=RobustScaler())), # MUDAR scaler\n",
    "    ('regressor', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "importance_pipeline = importance_pipeline.set_params(**params)\n",
    "importance_pipeline = importance_pipeline.fit(X,y)\n",
    "\n",
    "feature_importance = importance_pipeline.named_steps['regressor'].feature_importances_ \n",
    "feature_names = importance_pipeline.named_steps['scaler'].feats_names_\n",
    "\n",
    "# creating a dataframe of features and corresponding importance\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "# sorting the dataframe by importance in descending order for clearer visualization\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False) \n",
    "# visualizing importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importance_df['Importance'], y=importance_df['Feature'], color=\"skyblue\")\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f6f81",
   "metadata": {},
   "source": [
    "Feature importance values given by GradientBoosterRegressor completar (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2197a44",
   "metadata": {},
   "source": [
    "We will now obtain SHAP values for each feature in order to see how it affects predictions (how much and in which direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.bool = bool\n",
    "\n",
    "# getting the previous pipeline without the last step, i.e. keeping the transformers and excluding the regressor\n",
    "preprocessor = Pipeline(importance_pipeline.steps[:-1])\n",
    "\n",
    "# applying the new pipeline to train set to get pre-processed data\n",
    "X_processed = preprocessor.transform(X)\n",
    "\n",
    "# creating a shap TreeExplainer for the regressor\n",
    "explainer = shap.TreeExplainer(importance_pipeline.named_steps['regressor'])\n",
    "\n",
    "# getting shap values by passing processed data into the explainer\n",
    "shap_values = explainer.shap_values(X_processed)\n",
    "\n",
    "shap.summary_plot(shap_values, X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = ['low', 'medium-low', 'medium-high', 'high']\n",
    "y_bins, bin_edges = pd.cut(y, bins=4, labels=y_labels, retbins=True)\n",
    "bin_edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b206a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# converting shap values with features to dataframe for convenience\n",
    "shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "\n",
    "# computing absolute mean shap value for each feature in each bin\n",
    "mean_shap_per_bin = shap_df.abs().groupby(y_bins).mean()\n",
    "\n",
    "# heatmap for visualization\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(mean_shap_per_bin.T, cmap=\"RdBu\", annot=True, fmt=\".2f\", linewidths=0.5, cbar=True, center=0)\n",
    "plt.xlabel(\"Price Categories\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Mean Feature SHAP values per Price Categories \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7efa65",
   "metadata": {},
   "source": [
    "### **Conclusions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdeaad6-4cd2-4d30-8775-843573ef996d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
