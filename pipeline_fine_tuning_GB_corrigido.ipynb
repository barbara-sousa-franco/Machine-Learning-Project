{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badc007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_function_TodasCategorias import *\n",
    "from functions_MARISA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf18f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions_MARISA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334eb522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, ElasticNet, HuberRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, median_absolute_error, root_mean_squared_error\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d1ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665d8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.set_index('carID', inplace=True)\n",
    "df_test.set_index('carID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2441abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d69aa7",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9ac63",
   "metadata": {},
   "source": [
    "We start by defining the inconsistent values discussed in the EDA as NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0536e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train['year']>2020, 'year'] = np.nan\n",
    "df_test.loc[df_test['year']>2020, 'year'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mileage']<0, 'mileage'] = np.nan\n",
    "df_test.loc[df_test['mileage']<0, 'mileage'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['tax']<0, 'tax'] = np.nan\n",
    "df_test.loc[df_test['tax']<0,'tax'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mpg']<=0, 'mpg'] = np.nan\n",
    "df_test.loc[df_test['mpg']<=0, 'mpg'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['paintQuality%']>100, 'paintQuality%'] = np.nan\n",
    "df_test.loc[df_test['paintQuality%']>100, 'paintQuality%'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['previousOwners']< 0, 'previousOwners'] = np.nan\n",
    "df_test.loc[df_test['previousOwners']< 0, 'previousOwners'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['engineSize']<= 0, 'engineSize'] = np.nan\n",
    "df_test.loc[df_test['engineSize']<= 0, 'engineSize'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mpg'] < 40, 'mpg'] = np.nan\n",
    "df_train.loc[df_train['engineSize'] < 1, 'engineSize'] = np.nan\n",
    "\n",
    "df_test.loc[df_test['mpg'] < 40, 'mpg'] = np.nan\n",
    "df_test.loc[df_test['engineSize'] < 1, 'engineSize'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345109",
   "metadata": {},
   "source": [
    "We proceed to round 'year' and 'previousOwners' to whole numbers using the floor function. Other numerical features are rounded to 2 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80085750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['year'] = np.floor(df_train['year'])\n",
    "df_train['previousOwners'] = np.floor(df_train['previousOwners'])\n",
    "\n",
    "df_test['year'] = np.floor(df_test['year'])\n",
    "df_test['previousOwners'] = np.floor(df_test['previousOwners'])\n",
    "\n",
    "for feat in ['mileage', 'tax', 'mpg', 'engineSize', 'paintQuality%']:\n",
    "    df_train[feat] = df_train[feat].round(2)\n",
    "    df_test[feat] = df_test[feat].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf0b21",
   "metadata": {},
   "source": [
    "We also pre-process the categorical variables in order to have a uniform format for later treatment (inside k-fold CV). We remove leeading and trailing spaces and uppercase all letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "454801c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing the categorical variables to be easier to find clusters in typos:\n",
    "    # remove spaces (at the beginning and end) and uppercase all letters\n",
    "    # does not replace NaN's\n",
    "df_train['Brand'] = df_train['Brand'].where(df_train['Brand'].isna(), df_train['Brand'].astype(str).str.strip().str.upper())\n",
    "df_test['Brand']  = df_test['Brand'].where(df_test['Brand'].isna(), df_test['Brand'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['model'] = df_train['model'].where(df_train['model'].isna(), df_train['model'].astype(str).str.strip().str.upper())\n",
    "df_test['model']  = df_test['model'].where(df_test['model'].isna(), df_test['model'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['fuelType'] = df_train['fuelType'].where(df_train['fuelType'].isna(), df_train['fuelType'].astype(str).str.strip().str.upper())\n",
    "df_test['fuelType']  = df_test['fuelType'].where(df_test['fuelType'].isna(), df_test['fuelType'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['transmission'] = df_train['transmission'].where(df_train['transmission'].isna(), df_train['transmission'].astype(str).str.strip().str.upper())\n",
    "df_test['transmission']  = df_test['transmission'].where(df_test['transmission'].isna(), df_test['transmission'].astype(str).str.strip().str.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eca489",
   "metadata": {},
   "source": [
    "Drop informations given by the mechanic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87da10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop ('paintQuality%', axis =1 , inplace =True)\n",
    "df_test.drop ('paintQuality%', axis =1 , inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3225b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df_train['price']\n",
    "X = df_train.drop('price', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af4260",
   "metadata": {},
   "source": [
    "Notas sobre as classes:\n",
    "\n",
    "- variáveis criadas na inicialização não acabam em _; as ue são criadas dentro dos métodos acabam em _!\n",
    "- criando uma var nos métodos, se ela não começar em self. não será reconhecida por toda a classe, será apenas local!\n",
    "- logo, iniciar com self. para criar novos atributos gerais (assim transform() cconsegue aceder ao atributo criado em fit() por exemplo)\n",
    "- cuidado com data leakage! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543515b8",
   "metadata": {},
   "source": [
    "### Categorical_Correction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5bf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Categorical_Correction(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):  # initialize the transformer\n",
    "        pass             # self it's how Python refers to the transformer within the class.\n",
    "                         # self.something --> something its an attribute of the transformer.\n",
    "\n",
    "    def fit(self, X, y=None): # fit is where we learn from the data\n",
    "        \"\"\" Learns parameters from train data to then use in categorical variable correction inside \n",
    "        transformer function of this same class.\"\"\"\n",
    "\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        ###### -----------------------------------------------------BRAND----------------------------------------------------------------------######\n",
    "\n",
    "        self.brands_ = X['Brand'].dropna().unique().tolist()\n",
    "\n",
    "        _, self.mapping_brand_ = create_clusters(X, self.brands_, 'Brand')\n",
    "\n",
    "        ######---------------------------------------------------TRANSMISSION--------------------------------------------------------------------######\n",
    "\n",
    "        self.transmission_types_ = X['transmission'].dropna().unique().tolist()\n",
    "\n",
    "        _, self.mapping_transmission_ = create_clusters(X, self.transmission_types_, 'transmission')\n",
    "\n",
    "\n",
    "        ######----------------------------------------------------FUEL TYPE--------------------------------------------------------------------######\n",
    "\n",
    "        self.fuel_types_= X['fuelType'].dropna().unique().tolist()\n",
    "\n",
    "        _, self.mapping_fueltype_ = create_clusters(X, self.fuel_types_, 'fuelType')\n",
    "\n",
    "\n",
    "        ######------------------------------------------------------MODELS----------------------------------------------------------------------######\n",
    "\n",
    "        self.models_= X['model'].dropna().unique().tolist()\n",
    "\n",
    "        ## Fuzzywuzzy wasn't able to group the same models in the column 'model', so for this case we will use get_close_matches from the difflib library.\n",
    "        self.clusters_ = similar_models(self.models_)\n",
    "\n",
    "        # Calculate counts once before the function\n",
    "        self.model_counts_ = X['model'].value_counts().to_dict()\n",
    "\n",
    "        # List to store the most frequent model in each cluster which will be considered the correct one\n",
    "        self.correct_models_ = []\n",
    "\n",
    "        #   Dictionary to map each model to its correct version\n",
    "        self.model_mapping_ = {}\n",
    "\n",
    "        # For loop to go over all the clusters \n",
    "        for group in self.clusters_:\n",
    "\n",
    "            # The best model of each cluster will be the one with the highest count in the train set, it will be added to the correct_models list\n",
    "            best = max(group, key=lambda x: self.model_counts_.get(x, 0))\n",
    "            self.correct_models_.append(best)\n",
    "\n",
    "            # Map all models in the group to the best model\n",
    "            for model in group:\n",
    "                self.model_mapping_[model] = best\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "\n",
    "    def transform(self, X): # transform is where we apply the learned parameters to correct the data\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        # BRAND----------------------------------------------------------------------------------------------\n",
    "\n",
    "        # We first correct the 'W' brands which should be either VW or BMW\n",
    "        # this correction does not depent on params learned from train data:\n",
    "        X['Brand'] = X.apply(lambda row: correct_brand_w(X, row['Brand'], row['model']),axis=1)\n",
    "        # for cars with no model where correct_brand_w doesn't work:\n",
    "        X.loc[X['Brand'] =='W' ,'Brand'] = 'VW'\n",
    "\n",
    "        # Remaining typos:\n",
    "        X['Brand_cleaned'] = X['Brand'].apply(lambda x: correct_categorical(self.mapping_brand_, x))\n",
    "\n",
    "        # TRANSMISSION---------------------------------------------------------------------------------------\n",
    "        X['transmission_cleaned'] = X['transmission'].apply(lambda x: correct_categorical(self.mapping_transmission_, x))\n",
    "\n",
    "\n",
    "        # FUELTYPE -------------------------------------------------------------------------------------------\n",
    "        X['fuelType_cleaned'] = X['fuelType'].apply(lambda x: correct_categorical(self.mapping_fueltype_, x))\n",
    "\n",
    "\n",
    "        # MODEL ---------------------------------------------------------------------------------------------\n",
    "        X['model_cleaned'] = X['model'].apply(lambda x: correct_column_model(self.correct_models_, self.model_mapping_, x, self.clusters_))\n",
    "\n",
    "\n",
    "        # Finally, we drop the variables with typos:\n",
    "        X = X.drop(['Brand', 'model', 'transmission', 'fuelType'], axis=1)\n",
    "\n",
    "\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71397d",
   "metadata": {},
   "source": [
    "### Outlier_Treatment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f2bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outlier_Treatment(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dim=3, ratio = 0.00008):\n",
    "        self.feat_lst = ['tax', 'mileage', 'mpg', 'engineSize', 'year', 'previousOwners']\n",
    "        self.dim = dim\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        self.outlier_limits_ = {}\n",
    "        self.quantiles_ = {}\n",
    "\n",
    "        for feat in self.feat_lst:\n",
    "            Q1 = X[feat].quantile(0.25)\n",
    "            Q3 = X[feat].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            self.outlier_limits_[feat] = (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
    "            self.quantiles_[feat] = (X[feat].quantile(0.01),X[feat].quantile(0.99))\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        outlier_counts = pd.Series(0, index=X.index)\n",
    "\n",
    "        for feat in self.feat_lst:\n",
    "\n",
    "            upper = self.outlier_limits_[feat][1]\n",
    "            lower = self.outlier_limits_[feat][0]\n",
    "\n",
    "            # counting in how many feats each point is an outlier \n",
    "            #outlier_counts += ((X[feat] < lower) | (X[feat] >  upper)).astype(int)\n",
    "\n",
    "\n",
    "            if X[X[feat] > upper].shape[0]/ X.shape[0] >= self.ratio:\n",
    "                X.loc[X[feat] > upper, feat] = self.quantiles_[feat][1]\n",
    "\n",
    "            if X[X[feat] < lower].shape[0]/ X.shape[0] >= self.ratio:\n",
    "                X.loc[X[feat] < lower, feat] = self.quantiles_[feat][0]\n",
    "\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fa42e",
   "metadata": {},
   "source": [
    "### Missing_Value_Treatment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "590b727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Missing_Value_Treatment(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "\n",
    "        # Missing BRAND ---------------------------------------------------------------------------------\n",
    "        self.brand_mode_ = X['Brand_cleaned'].mode().iloc[0]\n",
    "        self.model_to_brand_ = (X.dropna(subset=['Brand_cleaned', 'model_cleaned'])\n",
    "                            .groupby('model_cleaned')['Brand_cleaned']\n",
    "                            .agg(lambda x: x.mode().iloc[0])  # get most frequent brand for each model\n",
    "                            .to_dict())\n",
    "\n",
    "        # Missing MODEL ---------------------------------------------------------------------------------\n",
    "        self.model_maps_ = build_model_mappings(X)\n",
    "\n",
    "        # Missing YEAR ---------------------------------------------------------------------------------\n",
    "        self.bins_ = X['mileage'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1]).values\n",
    "        self.labels_ = ['very low', 'low', 'average', 'high', 'very high']\n",
    "        self.year_median_ = X['year'].median()\n",
    "\n",
    "        X['mileage_bin'] = pd.cut(X['mileage'], bins=self.bins_, labels=self.labels_, include_lowest=True)\n",
    "\n",
    "        self.year_map_mileage_ = (X.dropna(subset=['year', 'mileage_bin']).groupby('mileage_bin', observed=False)['year'].median().to_dict())\n",
    "\n",
    "        self.year_map_tax_ = (X.dropna(subset=['year', 'tax']).groupby('tax', observed=False)['year'].median().to_dict())\n",
    "\n",
    "        self.year_map_mpg_ = (X.dropna(subset=['year', 'mpg']).groupby('mpg', observed=False)['year'].median().to_dict())\n",
    "\n",
    "        # Missing MILEAGE ---------------------------------------------------------------------------------\n",
    "        self.mileage_map_ = (X.dropna(subset=['mileage','year']).groupby(['year'])['mileage'].median().to_dict())\n",
    "\n",
    "        # Missing TAX --------------------------------------------------------------------------------\n",
    "        self.tax_maps_ = build_tax_mappings(X)\n",
    "\n",
    "        # Missing FUELTYPE --------------------------------------------------------------------------------\n",
    "        self.fueltype_maps_ = build_fuel_mappings(X)\n",
    "\n",
    "        # Missing MPG -----------------------------------------------------------------------------------------\n",
    "        self.mpg_maps_ = build_mpg_mappings(X)\n",
    "\n",
    "        # Missing ENGINE SIZE --------------------------------------------------------------------------------\n",
    "        self.enginesize_maps_ = build_engine_mappings(X)\n",
    "\n",
    "        # Missing PREVIOUS OWNERS --------------------------------------------------------------------------------\n",
    "        self.previous_owners_maps_ = build_owners_mappings(X)\n",
    "\n",
    "        # Missing TRANSMISSION --------------------------------------------------------------------------------\n",
    "        self.transmission_maps_ = build_transmission_mappings(X)\n",
    "\n",
    "        # Missing HASDAMAGE --------------------------------------------------------------------------------\n",
    "        # We just replace NaN values with True in transform()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        if 'mileage_bin' in X.columns:\n",
    "            X = X.drop('mileage_bin', axis=1)\n",
    "\n",
    "        # Missing BRAND --------------------------------------------------------------------------------\n",
    "        X['Brand_cleaned'] = X.apply(lambda row: impute_brand(row, self.model_to_brand_, self.brand_mode_),axis=1)\n",
    "\n",
    "        # Missing MODEL --------------------------------------------------------------------------------\n",
    "        X['model_cleaned'] = X.apply(lambda row: impute_model_flexible(row, self.model_maps_),axis=1)\n",
    "\n",
    "        # Missing YEAR --------------------------------------------------------------------------------\n",
    "        X['mileage_bin'] = pd.cut(X['mileage'], bins=self.bins_, labels=self.labels_, include_lowest=True)\n",
    "\n",
    "        X['year'] = X.apply(lambda row: impute_year(row, self.year_map_mileage_, self.year_map_tax_, self.year_map_mpg_, self.year_median_),axis=1)\n",
    "\n",
    "        X = X.drop('mileage_bin', axis= 1)\n",
    "\n",
    "        # Missing MILEAGE --------------------------------------------------------------------------------\n",
    "        X['mileage'] = X.apply(lambda row: impute_mileage(row, self.mileage_map_), axis=1)\n",
    "\n",
    "        # Missing TAX --------------------------------------------------------------------------------\n",
    "        X['tax'] = X.apply(lambda row: impute_tax(row, self.tax_maps_), axis=1)\n",
    "\n",
    "        # Missing FUELTYPE --------------------------------------------------------------------------------\n",
    "        X['fuelType_cleaned'] = X.apply(lambda row: impute_fueltype(row, self.fueltype_maps_),axis=1)\n",
    "\n",
    "        # Missing MPG --------------------------------------------------------------------------------\n",
    "        X['mpg'] = X.apply(lambda row: impute_mpg(row, self.mpg_maps_), axis=1)\n",
    "\n",
    "        # Missing ENGINESIZE --------------------------------------------------------------------------------\n",
    "        X['engineSize'] = X.apply(lambda row: impute_engine(row, self.enginesize_maps_), axis=1)\n",
    "\n",
    "        # Missing PREVIOUS OWNERS --------------------------------------------------------------------------------\n",
    "        X['previousOwners'] = X.apply(lambda row: impute_owners(row, self.previous_owners_maps_), axis=1)\n",
    "\n",
    "        # Missing TRANSMISSION --------------------------------------------------------------------------------\n",
    "        X['transmission_cleaned'] = X.apply(lambda row: impute_transmission(row, self.transmission_maps_),axis=1)\n",
    "\n",
    "        # Missing HASDAMAGE --------------------------------------------------------------------------------\n",
    "        X['hasDamage'] = X['hasDamage'].fillna(True)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02573c",
   "metadata": {},
   "source": [
    "### Typecasting Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f8f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Typecasting(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        X['year'] = X['year'].astype(int)\n",
    "        X['previousOwners'] = X['previousOwners'].astype(int)\n",
    "        X['hasDamage'] = X['hasDamage'].astype(bool) \n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f45da",
   "metadata": {},
   "source": [
    "### Feature_Engineering Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8a933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Engineering(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        df_temp = X.copy()\n",
    "        df_temp['price'] = y.values\n",
    "        brand_price = df_temp.groupby('Brand_cleaned')['price'].mean().sort_values()\n",
    "        economy_limit = brand_price.quantile(0.33)\n",
    "        semi_premium_limit = brand_price.quantile(0.66)\n",
    "\n",
    "        self.segment_ = {brand: (1 if price <= economy_limit else 2 if price <= semi_premium_limit else 3) for brand, price in brand_price.items()}\n",
    "\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        # Car Age\n",
    "        X['carAge'] = (2020 - X['year']).round(0).astype(int)\n",
    "\n",
    "        # Average Car Usage\n",
    "        X['AvgUsage'] = X['mileage'] / (X['carAge'] +1)\n",
    "\n",
    "        # Car Segment based on Brand\n",
    "        X['carSegment'] = X['Brand_cleaned'].map(self.segment_)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94582e5e",
   "metadata": {},
   "source": [
    "### Encoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a735b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        # features encoded by target encoding:\n",
    "        self.target_features_ = ['model_cleaned','Brand_cleaned']\n",
    "\n",
    "        self.encoder_ = TargetEncoder(categories=\"auto\" , target_type=\"continuous\" ) \n",
    "        self.encoder_.fit(X[self.target_features_], y)\n",
    "\n",
    "        # features encoded by one-hot encoding:\n",
    "        self.one_hot_features_ = ['fuelType_cleaned', 'transmission_cleaned']\n",
    "\n",
    "        self.encoder_fueltype_ = OneHotEncoder(categories=\"auto\" , handle_unknown=\"ignore\", sparse_output=False).set_output(transform='pandas')\n",
    "        self.encoder_fueltype_.fit(X[['fuelType_cleaned']])\n",
    "\n",
    "        self.encoder_transmission_ = OneHotEncoder(categories=\"auto\" , handle_unknown=\"ignore\", sparse_output=False).set_output(transform='pandas')\n",
    "        self.encoder_transmission_.fit(X[['transmission_cleaned']])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        # target\n",
    "        X[[f'{feat}_encoded' for feat in self.target_features_]] = self.encoder_.transform(X[self.target_features_])\n",
    "\n",
    "        # one_hot\n",
    "        self.dummies_fueltype_ = self.encoder_fueltype_.transform(X[['fuelType_cleaned']])\n",
    "        self.dummies_transmission_ = self.encoder_transmission_.transform(X[['transmission_cleaned']])\n",
    "\n",
    "        X = pd.concat([X, self.dummies_fueltype_, self.dummies_transmission_], axis=1)\n",
    "\n",
    "        X = X.drop(['Brand_cleaned', 'transmission_cleaned', 'fuelType_cleaned','model_cleaned'], axis=1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33071c",
   "metadata": {},
   "source": [
    "### Scaler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627871a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, scaler=StandardScaler()):\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        self.numeric_features_ = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        for feat in [c for c in self.numeric_features_ if c.startswith(\"fuelType\")]:\n",
    "            if feat in self.numeric_features_:\n",
    "                self.numeric_features_.remove(feat)\n",
    "\n",
    "        for feat in [c for c in self.numeric_features_ if c.startswith(\"transmission\")]:\n",
    "            if feat in self.numeric_features_:\n",
    "                self.numeric_features_.remove(feat)\n",
    "        \n",
    "        self.scaler.fit(X[self.numeric_features_])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X = X.copy()\n",
    "        self.feature_names_in_ = X.columns\n",
    "\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        \n",
    "        X[self.numeric_features_] = self.scaler.transform(X[self.numeric_features_])\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2233eb",
   "metadata": {},
   "source": [
    "### Pre-processing complete: stack everything inside pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4cfe46",
   "metadata": {},
   "source": [
    "### Feature_Selection Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d0ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Selection(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, rfe_options=[8, 10], spearman_options=[0.2, 0.25, 0.3],var_threshold=0.01):\n",
    "\n",
    "        self.rfe_options = rfe_options\n",
    "        self.spearman_options = spearman_options\n",
    "        self.var_threshold = var_threshold\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "\n",
    "        X['hasDamage'] = X['hasDamage'].astype(int)\n",
    "\n",
    "        # From numerical features we keep only the ones with variance != 0 (non-constant)\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        non_constant_features = [f for f in numeric_features if X[f].var() > 0]\n",
    "\n",
    "        results = []  # to store all combinations and MAE\n",
    "        mae_scores = []\n",
    "\n",
    "        features = [f for f in non_constant_features if X[f].var() > self.var_threshold]\n",
    "\n",
    "        # Feature Importance\n",
    "        tree = DecisionTreeRegressor()\n",
    "        tree.fit(X = X[features], y= y)\n",
    "        gini_importance = tree.feature_importances_\n",
    "        selected_gini = [f for f, imp in zip (features, gini_importance) if imp > (1 / len(features))]\n",
    "\n",
    "        # Loop over all parameter combinations\n",
    "        for n_feats in self.rfe_options:\n",
    "            for spearman_thr in self.spearman_options:\n",
    "                mae_scores = []\n",
    "\n",
    "                # Compute absolute Spearman correlation with target\n",
    "                features_fold = X[features]\n",
    "                corr_with_target = features_fold.apply(lambda x: x.corr(y, method='spearman'))\n",
    "                selected_spearman = corr_with_target[abs(corr_with_target) > spearman_thr].index.tolist() # each index is a feature\n",
    "                \n",
    "                # Wrapper method: RFE with Linear Regression\n",
    "                model = LinearRegression()\n",
    "                rfe_lr = RFE(estimator=model, n_features_to_select=n_feats)\n",
    "                rfe_lr.fit(X= X[features], y= y)\n",
    "                rfe_lr_features = pd.Series(rfe_lr.support_, index=features)\n",
    "                rfe_lr_features_list = rfe_lr_features[rfe_lr_features].index.tolist() # only chooses the features where RFE selected True\n",
    "\n",
    "\n",
    "                # Majority vote: keep features that appear in more than or at least half of the methods\n",
    "                feature_counts = {}\n",
    "                for method in [selected_spearman, rfe_lr_features_list, selected_gini]:\n",
    "                    for f in method:\n",
    "                        feature_counts[f] = feature_counts.get(f,0)+1\n",
    "                n_methods = len([selected_spearman, rfe_lr_features_list, selected_gini])\n",
    "                threshold = n_methods//2 + n_methods%2\n",
    "                final_features = [f for f,count in feature_counts.items() if count >= threshold]\n",
    "\n",
    "                # Evaluate performance with selected features\n",
    "                model = LinearRegression()\n",
    "                model.fit(X[final_features], y)\n",
    "                y_pred = model.predict(X[final_features])\n",
    "                mae = mean_absolute_error(y, y_pred)\n",
    "                mae_scores.append(mae)\n",
    "                results.append({'features': final_features, 'mae': mae})\n",
    "\n",
    "        # Select combination with lowest MAE\n",
    "        self.best_ = min(results, key=lambda x: x['mae'])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        selected_features = self.best_['features']\n",
    "\n",
    "        X = X[selected_features]\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732ce2e",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc4c7c3",
   "metadata": {},
   "source": [
    "Ao fazer apenas categorial = Categorical Correction(), por exemplo, estamos a criar um objeto, não são aplicadas nenhumas transformações, nem é aprendido nada.\n",
    "\n",
    "Ao fazer categorical.fit(X_train) estamos a aprender os parâmetros.\n",
    "\n",
    "Ao fazer categorical.transform(X_train) são efetivamente feitas as alterações com base nas regras aprendidas em .fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31645d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.9,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'squared_error',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fdfa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  #using just feature selection\n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), \n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42 ) )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea32ecd",
   "metadata": {},
   "source": [
    "\n",
    "- Gradient Boosting is a machine learning technique that builds predictive models in a sequential manner by combining the predictions of multiple weak learners (typically decision trees) in an ensemble --> iteratively improving upon the weaknesses of previous models.\n",
    "    - In each iteration, a new weak learner (decision tree) is trained to predict the residuals (the differences between the actual and predicted values) of the ensemble model constructed so far. The new weak learner is trained to minimize the residual error, typically using a technique like gradient descent. The predictions of this new model are then added to the ensemble, updating the overall prediction.\n",
    "    - Regularization: shrinkage (learning rate), limiting the depth of trees (tree depth), and stochastic gradient boosting (randomly subsampling the training data).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820757",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ada50733",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "\n",
    "param_distributions = {\n",
    "\n",
    "    # The number of features to consider when looking for the best split\n",
    "    'regressor__max_features': [None, 'sqrt', 'log2', 0.2, 0.3, 0.5],\n",
    "\n",
    "    # Loss function to be optimized\n",
    "    'regressor__loss': ['absolute_error', 'huber', 'quantile'],\n",
    "\n",
    "    #The number of trees in the forest\n",
    "    'regressor__n_estimators': [400, 700, 1000],\n",
    "\n",
    "    # The maximum depth of the tree\n",
    "    'regressor__max_depth': [2, 3, 4, 5, 6, 7],    \n",
    "\n",
    "    #The learning rate, the size of the steps to reach a minimum\n",
    "    'regressor__learning_rate': [0.03, 0.05, 0.1],\n",
    "\n",
    "    # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'regressor__subsample': [0.6, 0.7, 0.8],\n",
    "\n",
    "    # The minimum number of samples required to split an internal node\n",
    "    'regressor__min_samples_split': [5, 8, 10], \n",
    "\n",
    "    # The minimum number of samples required to be at a leaf node\n",
    "    'regressor__min_samples_leaf': [3, 4, 5],\n",
    "    \n",
    "    # The function to measure the quality of a split\n",
    "    'regressor__criterion': ['squared_error'],\n",
    "\n",
    "    # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'regressor__min_impurity_decrease': [0.0, 0.001, 0.0001, 0.0005],\n",
    "\n",
    "    # The maximum number of terminal nodes in a tree\n",
    "    'regressor__max_leaf_nodes': [None, 15, 20, 30],\n",
    "\n",
    "    # Stop training when the validation score is not improving (early stopping)\n",
    "    'regressor__n_iter_no_change': [None, 10, 15, 20],\n",
    "\n",
    "    # The proportion of training data to set aside as validation set for early stopping\n",
    "     'regressor__validation_fraction': [0.1, 0.15, 0.2],\n",
    "}\n",
    "\n",
    "# Making an adjusted R2 function:\n",
    "\n",
    "def adjusted_r2_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    n, p = X.shape\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1) # erros\n",
    "\n",
    "adj_r2 = make_scorer(adjusted_r2_scorer, greater_is_better=True) #erros\n",
    "\n",
    "\n",
    "scoring = { 'R2': 'r2', #'AdjR2': adj_r2 -> esta funcao e dificil de implementar, erros\n",
    "    'MAE': 'neg_mean_absolute_error',\n",
    "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "    'MedAE': 'neg_median_absolute_error',\n",
    "    'RMSE': 'neg_root_mean_squared_error'}\n",
    "\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline, # estimator is the model to be optimized\n",
    "                        # in this case RandomizedSearchCV will test parameter options for the regressor but performing all the preprocessing steps\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=40,             \n",
    "    scoring=scoring, # evaluation metrics\n",
    "    refit = 'MAE',\n",
    "    cv=5,                 # 10-fold CV -> mudei para 5 \n",
    "    verbose=2, # to show iterations\n",
    "    return_train_score=True, # to return train metric results in cv_scores_\n",
    "    random_state=random_state, # defined on top of the nb\n",
    "    n_jobs=-1 # to use all cpu cores for faster results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1055e41",
   "metadata": {},
   "source": [
    "### Running RnadomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd939715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.5, 'regressor__max_depth': 7, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "Best CV score: -1477.1446\n"
     ]
    }
   ],
   "source": [
    "print(\"Running RandomizedSearchCV with Pipeline...\")\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "#test_score_pipeline = random_search.score(X_test, y_test)\n",
    "#print(f\"Test score: {test_score_pipeline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0ca741b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <th>std_train_MAE</th>\n",
       "      <th>mean_train_R2</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>std_train_R2</th>\n",
       "      <th>mean_train_MAPE</th>\n",
       "      <th>mean_test_MAPE</th>\n",
       "      <th>std_train_MAPE</th>\n",
       "      <th>mean_train_MedAE</th>\n",
       "      <th>mean_test_MedAE</th>\n",
       "      <th>std_train_MedAE</th>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <th>std_train_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1902.709170</td>\n",
       "      <td>-1913.821556</td>\n",
       "      <td>6.550780</td>\n",
       "      <td>0.868193</td>\n",
       "      <td>0.866699</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>-0.116381</td>\n",
       "      <td>-0.117183</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-1195.951489</td>\n",
       "      <td>-1202.514895</td>\n",
       "      <td>4.451864</td>\n",
       "      <td>-3534.804031</td>\n",
       "      <td>-3552.345531</td>\n",
       "      <td>31.216914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-1379.108161</td>\n",
       "      <td>-1491.090617</td>\n",
       "      <td>2.816303</td>\n",
       "      <td>0.934794</td>\n",
       "      <td>0.922143</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>-0.086472</td>\n",
       "      <td>-0.091977</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-891.756292</td>\n",
       "      <td>-940.195702</td>\n",
       "      <td>1.302036</td>\n",
       "      <td>-2485.924302</td>\n",
       "      <td>-2708.034816</td>\n",
       "      <td>39.910314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-2606.440050</td>\n",
       "      <td>-2696.516823</td>\n",
       "      <td>24.798812</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>0.823049</td>\n",
       "      <td>0.005138</td>\n",
       "      <td>-0.171500</td>\n",
       "      <td>-0.176530</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>-1812.472355</td>\n",
       "      <td>-1866.624640</td>\n",
       "      <td>17.523903</td>\n",
       "      <td>-3928.626013</td>\n",
       "      <td>-4093.137520</td>\n",
       "      <td>57.948536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1552.968773</td>\n",
       "      <td>-1589.626143</td>\n",
       "      <td>5.226881</td>\n",
       "      <td>0.903731</td>\n",
       "      <td>0.900233</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>-0.092636</td>\n",
       "      <td>-0.095147</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>-934.902157</td>\n",
       "      <td>-959.078798</td>\n",
       "      <td>2.310459</td>\n",
       "      <td>-3020.409363</td>\n",
       "      <td>-3071.632832</td>\n",
       "      <td>59.300114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-3545.216016</td>\n",
       "      <td>-3555.931597</td>\n",
       "      <td>12.034597</td>\n",
       "      <td>0.756376</td>\n",
       "      <td>0.754466</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>-0.281879</td>\n",
       "      <td>-0.282689</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>-2841.103502</td>\n",
       "      <td>-2850.709199</td>\n",
       "      <td>4.526609</td>\n",
       "      <td>-4805.804560</td>\n",
       "      <td>-4822.594066</td>\n",
       "      <td>29.666644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-3514.197285</td>\n",
       "      <td>-3525.774277</td>\n",
       "      <td>20.860812</td>\n",
       "      <td>0.746354</td>\n",
       "      <td>0.744054</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>-0.252532</td>\n",
       "      <td>-0.253389</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>-2653.028558</td>\n",
       "      <td>-2652.473796</td>\n",
       "      <td>15.640689</td>\n",
       "      <td>-4903.531204</td>\n",
       "      <td>-4923.964820</td>\n",
       "      <td>42.781725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1602.653524</td>\n",
       "      <td>-1635.268952</td>\n",
       "      <td>3.087937</td>\n",
       "      <td>0.908688</td>\n",
       "      <td>0.904805</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>-0.098338</td>\n",
       "      <td>-0.100264</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-1021.007872</td>\n",
       "      <td>-1040.014289</td>\n",
       "      <td>2.593244</td>\n",
       "      <td>-2941.671697</td>\n",
       "      <td>-2997.235170</td>\n",
       "      <td>56.040775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-3571.060371</td>\n",
       "      <td>-3580.978788</td>\n",
       "      <td>14.139140</td>\n",
       "      <td>0.754733</td>\n",
       "      <td>0.752646</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>-0.285260</td>\n",
       "      <td>-0.286034</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>-2881.675519</td>\n",
       "      <td>-2893.635645</td>\n",
       "      <td>10.847789</td>\n",
       "      <td>-4821.894551</td>\n",
       "      <td>-4840.473043</td>\n",
       "      <td>40.545631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1348.259744</td>\n",
       "      <td>-1477.144631</td>\n",
       "      <td>15.687798</td>\n",
       "      <td>0.938741</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>-0.084808</td>\n",
       "      <td>-0.091325</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>-873.906531</td>\n",
       "      <td>-928.400208</td>\n",
       "      <td>9.591919</td>\n",
       "      <td>-2409.085738</td>\n",
       "      <td>-2668.511222</td>\n",
       "      <td>55.045667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-1745.815522</td>\n",
       "      <td>-1762.693825</td>\n",
       "      <td>4.601109</td>\n",
       "      <td>0.882891</td>\n",
       "      <td>0.881166</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>-0.104591</td>\n",
       "      <td>-0.105791</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>-1066.551951</td>\n",
       "      <td>-1079.186879</td>\n",
       "      <td>2.913971</td>\n",
       "      <td>-3331.854407</td>\n",
       "      <td>-3353.694658</td>\n",
       "      <td>33.522781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-4008.971569</td>\n",
       "      <td>-4014.829745</td>\n",
       "      <td>19.796797</td>\n",
       "      <td>0.704523</td>\n",
       "      <td>0.702808</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>-0.334815</td>\n",
       "      <td>-0.335063</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>-3384.328381</td>\n",
       "      <td>-3383.853679</td>\n",
       "      <td>22.977369</td>\n",
       "      <td>-5292.657932</td>\n",
       "      <td>-5305.351942</td>\n",
       "      <td>34.654544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-3954.561361</td>\n",
       "      <td>-3966.941517</td>\n",
       "      <td>21.897217</td>\n",
       "      <td>0.718541</td>\n",
       "      <td>0.716249</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>-0.335624</td>\n",
       "      <td>-0.336623</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>-3424.547990</td>\n",
       "      <td>-3430.105496</td>\n",
       "      <td>10.933167</td>\n",
       "      <td>-5165.502636</td>\n",
       "      <td>-5184.691824</td>\n",
       "      <td>37.101987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1905.795401</td>\n",
       "      <td>-1916.147168</td>\n",
       "      <td>3.349821</td>\n",
       "      <td>0.867526</td>\n",
       "      <td>0.866124</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>-0.116616</td>\n",
       "      <td>-0.117440</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>-1201.023075</td>\n",
       "      <td>-1209.486617</td>\n",
       "      <td>7.406479</td>\n",
       "      <td>-3543.838874</td>\n",
       "      <td>-3559.637373</td>\n",
       "      <td>20.066147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-3762.809486</td>\n",
       "      <td>-3769.803248</td>\n",
       "      <td>69.187174</td>\n",
       "      <td>0.721537</td>\n",
       "      <td>0.720156</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>-0.291227</td>\n",
       "      <td>-0.292011</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>-2926.224678</td>\n",
       "      <td>-2928.556706</td>\n",
       "      <td>28.837686</td>\n",
       "      <td>-5137.323707</td>\n",
       "      <td>-5147.073455</td>\n",
       "      <td>91.728958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-3922.471276</td>\n",
       "      <td>-3931.204313</td>\n",
       "      <td>15.716453</td>\n",
       "      <td>0.725111</td>\n",
       "      <td>0.723486</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>-0.338726</td>\n",
       "      <td>-0.339304</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>-3447.430888</td>\n",
       "      <td>-3449.378951</td>\n",
       "      <td>29.975163</td>\n",
       "      <td>-5104.852939</td>\n",
       "      <td>-5118.429727</td>\n",
       "      <td>32.318653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-1575.725157</td>\n",
       "      <td>-1610.541528</td>\n",
       "      <td>4.250632</td>\n",
       "      <td>0.910527</td>\n",
       "      <td>0.906175</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>-0.096538</td>\n",
       "      <td>-0.098650</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>-1004.202514</td>\n",
       "      <td>-1016.998942</td>\n",
       "      <td>2.617098</td>\n",
       "      <td>-2911.729856</td>\n",
       "      <td>-2975.908256</td>\n",
       "      <td>61.789670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1666.335907</td>\n",
       "      <td>-1689.145668</td>\n",
       "      <td>4.493318</td>\n",
       "      <td>0.894689</td>\n",
       "      <td>0.892496</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>-0.100364</td>\n",
       "      <td>-0.102081</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>-1020.721238</td>\n",
       "      <td>-1036.546596</td>\n",
       "      <td>3.264020</td>\n",
       "      <td>-3159.378102</td>\n",
       "      <td>-3188.629165</td>\n",
       "      <td>44.789376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-3743.861061</td>\n",
       "      <td>-3753.733342</td>\n",
       "      <td>26.297480</td>\n",
       "      <td>0.735489</td>\n",
       "      <td>0.733409</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>-0.296593</td>\n",
       "      <td>-0.297299</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>-3021.988641</td>\n",
       "      <td>-3025.722782</td>\n",
       "      <td>42.616535</td>\n",
       "      <td>-5007.483863</td>\n",
       "      <td>-5025.344557</td>\n",
       "      <td>46.131940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-3110.088152</td>\n",
       "      <td>-3135.679393</td>\n",
       "      <td>12.126529</td>\n",
       "      <td>0.786428</td>\n",
       "      <td>0.782095</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>-0.207627</td>\n",
       "      <td>-0.209211</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>-2243.831241</td>\n",
       "      <td>-2258.587771</td>\n",
       "      <td>5.917074</td>\n",
       "      <td>-4499.539915</td>\n",
       "      <td>-4541.209404</td>\n",
       "      <td>44.050626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-3933.165828</td>\n",
       "      <td>-3942.725428</td>\n",
       "      <td>21.311740</td>\n",
       "      <td>0.723657</td>\n",
       "      <td>0.721665</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>-0.339538</td>\n",
       "      <td>-0.340256</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>-3457.164536</td>\n",
       "      <td>-3463.061867</td>\n",
       "      <td>31.716033</td>\n",
       "      <td>-5118.324254</td>\n",
       "      <td>-5135.141430</td>\n",
       "      <td>23.848848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1597.331609</td>\n",
       "      <td>-1632.045933</td>\n",
       "      <td>4.603584</td>\n",
       "      <td>0.907248</td>\n",
       "      <td>0.902831</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>-0.097766</td>\n",
       "      <td>-0.099850</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>-1013.915304</td>\n",
       "      <td>-1030.697056</td>\n",
       "      <td>3.546302</td>\n",
       "      <td>-2964.796514</td>\n",
       "      <td>-3029.265116</td>\n",
       "      <td>54.341476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1850.499124</td>\n",
       "      <td>-1864.331754</td>\n",
       "      <td>4.910331</td>\n",
       "      <td>0.883376</td>\n",
       "      <td>0.881097</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>-0.115725</td>\n",
       "      <td>-0.116671</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-1205.005456</td>\n",
       "      <td>-1212.849936</td>\n",
       "      <td>5.115804</td>\n",
       "      <td>-3324.893167</td>\n",
       "      <td>-3353.731076</td>\n",
       "      <td>37.677925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1555.102083</td>\n",
       "      <td>-1596.788141</td>\n",
       "      <td>6.450063</td>\n",
       "      <td>0.907995</td>\n",
       "      <td>0.904473</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>-0.093419</td>\n",
       "      <td>-0.096197</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>-944.546413</td>\n",
       "      <td>-978.581322</td>\n",
       "      <td>4.164586</td>\n",
       "      <td>-2952.937090</td>\n",
       "      <td>-3002.772624</td>\n",
       "      <td>47.160133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-2693.309457</td>\n",
       "      <td>-2765.991742</td>\n",
       "      <td>30.198036</td>\n",
       "      <td>0.830257</td>\n",
       "      <td>0.819418</td>\n",
       "      <td>0.006258</td>\n",
       "      <td>-0.177925</td>\n",
       "      <td>-0.182251</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>-1889.142512</td>\n",
       "      <td>-1932.669492</td>\n",
       "      <td>14.145415</td>\n",
       "      <td>-4010.862596</td>\n",
       "      <td>-4134.165568</td>\n",
       "      <td>73.062092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-4012.487187</td>\n",
       "      <td>-4023.471361</td>\n",
       "      <td>41.622382</td>\n",
       "      <td>0.708037</td>\n",
       "      <td>0.706359</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>-0.339661</td>\n",
       "      <td>-0.340200</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>-3425.922896</td>\n",
       "      <td>-3432.434850</td>\n",
       "      <td>36.884425</td>\n",
       "      <td>-5260.719946</td>\n",
       "      <td>-5275.075827</td>\n",
       "      <td>51.476336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-3969.409592</td>\n",
       "      <td>-3977.556314</td>\n",
       "      <td>20.745404</td>\n",
       "      <td>0.717048</td>\n",
       "      <td>0.715686</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.342330</td>\n",
       "      <td>-0.342845</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>-3462.093230</td>\n",
       "      <td>-3474.636215</td>\n",
       "      <td>26.720797</td>\n",
       "      <td>-5179.164496</td>\n",
       "      <td>-5190.201682</td>\n",
       "      <td>30.419003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-2784.048126</td>\n",
       "      <td>-2833.882150</td>\n",
       "      <td>10.351194</td>\n",
       "      <td>0.824294</td>\n",
       "      <td>0.816583</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>-0.185349</td>\n",
       "      <td>-0.188272</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>-1979.866573</td>\n",
       "      <td>-2010.898048</td>\n",
       "      <td>5.126321</td>\n",
       "      <td>-4081.224795</td>\n",
       "      <td>-4165.995067</td>\n",
       "      <td>37.153410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1380.253487</td>\n",
       "      <td>-1486.209582</td>\n",
       "      <td>3.733821</td>\n",
       "      <td>0.932345</td>\n",
       "      <td>0.920603</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>-0.085304</td>\n",
       "      <td>-0.091372</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-882.050244</td>\n",
       "      <td>-934.610534</td>\n",
       "      <td>1.175843</td>\n",
       "      <td>-2531.583102</td>\n",
       "      <td>-2734.664551</td>\n",
       "      <td>67.453985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-3192.259219</td>\n",
       "      <td>-3216.476065</td>\n",
       "      <td>8.700915</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.786985</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>-0.233625</td>\n",
       "      <td>-0.235049</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>-2444.142646</td>\n",
       "      <td>-2453.496552</td>\n",
       "      <td>12.739422</td>\n",
       "      <td>-4441.863696</td>\n",
       "      <td>-4490.432637</td>\n",
       "      <td>36.599265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1849.777603</td>\n",
       "      <td>-1862.768820</td>\n",
       "      <td>3.137485</td>\n",
       "      <td>0.875602</td>\n",
       "      <td>0.873902</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>-0.112688</td>\n",
       "      <td>-0.113622</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-1158.483697</td>\n",
       "      <td>-1161.768014</td>\n",
       "      <td>1.745900</td>\n",
       "      <td>-3434.089240</td>\n",
       "      <td>-3454.230079</td>\n",
       "      <td>25.901896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-1610.123547</td>\n",
       "      <td>-1637.726630</td>\n",
       "      <td>3.237374</td>\n",
       "      <td>0.899884</td>\n",
       "      <td>0.896947</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>-0.096294</td>\n",
       "      <td>-0.098179</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>-978.616907</td>\n",
       "      <td>-996.107489</td>\n",
       "      <td>4.951785</td>\n",
       "      <td>-3080.564641</td>\n",
       "      <td>-3120.686926</td>\n",
       "      <td>38.999209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1704.887276</td>\n",
       "      <td>-1724.970836</td>\n",
       "      <td>6.203267</td>\n",
       "      <td>0.898093</td>\n",
       "      <td>0.895257</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>-0.105294</td>\n",
       "      <td>-0.106606</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-1097.642271</td>\n",
       "      <td>-1106.132649</td>\n",
       "      <td>4.593574</td>\n",
       "      <td>-3107.756425</td>\n",
       "      <td>-3146.155261</td>\n",
       "      <td>53.345489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1648.504353</td>\n",
       "      <td>-1672.793973</td>\n",
       "      <td>6.883079</td>\n",
       "      <td>0.897162</td>\n",
       "      <td>0.894414</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>-0.098857</td>\n",
       "      <td>-0.100386</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>-1006.228603</td>\n",
       "      <td>-1024.551891</td>\n",
       "      <td>2.949607</td>\n",
       "      <td>-3122.232061</td>\n",
       "      <td>-3159.025131</td>\n",
       "      <td>36.105847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-1428.659816</td>\n",
       "      <td>-1505.447556</td>\n",
       "      <td>3.884098</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.919638</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>-0.088504</td>\n",
       "      <td>-0.092686</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>-917.807450</td>\n",
       "      <td>-950.416247</td>\n",
       "      <td>1.679472</td>\n",
       "      <td>-2586.243194</td>\n",
       "      <td>-2751.181578</td>\n",
       "      <td>51.152137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1583.635557</td>\n",
       "      <td>-1619.411416</td>\n",
       "      <td>4.717537</td>\n",
       "      <td>0.911435</td>\n",
       "      <td>0.906689</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>-0.097893</td>\n",
       "      <td>-0.099889</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>-1019.646023</td>\n",
       "      <td>-1031.529164</td>\n",
       "      <td>1.583949</td>\n",
       "      <td>-2896.914953</td>\n",
       "      <td>-2967.493400</td>\n",
       "      <td>60.132552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'regressor__validation_fraction': 0.15, 'regr...</td>\n",
       "      <td>-1728.183686</td>\n",
       "      <td>-1747.444044</td>\n",
       "      <td>4.973728</td>\n",
       "      <td>0.882778</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>-0.103259</td>\n",
       "      <td>-0.104548</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-1050.571603</td>\n",
       "      <td>-1064.488708</td>\n",
       "      <td>2.665412</td>\n",
       "      <td>-3333.391926</td>\n",
       "      <td>-3357.881736</td>\n",
       "      <td>37.434358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1378.087152</td>\n",
       "      <td>-1482.922437</td>\n",
       "      <td>3.001448</td>\n",
       "      <td>0.931465</td>\n",
       "      <td>0.920142</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>-0.085048</td>\n",
       "      <td>-0.091014</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>-876.777584</td>\n",
       "      <td>-926.898086</td>\n",
       "      <td>1.887976</td>\n",
       "      <td>-2548.435826</td>\n",
       "      <td>-2742.798890</td>\n",
       "      <td>49.774321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'regressor__validation_fraction': 0.2, 'regre...</td>\n",
       "      <td>-1530.335464</td>\n",
       "      <td>-1576.445822</td>\n",
       "      <td>4.049921</td>\n",
       "      <td>0.916225</td>\n",
       "      <td>0.910109</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>-0.094099</td>\n",
       "      <td>-0.096758</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>-981.298072</td>\n",
       "      <td>-997.771491</td>\n",
       "      <td>2.599228</td>\n",
       "      <td>-2817.371072</td>\n",
       "      <td>-2911.792644</td>\n",
       "      <td>65.934522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1623.583484</td>\n",
       "      <td>-1651.397045</td>\n",
       "      <td>4.569530</td>\n",
       "      <td>0.904471</td>\n",
       "      <td>0.901043</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>-0.099116</td>\n",
       "      <td>-0.101070</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>-1035.722493</td>\n",
       "      <td>-1044.971619</td>\n",
       "      <td>2.365142</td>\n",
       "      <td>-3008.932844</td>\n",
       "      <td>-3057.527813</td>\n",
       "      <td>53.751045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'regressor__validation_fraction': 0.1, 'regre...</td>\n",
       "      <td>-1922.222031</td>\n",
       "      <td>-1933.146456</td>\n",
       "      <td>10.448014</td>\n",
       "      <td>0.865881</td>\n",
       "      <td>0.864476</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.116835</td>\n",
       "      <td>-0.117619</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-1200.067101</td>\n",
       "      <td>-1206.311729</td>\n",
       "      <td>3.564783</td>\n",
       "      <td>-3565.667344</td>\n",
       "      <td>-3581.899897</td>\n",
       "      <td>39.958084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_train_MAE  \\\n",
       "0   {'regressor__validation_fraction': 0.2, 'regre...    -1902.709170   \n",
       "1   {'regressor__validation_fraction': 0.15, 'regr...    -1379.108161   \n",
       "2   {'regressor__validation_fraction': 0.15, 'regr...    -2606.440050   \n",
       "3   {'regressor__validation_fraction': 0.2, 'regre...    -1552.968773   \n",
       "4   {'regressor__validation_fraction': 0.15, 'regr...    -3545.216016   \n",
       "5   {'regressor__validation_fraction': 0.15, 'regr...    -3514.197285   \n",
       "6   {'regressor__validation_fraction': 0.2, 'regre...    -1602.653524   \n",
       "7   {'regressor__validation_fraction': 0.2, 'regre...    -3571.060371   \n",
       "8   {'regressor__validation_fraction': 0.2, 'regre...    -1348.259744   \n",
       "9   {'regressor__validation_fraction': 0.15, 'regr...    -1745.815522   \n",
       "10  {'regressor__validation_fraction': 0.2, 'regre...    -4008.971569   \n",
       "11  {'regressor__validation_fraction': 0.2, 'regre...    -3954.561361   \n",
       "12  {'regressor__validation_fraction': 0.2, 'regre...    -1905.795401   \n",
       "13  {'regressor__validation_fraction': 0.15, 'regr...    -3762.809486   \n",
       "14  {'regressor__validation_fraction': 0.1, 'regre...    -3922.471276   \n",
       "15  {'regressor__validation_fraction': 0.15, 'regr...    -1575.725157   \n",
       "16  {'regressor__validation_fraction': 0.2, 'regre...    -1666.335907   \n",
       "17  {'regressor__validation_fraction': 0.15, 'regr...    -3743.861061   \n",
       "18  {'regressor__validation_fraction': 0.2, 'regre...    -3110.088152   \n",
       "19  {'regressor__validation_fraction': 0.15, 'regr...    -3933.165828   \n",
       "20  {'regressor__validation_fraction': 0.1, 'regre...    -1597.331609   \n",
       "21  {'regressor__validation_fraction': 0.1, 'regre...    -1850.499124   \n",
       "22  {'regressor__validation_fraction': 0.1, 'regre...    -1555.102083   \n",
       "23  {'regressor__validation_fraction': 0.2, 'regre...    -2693.309457   \n",
       "24  {'regressor__validation_fraction': 0.2, 'regre...    -4012.487187   \n",
       "25  {'regressor__validation_fraction': 0.1, 'regre...    -3969.409592   \n",
       "26  {'regressor__validation_fraction': 0.1, 'regre...    -2784.048126   \n",
       "27  {'regressor__validation_fraction': 0.2, 'regre...    -1380.253487   \n",
       "28  {'regressor__validation_fraction': 0.1, 'regre...    -3192.259219   \n",
       "29  {'regressor__validation_fraction': 0.1, 'regre...    -1849.777603   \n",
       "30  {'regressor__validation_fraction': 0.15, 'regr...    -1610.123547   \n",
       "31  {'regressor__validation_fraction': 0.1, 'regre...    -1704.887276   \n",
       "32  {'regressor__validation_fraction': 0.1, 'regre...    -1648.504353   \n",
       "33  {'regressor__validation_fraction': 0.15, 'regr...    -1428.659816   \n",
       "34  {'regressor__validation_fraction': 0.2, 'regre...    -1583.635557   \n",
       "35  {'regressor__validation_fraction': 0.15, 'regr...    -1728.183686   \n",
       "36  {'regressor__validation_fraction': 0.1, 'regre...    -1378.087152   \n",
       "37  {'regressor__validation_fraction': 0.2, 'regre...    -1530.335464   \n",
       "38  {'regressor__validation_fraction': 0.1, 'regre...    -1623.583484   \n",
       "39  {'regressor__validation_fraction': 0.1, 'regre...    -1922.222031   \n",
       "\n",
       "    mean_test_MAE  std_train_MAE  mean_train_R2  mean_test_R2  std_train_R2  \\\n",
       "0    -1913.821556       6.550780       0.868193      0.866699      0.002444   \n",
       "1    -1491.090617       2.816303       0.934794      0.922143      0.002232   \n",
       "2    -2696.516823      24.798812       0.837155      0.823049      0.005138   \n",
       "3    -1589.626143       5.226881       0.903731      0.900233      0.003891   \n",
       "4    -3555.931597      12.034597       0.756376      0.754466      0.003173   \n",
       "5    -3525.774277      20.860812       0.746354      0.744054      0.004725   \n",
       "6    -1635.268952       3.087937       0.908688      0.904805      0.003551   \n",
       "7    -3580.978788      14.139140       0.754733      0.752646      0.004311   \n",
       "8    -1477.144631      15.687798       0.938741      0.924354      0.002992   \n",
       "9    -1762.693825       4.601109       0.882891      0.881166      0.002462   \n",
       "10   -4014.829745      19.796797       0.704523      0.702808      0.003318   \n",
       "11   -3966.941517      21.897217       0.718541      0.716249      0.004058   \n",
       "12   -1916.147168       3.349821       0.867526      0.866124      0.001579   \n",
       "13   -3769.803248      69.187174       0.721537      0.720156      0.009730   \n",
       "14   -3931.204313      15.716453       0.725111      0.723486      0.003817   \n",
       "15   -1610.541528       4.250632       0.910527      0.906175      0.003903   \n",
       "16   -1689.145668       4.493318       0.894689      0.892496      0.003102   \n",
       "17   -3753.733342      26.297480       0.735489      0.733409      0.004916   \n",
       "18   -3135.679393      12.126529       0.786428      0.782095      0.004179   \n",
       "19   -3942.725428      21.311740       0.723657      0.721665      0.003561   \n",
       "20   -1632.045933       4.603584       0.907248      0.902831      0.003509   \n",
       "21   -1864.331754       4.910331       0.883376      0.881097      0.002760   \n",
       "22   -1596.788141       6.450063       0.907995      0.904473      0.003089   \n",
       "23   -2765.991742      30.198036       0.830257      0.819418      0.006258   \n",
       "24   -4023.471361      41.622382       0.708037      0.706359      0.006634   \n",
       "25   -3977.556314      20.745404       0.717048      0.715686      0.003933   \n",
       "26   -2833.882150      10.351194       0.824294      0.816583      0.003269   \n",
       "27   -1486.209582       3.733821       0.932345      0.920603      0.003654   \n",
       "28   -3216.476065       8.700915       0.791874      0.786985      0.003427   \n",
       "29   -1862.768820       3.137485       0.875602      0.873902      0.001879   \n",
       "30   -1637.726630       3.237374       0.899884      0.896947      0.002601   \n",
       "31   -1724.970836       6.203267       0.898093      0.895257      0.003592   \n",
       "32   -1672.793973       6.883079       0.897162      0.894414      0.002347   \n",
       "33   -1505.447556       3.884098       0.929417      0.919638      0.002862   \n",
       "34   -1619.411416       4.717537       0.911435      0.906689      0.003829   \n",
       "35   -1747.444044       4.973728       0.882778      0.880869      0.002806   \n",
       "36   -1482.922437       3.001448       0.931465      0.920142      0.002761   \n",
       "37   -1576.445822       4.049921       0.916225      0.910109      0.003978   \n",
       "38   -1651.397045       4.569530       0.904471      0.901043      0.003459   \n",
       "39   -1933.146456      10.448014       0.865881      0.864476      0.002829   \n",
       "\n",
       "    mean_train_MAPE  mean_test_MAPE  std_train_MAPE  mean_train_MedAE  \\\n",
       "0         -0.116381       -0.117183        0.000487      -1195.951489   \n",
       "1         -0.086472       -0.091977        0.000400       -891.756292   \n",
       "2         -0.171500       -0.176530        0.002208      -1812.472355   \n",
       "3         -0.092636       -0.095147        0.000381       -934.902157   \n",
       "4         -0.281879       -0.282689        0.000532      -2841.103502   \n",
       "5         -0.252532       -0.253389        0.001114      -2653.028558   \n",
       "6         -0.098338       -0.100264        0.000385      -1021.007872   \n",
       "7         -0.285260       -0.286034        0.000864      -2881.675519   \n",
       "8         -0.084808       -0.091325        0.001044       -873.906531   \n",
       "9         -0.104591       -0.105791        0.000424      -1066.551951   \n",
       "10        -0.334815       -0.335063        0.001554      -3384.328381   \n",
       "11        -0.335624       -0.336623        0.002096      -3424.547990   \n",
       "12        -0.116616       -0.117440        0.000587      -1201.023075   \n",
       "13        -0.291227       -0.292011        0.004596      -2926.224678   \n",
       "14        -0.338726       -0.339304        0.002341      -3447.430888   \n",
       "15        -0.096538       -0.098650        0.000447      -1004.202514   \n",
       "16        -0.100364       -0.102081        0.000273      -1020.721238   \n",
       "17        -0.296593       -0.297299        0.003446      -3021.988641   \n",
       "18        -0.207627       -0.209211        0.000506      -2243.831241   \n",
       "19        -0.339538       -0.340256        0.003304      -3457.164536   \n",
       "20        -0.097766       -0.099850        0.000468      -1013.915304   \n",
       "21        -0.115725       -0.116671        0.000444      -1205.005456   \n",
       "22        -0.093419       -0.096197        0.000769       -944.546413   \n",
       "23        -0.177925       -0.182251        0.001368      -1889.142512   \n",
       "24        -0.339661       -0.340200        0.004963      -3425.922896   \n",
       "25        -0.342330       -0.342845        0.002862      -3462.093230   \n",
       "26        -0.185349       -0.188272        0.000770      -1979.866573   \n",
       "27        -0.085304       -0.091372        0.000361       -882.050244   \n",
       "28        -0.233625       -0.235049        0.001169      -2444.142646   \n",
       "29        -0.112688       -0.113622        0.000444      -1158.483697   \n",
       "30        -0.096294       -0.098179        0.000565       -978.616907   \n",
       "31        -0.105294       -0.106606        0.000451      -1097.642271   \n",
       "32        -0.098857       -0.100386        0.000619      -1006.228603   \n",
       "33        -0.088504       -0.092686        0.000292       -917.807450   \n",
       "34        -0.097893       -0.099889        0.000249      -1019.646023   \n",
       "35        -0.103259       -0.104548        0.000416      -1050.571603   \n",
       "36        -0.085048       -0.091014        0.000420       -876.777584   \n",
       "37        -0.094099       -0.096758        0.000365       -981.298072   \n",
       "38        -0.099116       -0.101070        0.000365      -1035.722493   \n",
       "39        -0.116835       -0.117619        0.000476      -1200.067101   \n",
       "\n",
       "    mean_test_MedAE  std_train_MedAE  mean_train_RMSE  mean_test_RMSE  \\\n",
       "0      -1202.514895         4.451864     -3534.804031    -3552.345531   \n",
       "1       -940.195702         1.302036     -2485.924302    -2708.034816   \n",
       "2      -1866.624640        17.523903     -3928.626013    -4093.137520   \n",
       "3       -959.078798         2.310459     -3020.409363    -3071.632832   \n",
       "4      -2850.709199         4.526609     -4805.804560    -4822.594066   \n",
       "5      -2652.473796        15.640689     -4903.531204    -4923.964820   \n",
       "6      -1040.014289         2.593244     -2941.671697    -2997.235170   \n",
       "7      -2893.635645        10.847789     -4821.894551    -4840.473043   \n",
       "8       -928.400208         9.591919     -2409.085738    -2668.511222   \n",
       "9      -1079.186879         2.913971     -3331.854407    -3353.694658   \n",
       "10     -3383.853679        22.977369     -5292.657932    -5305.351942   \n",
       "11     -3430.105496        10.933167     -5165.502636    -5184.691824   \n",
       "12     -1209.486617         7.406479     -3543.838874    -3559.637373   \n",
       "13     -2928.556706        28.837686     -5137.323707    -5147.073455   \n",
       "14     -3449.378951        29.975163     -5104.852939    -5118.429727   \n",
       "15     -1016.998942         2.617098     -2911.729856    -2975.908256   \n",
       "16     -1036.546596         3.264020     -3159.378102    -3188.629165   \n",
       "17     -3025.722782        42.616535     -5007.483863    -5025.344557   \n",
       "18     -2258.587771         5.917074     -4499.539915    -4541.209404   \n",
       "19     -3463.061867        31.716033     -5118.324254    -5135.141430   \n",
       "20     -1030.697056         3.546302     -2964.796514    -3029.265116   \n",
       "21     -1212.849936         5.115804     -3324.893167    -3353.731076   \n",
       "22      -978.581322         4.164586     -2952.937090    -3002.772624   \n",
       "23     -1932.669492        14.145415     -4010.862596    -4134.165568   \n",
       "24     -3432.434850        36.884425     -5260.719946    -5275.075827   \n",
       "25     -3474.636215        26.720797     -5179.164496    -5190.201682   \n",
       "26     -2010.898048         5.126321     -4081.224795    -4165.995067   \n",
       "27      -934.610534         1.175843     -2531.583102    -2734.664551   \n",
       "28     -2453.496552        12.739422     -4441.863696    -4490.432637   \n",
       "29     -1161.768014         1.745900     -3434.089240    -3454.230079   \n",
       "30      -996.107489         4.951785     -3080.564641    -3120.686926   \n",
       "31     -1106.132649         4.593574     -3107.756425    -3146.155261   \n",
       "32     -1024.551891         2.949607     -3122.232061    -3159.025131   \n",
       "33      -950.416247         1.679472     -2586.243194    -2751.181578   \n",
       "34     -1031.529164         1.583949     -2896.914953    -2967.493400   \n",
       "35     -1064.488708         2.665412     -3333.391926    -3357.881736   \n",
       "36      -926.898086         1.887976     -2548.435826    -2742.798890   \n",
       "37      -997.771491         2.599228     -2817.371072    -2911.792644   \n",
       "38     -1044.971619         2.365142     -3008.932844    -3057.527813   \n",
       "39     -1206.311729         3.564783     -3565.667344    -3581.899897   \n",
       "\n",
       "    std_train_RMSE  \n",
       "0        31.216914  \n",
       "1        39.910314  \n",
       "2        57.948536  \n",
       "3        59.300114  \n",
       "4        29.666644  \n",
       "5        42.781725  \n",
       "6        56.040775  \n",
       "7        40.545631  \n",
       "8        55.045667  \n",
       "9        33.522781  \n",
       "10       34.654544  \n",
       "11       37.101987  \n",
       "12       20.066147  \n",
       "13       91.728958  \n",
       "14       32.318653  \n",
       "15       61.789670  \n",
       "16       44.789376  \n",
       "17       46.131940  \n",
       "18       44.050626  \n",
       "19       23.848848  \n",
       "20       54.341476  \n",
       "21       37.677925  \n",
       "22       47.160133  \n",
       "23       73.062092  \n",
       "24       51.476336  \n",
       "25       30.419003  \n",
       "26       37.153410  \n",
       "27       67.453985  \n",
       "28       36.599265  \n",
       "29       25.901896  \n",
       "30       38.999209  \n",
       "31       53.345489  \n",
       "32       36.105847  \n",
       "33       51.152137  \n",
       "34       60.132552  \n",
       "35       37.434358  \n",
       "36       49.774321  \n",
       "37       65.934522  \n",
       "38       53.751045  \n",
       "39       39.958084  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "metric_cols_train_R2 = [c for c in results_df.columns if c.startswith(\"mean_train_R2\")]\n",
    "metric_cols_test_R2 = [c for c in results_df.columns if c.startswith(\"mean_test_R2\")]\n",
    "\n",
    "metric_cols_train_MAE = [c for c in results_df.columns if c.startswith(\"mean_train_MAE\")]\n",
    "metric_cols_test_MAE = [c for c in results_df.columns if c.startswith(\"mean_test_MAE\")]\n",
    "\n",
    "metric_cols_train_MAPE = [c for c in results_df.columns if c.startswith(\"mean_train_MAPE\")]\n",
    "metric_cols_test_MAPE = [c for c in results_df.columns if c.startswith(\"mean_test_MAPE\")]\n",
    "\n",
    "metric_cols_train_MedAE = [c for c in results_df.columns if c.startswith(\"mean_train_MedAE\")]\n",
    "metric_cols_test_MedAE = [c for c in results_df.columns if c.startswith(\"mean_test_MedAE\")]\n",
    "\n",
    "metric_cols_train_RMSE = [c for c in results_df.columns if c.startswith(\"mean_train_RMSE\")]\n",
    "metric_cols_test_RMSE = [c for c in results_df.columns if c.startswith(\"mean_test_RMSE\")]\n",
    "\n",
    "std_cols_train_R2 = [c for c in results_df.columns if c.startswith(\"std_train_R2\")]\n",
    "std_cols_test_R2 = [c for c in results_df.columns if c.startswith(\"std_test_R2\")]\n",
    "\n",
    "std_cols_train_MAE = [c for c in results_df.columns if c.startswith(\"std_train_MAE\")]\n",
    "std_cols_test_MAE = [c for c in results_df.columns if c.startswith(\"std_test_MAE\")]\n",
    "\n",
    "std_cols_train_MAPE = [c for c in results_df.columns if c.startswith(\"std_train_MAPE\")]\n",
    "std_cols_test_MAPE = [c for c in results_df.columns if c.startswith(\"std_test_MAPE\")]\n",
    "\n",
    "std_cols_train_MedAE = [c for c in results_df.columns if c.startswith(\"std_train_MedAE\")]\n",
    "std_cols_test_MedAE = [c for c in results_df.columns if c.startswith(\"std_test_MedAE\")]\n",
    "\n",
    "std_cols_train_RMSE = [c for c in results_df.columns if c.startswith(\"std_train_RMSE\")]\n",
    "std_cols_test_RMSE = [c for c in results_df.columns if c.startswith(\"std_test_RMSE\")]\n",
    "\n",
    "\n",
    "df_metrics = results_df[[\"params\"]+ metric_cols_train_MAE + metric_cols_test_MAE + std_cols_train_MAE + \n",
    "                        metric_cols_train_R2 + metric_cols_test_R2 + std_cols_train_R2 +\n",
    "                        metric_cols_train_MAPE + metric_cols_test_MAPE + std_cols_train_MAPE +\n",
    "                        metric_cols_train_MedAE + metric_cols_test_MedAE + std_cols_train_MedAE +\n",
    "                        metric_cols_train_RMSE + metric_cols_test_RMSE + std_cols_train_RMSE]\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7e5f1",
   "metadata": {},
   "source": [
    "Os valores dos scores são negativos, porque internamente o sklearn assume que maior = melhor. Então, para encontrar o melhor (menor) MAE tem de colocar o sinal de menos antes. \n",
    "\n",
    "Medidas onde diz test são referentes aos folds de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a69252bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics ['overfit_mae'] = df_metrics ['mean_test_MAE'] / df_metrics ['mean_train_MAE']\n",
    "df_metrics ['overfit_R2'] = df_metrics ['mean_train_R2'] - df_metrics ['mean_test_R2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e75fee",
   "metadata": {},
   "source": [
    "Interpretação:\n",
    "- Mae: 1--> sem overfit; maior que 1 --> overfit; menor que 1 --> underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11197389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <th>overfit_mae</th>\n",
       "      <th>mean_train_R2</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>overfit_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1902.709170</td>\n",
       "      <td>-1913.821556</td>\n",
       "      <td>1.005840</td>\n",
       "      <td>0.868193</td>\n",
       "      <td>0.866699</td>\n",
       "      <td>0.001494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1379.108161</td>\n",
       "      <td>-1491.090617</td>\n",
       "      <td>1.081199</td>\n",
       "      <td>0.934794</td>\n",
       "      <td>0.922143</td>\n",
       "      <td>0.012651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2606.440050</td>\n",
       "      <td>-2696.516823</td>\n",
       "      <td>1.034559</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>0.823049</td>\n",
       "      <td>0.014106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1552.968773</td>\n",
       "      <td>-1589.626143</td>\n",
       "      <td>1.023605</td>\n",
       "      <td>0.903731</td>\n",
       "      <td>0.900233</td>\n",
       "      <td>0.003497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3545.216016</td>\n",
       "      <td>-3555.931597</td>\n",
       "      <td>1.003023</td>\n",
       "      <td>0.756376</td>\n",
       "      <td>0.754466</td>\n",
       "      <td>0.001910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3514.197285</td>\n",
       "      <td>-3525.774277</td>\n",
       "      <td>1.003294</td>\n",
       "      <td>0.746354</td>\n",
       "      <td>0.744054</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1602.653524</td>\n",
       "      <td>-1635.268952</td>\n",
       "      <td>1.020351</td>\n",
       "      <td>0.908688</td>\n",
       "      <td>0.904805</td>\n",
       "      <td>0.003883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3571.060371</td>\n",
       "      <td>-3580.978788</td>\n",
       "      <td>1.002777</td>\n",
       "      <td>0.754733</td>\n",
       "      <td>0.752646</td>\n",
       "      <td>0.002087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1348.259744</td>\n",
       "      <td>-1477.144631</td>\n",
       "      <td>1.095594</td>\n",
       "      <td>0.938741</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.014387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1745.815522</td>\n",
       "      <td>-1762.693825</td>\n",
       "      <td>1.009668</td>\n",
       "      <td>0.882891</td>\n",
       "      <td>0.881166</td>\n",
       "      <td>0.001725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-4008.971569</td>\n",
       "      <td>-4014.829745</td>\n",
       "      <td>1.001461</td>\n",
       "      <td>0.704523</td>\n",
       "      <td>0.702808</td>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3954.561361</td>\n",
       "      <td>-3966.941517</td>\n",
       "      <td>1.003131</td>\n",
       "      <td>0.718541</td>\n",
       "      <td>0.716249</td>\n",
       "      <td>0.002292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1905.795401</td>\n",
       "      <td>-1916.147168</td>\n",
       "      <td>1.005432</td>\n",
       "      <td>0.867526</td>\n",
       "      <td>0.866124</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-3762.809486</td>\n",
       "      <td>-3769.803248</td>\n",
       "      <td>1.001859</td>\n",
       "      <td>0.721537</td>\n",
       "      <td>0.720156</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-3922.471276</td>\n",
       "      <td>-3931.204313</td>\n",
       "      <td>1.002226</td>\n",
       "      <td>0.725111</td>\n",
       "      <td>0.723486</td>\n",
       "      <td>0.001624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1575.725157</td>\n",
       "      <td>-1610.541528</td>\n",
       "      <td>1.022095</td>\n",
       "      <td>0.910527</td>\n",
       "      <td>0.906175</td>\n",
       "      <td>0.004351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1666.335907</td>\n",
       "      <td>-1689.145668</td>\n",
       "      <td>1.013689</td>\n",
       "      <td>0.894689</td>\n",
       "      <td>0.892496</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-3743.861061</td>\n",
       "      <td>-3753.733342</td>\n",
       "      <td>1.002637</td>\n",
       "      <td>0.735489</td>\n",
       "      <td>0.733409</td>\n",
       "      <td>0.002080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-3110.088152</td>\n",
       "      <td>-3135.679393</td>\n",
       "      <td>1.008228</td>\n",
       "      <td>0.786428</td>\n",
       "      <td>0.782095</td>\n",
       "      <td>0.004333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-3933.165828</td>\n",
       "      <td>-3942.725428</td>\n",
       "      <td>1.002431</td>\n",
       "      <td>0.723657</td>\n",
       "      <td>0.721665</td>\n",
       "      <td>0.001991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1597.331609</td>\n",
       "      <td>-1632.045933</td>\n",
       "      <td>1.021733</td>\n",
       "      <td>0.907248</td>\n",
       "      <td>0.902831</td>\n",
       "      <td>0.004417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1850.499124</td>\n",
       "      <td>-1864.331754</td>\n",
       "      <td>1.007475</td>\n",
       "      <td>0.883376</td>\n",
       "      <td>0.881097</td>\n",
       "      <td>0.002279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1555.102083</td>\n",
       "      <td>-1596.788141</td>\n",
       "      <td>1.026806</td>\n",
       "      <td>0.907995</td>\n",
       "      <td>0.904473</td>\n",
       "      <td>0.003522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-2693.309457</td>\n",
       "      <td>-2765.991742</td>\n",
       "      <td>1.026986</td>\n",
       "      <td>0.830257</td>\n",
       "      <td>0.819418</td>\n",
       "      <td>0.010839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-4012.487187</td>\n",
       "      <td>-4023.471361</td>\n",
       "      <td>1.002737</td>\n",
       "      <td>0.708037</td>\n",
       "      <td>0.706359</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-3969.409592</td>\n",
       "      <td>-3977.556314</td>\n",
       "      <td>1.002052</td>\n",
       "      <td>0.717048</td>\n",
       "      <td>0.715686</td>\n",
       "      <td>0.001362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-2784.048126</td>\n",
       "      <td>-2833.882150</td>\n",
       "      <td>1.017900</td>\n",
       "      <td>0.824294</td>\n",
       "      <td>0.816583</td>\n",
       "      <td>0.007710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1380.253487</td>\n",
       "      <td>-1486.209582</td>\n",
       "      <td>1.076766</td>\n",
       "      <td>0.932345</td>\n",
       "      <td>0.920603</td>\n",
       "      <td>0.011742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-3192.259219</td>\n",
       "      <td>-3216.476065</td>\n",
       "      <td>1.007586</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.786985</td>\n",
       "      <td>0.004889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1849.777603</td>\n",
       "      <td>-1862.768820</td>\n",
       "      <td>1.007023</td>\n",
       "      <td>0.875602</td>\n",
       "      <td>0.873902</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1610.123547</td>\n",
       "      <td>-1637.726630</td>\n",
       "      <td>1.017143</td>\n",
       "      <td>0.899884</td>\n",
       "      <td>0.896947</td>\n",
       "      <td>0.002936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1704.887276</td>\n",
       "      <td>-1724.970836</td>\n",
       "      <td>1.011780</td>\n",
       "      <td>0.898093</td>\n",
       "      <td>0.895257</td>\n",
       "      <td>0.002836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1648.504353</td>\n",
       "      <td>-1672.793973</td>\n",
       "      <td>1.014734</td>\n",
       "      <td>0.897162</td>\n",
       "      <td>0.894414</td>\n",
       "      <td>0.002749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-1428.659816</td>\n",
       "      <td>-1505.447556</td>\n",
       "      <td>1.053748</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.919638</td>\n",
       "      <td>0.009779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-1583.635557</td>\n",
       "      <td>-1619.411416</td>\n",
       "      <td>1.022591</td>\n",
       "      <td>0.911435</td>\n",
       "      <td>0.906689</td>\n",
       "      <td>0.004746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1728.183686</td>\n",
       "      <td>-1747.444044</td>\n",
       "      <td>1.011145</td>\n",
       "      <td>0.882778</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.001909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1378.087152</td>\n",
       "      <td>-1482.922437</td>\n",
       "      <td>1.076073</td>\n",
       "      <td>0.931465</td>\n",
       "      <td>0.920142</td>\n",
       "      <td>0.011323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1530.335464</td>\n",
       "      <td>-1576.445822</td>\n",
       "      <td>1.030131</td>\n",
       "      <td>0.916225</td>\n",
       "      <td>0.910109</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1623.583484</td>\n",
       "      <td>-1651.397045</td>\n",
       "      <td>1.017131</td>\n",
       "      <td>0.904471</td>\n",
       "      <td>0.901043</td>\n",
       "      <td>0.003427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1922.222031</td>\n",
       "      <td>-1933.146456</td>\n",
       "      <td>1.005683</td>\n",
       "      <td>0.865881</td>\n",
       "      <td>0.864476</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_MAE  mean_test_MAE  overfit_mae  mean_train_R2  mean_test_R2  \\\n",
       "0     -1902.709170   -1913.821556     1.005840       0.868193      0.866699   \n",
       "1     -1379.108161   -1491.090617     1.081199       0.934794      0.922143   \n",
       "2     -2606.440050   -2696.516823     1.034559       0.837155      0.823049   \n",
       "3     -1552.968773   -1589.626143     1.023605       0.903731      0.900233   \n",
       "4     -3545.216016   -3555.931597     1.003023       0.756376      0.754466   \n",
       "5     -3514.197285   -3525.774277     1.003294       0.746354      0.744054   \n",
       "6     -1602.653524   -1635.268952     1.020351       0.908688      0.904805   \n",
       "7     -3571.060371   -3580.978788     1.002777       0.754733      0.752646   \n",
       "8     -1348.259744   -1477.144631     1.095594       0.938741      0.924354   \n",
       "9     -1745.815522   -1762.693825     1.009668       0.882891      0.881166   \n",
       "10    -4008.971569   -4014.829745     1.001461       0.704523      0.702808   \n",
       "11    -3954.561361   -3966.941517     1.003131       0.718541      0.716249   \n",
       "12    -1905.795401   -1916.147168     1.005432       0.867526      0.866124   \n",
       "13    -3762.809486   -3769.803248     1.001859       0.721537      0.720156   \n",
       "14    -3922.471276   -3931.204313     1.002226       0.725111      0.723486   \n",
       "15    -1575.725157   -1610.541528     1.022095       0.910527      0.906175   \n",
       "16    -1666.335907   -1689.145668     1.013689       0.894689      0.892496   \n",
       "17    -3743.861061   -3753.733342     1.002637       0.735489      0.733409   \n",
       "18    -3110.088152   -3135.679393     1.008228       0.786428      0.782095   \n",
       "19    -3933.165828   -3942.725428     1.002431       0.723657      0.721665   \n",
       "20    -1597.331609   -1632.045933     1.021733       0.907248      0.902831   \n",
       "21    -1850.499124   -1864.331754     1.007475       0.883376      0.881097   \n",
       "22    -1555.102083   -1596.788141     1.026806       0.907995      0.904473   \n",
       "23    -2693.309457   -2765.991742     1.026986       0.830257      0.819418   \n",
       "24    -4012.487187   -4023.471361     1.002737       0.708037      0.706359   \n",
       "25    -3969.409592   -3977.556314     1.002052       0.717048      0.715686   \n",
       "26    -2784.048126   -2833.882150     1.017900       0.824294      0.816583   \n",
       "27    -1380.253487   -1486.209582     1.076766       0.932345      0.920603   \n",
       "28    -3192.259219   -3216.476065     1.007586       0.791874      0.786985   \n",
       "29    -1849.777603   -1862.768820     1.007023       0.875602      0.873902   \n",
       "30    -1610.123547   -1637.726630     1.017143       0.899884      0.896947   \n",
       "31    -1704.887276   -1724.970836     1.011780       0.898093      0.895257   \n",
       "32    -1648.504353   -1672.793973     1.014734       0.897162      0.894414   \n",
       "33    -1428.659816   -1505.447556     1.053748       0.929417      0.919638   \n",
       "34    -1583.635557   -1619.411416     1.022591       0.911435      0.906689   \n",
       "35    -1728.183686   -1747.444044     1.011145       0.882778      0.880869   \n",
       "36    -1378.087152   -1482.922437     1.076073       0.931465      0.920142   \n",
       "37    -1530.335464   -1576.445822     1.030131       0.916225      0.910109   \n",
       "38    -1623.583484   -1651.397045     1.017131       0.904471      0.901043   \n",
       "39    -1922.222031   -1933.146456     1.005683       0.865881      0.864476   \n",
       "\n",
       "    overfit_R2  \n",
       "0     0.001494  \n",
       "1     0.012651  \n",
       "2     0.014106  \n",
       "3     0.003497  \n",
       "4     0.001910  \n",
       "5     0.002300  \n",
       "6     0.003883  \n",
       "7     0.002087  \n",
       "8     0.014387  \n",
       "9     0.001725  \n",
       "10    0.001715  \n",
       "11    0.002292  \n",
       "12    0.001402  \n",
       "13    0.001381  \n",
       "14    0.001624  \n",
       "15    0.004351  \n",
       "16    0.002193  \n",
       "17    0.002080  \n",
       "18    0.004333  \n",
       "19    0.001991  \n",
       "20    0.004417  \n",
       "21    0.002279  \n",
       "22    0.003522  \n",
       "23    0.010839  \n",
       "24    0.001678  \n",
       "25    0.001362  \n",
       "26    0.007710  \n",
       "27    0.011742  \n",
       "28    0.004889  \n",
       "29    0.001700  \n",
       "30    0.002936  \n",
       "31    0.002836  \n",
       "32    0.002749  \n",
       "33    0.009779  \n",
       "34    0.004746  \n",
       "35    0.001909  \n",
       "36    0.011323  \n",
       "37    0.006116  \n",
       "38    0.003427  \n",
       "39    0.001405  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics [['mean_train_MAE','mean_test_MAE', 'overfit_mae', 'mean_train_R2', 'mean_test_R2', 'overfit_R2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea08576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Option 0:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': None, 'regressor__max_depth': 2, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 1:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': None, 'regressor__max_depth': 6, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 2:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.3, 'regressor__max_depth': 6, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 3:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.5, 'regressor__max_depth': 7, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 4:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.3, 'regressor__max_depth': 6, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 5:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 'sqrt', 'regressor__max_depth': 2, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 6:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 'log2', 'regressor__max_depth': 3, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 7:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 'sqrt', 'regressor__max_depth': 3, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 8:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.5, 'regressor__max_depth': 7, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 9:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 0.5, 'regressor__max_depth': 6, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 10:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 'log2', 'regressor__max_depth': 2, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 11:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 'log2', 'regressor__max_depth': 7, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 12:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.5, 'regressor__max_depth': 2, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 13:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 'log2', 'regressor__max_depth': 2, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 14:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 0.5, 'regressor__max_depth': 7, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 15:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 'log2', 'regressor__max_depth': 4, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 16:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': None, 'regressor__max_depth': 3, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 17:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 0.2, 'regressor__max_depth': 3, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 18:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.5, 'regressor__max_depth': 4, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 19:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': None, 'regressor__max_features': None, 'regressor__max_depth': 4, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 20:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.3, 'regressor__max_depth': 5, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 21:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': None, 'regressor__max_depth': 2, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 22:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 0.5, 'regressor__max_depth': 3, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 23:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.5, 'regressor__max_depth': 5, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 24:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.2, 'regressor__max_depth': 4, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 25:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 0.5, 'regressor__max_depth': 5, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 26:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': None, 'regressor__max_depth': 6, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 27:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.2, 'regressor__max_depth': 5, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 28:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': None, 'regressor__max_depth': 7, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 29:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 0.5, 'regressor__max_depth': 2, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 30:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.5, 'regressor__max_depth': 4, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 31:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 0.5, 'regressor__max_depth': 3, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 32:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 'sqrt', 'regressor__max_depth': 3, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 33:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 'log2', 'regressor__max_depth': 6, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 34:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': None, 'regressor__max_features': None, 'regressor__max_depth': 4, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 35:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.2, 'regressor__max_depth': 7, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 36:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 0.2, 'regressor__max_depth': 6, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 37:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 0.3, 'regressor__max_depth': 5, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 38:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 0.2, 'regressor__max_depth': 5, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "\n",
      "Option 39:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.2, 'regressor__max_depth': 2, 'regressor__loss': 'absolute_error', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n"
     ]
    }
   ],
   "source": [
    "for i, row in df_metrics.iterrows():\n",
    "    print(f\"\\nOption {i}:\")\n",
    "    print(\"params:\", row[\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5520be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg overfit mae: 1.0200080791647257\n",
      "Min mae test: -1477.1446306608545\n",
      "Min overfit mae: 1.0014612666995744\n"
     ]
    }
   ],
   "source": [
    "print('Avg overfit mae:', df_metrics ['overfit_mae'].mean())\n",
    "print( 'Min mae test:', df_metrics ['mean_test_MAE'].max())\n",
    "print( 'Min overfit mae:', df_metrics ['overfit_mae'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87e608d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Greater mean_test_MAE:\n",
      "\n",
      "Option 33:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 'log2', 'regressor__max_depth': 6, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "mae -1505.4475562661257 ; overfit: 1.053748092865499\n",
      "\n",
      "Option 1:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': None, 'regressor__max_depth': 6, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "mae -1491.0906170950516 ; overfit: 1.0811991830498842\n",
      "\n",
      "Option 27:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': None, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0001, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 0.2, 'regressor__max_depth': 5, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "mae -1486.209582292447 ; overfit: 1.0767656782941715\n",
      "\n",
      "Option 36:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 0.2, 'regressor__max_depth': 6, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "mae -1482.92243656563 ; overfit: 1.0760730437008152\n",
      "\n",
      "Option 8:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 0.5, 'regressor__max_depth': 7, 'regressor__loss': 'huber', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "mae -1477.1446306608545 ; overfit: 1.095593513562594\n"
     ]
    }
   ],
   "source": [
    "top3_test = df_metrics.sort_values('mean_test_MAE').tail(5)[['params','mean_test_MAE','overfit_mae']]\n",
    "\n",
    "top3_overfit = df_metrics.sort_values('overfit_mae').head(5)[['params','mean_test_MAE','overfit_mae']]\n",
    "\n",
    "print(\"\\n Greater mean_test_MAE:\")\n",
    "for i, row in top3_test.iterrows():\n",
    "    print(f\"\\nOption {i}:\")\n",
    "    print(\"params:\", row[\"params\"])\n",
    "    print('mae', row ['mean_test_MAE'], '; overfit:', row ['overfit_mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a9a27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smaller overfit_mae:\n",
      "\n",
      "Option 10:\n",
      "params: {'regressor__validation_fraction': 0.2, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 1000, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 30, 'regressor__max_features': 'log2', 'regressor__max_depth': 2, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "mae -4014.8297451657077 ; overfit: 1.0014612666995744\n",
      "\n",
      "Option 13:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 15, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 4, 'regressor__min_impurity_decrease': 0.0005, 'regressor__max_leaf_nodes': 20, 'regressor__max_features': 'log2', 'regressor__max_depth': 2, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "mae -3769.8032475056375 ; overfit: 1.0018586540915853\n",
      "\n",
      "Option 25:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.8, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 700, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 5, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': 15, 'regressor__max_features': 0.5, 'regressor__max_depth': 5, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.1, 'regressor__criterion': 'squared_error'}\n",
      "mae -3977.5563141846565 ; overfit: 1.0020523764051832\n",
      "\n",
      "Option 14:\n",
      "params: {'regressor__validation_fraction': 0.1, 'regressor__subsample': 0.6, 'regressor__n_iter_no_change': 10, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 8, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': None, 'regressor__max_features': 0.5, 'regressor__max_depth': 7, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.05, 'regressor__criterion': 'squared_error'}\n",
      "mae -3931.2043132523722 ; overfit: 1.0022264121446418\n",
      "\n",
      "Option 19:\n",
      "params: {'regressor__validation_fraction': 0.15, 'regressor__subsample': 0.7, 'regressor__n_iter_no_change': 20, 'regressor__n_estimators': 400, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__min_impurity_decrease': 0.001, 'regressor__max_leaf_nodes': None, 'regressor__max_features': None, 'regressor__max_depth': 4, 'regressor__loss': 'quantile', 'regressor__learning_rate': 0.03, 'regressor__criterion': 'squared_error'}\n",
      "mae -3942.7254277303014 ; overfit: 1.0024305103039792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSmaller overfit_mae:\")\n",
    "for i, row in top3_overfit.iterrows():\n",
    "    print(f\"\\nOption {i}:\")\n",
    "    print(\"params:\", row[\"params\"])\n",
    "    print('mae', row ['mean_test_MAE'], '; overfit:', row ['overfit_mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d216eac",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef140f7",
   "metadata": {},
   "source": [
    "Usar separadamente (sem ter corrido random search) (isto foi só para dar debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafda770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical_Correction works\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>tax</th>\n",
       "      <th>mpg</th>\n",
       "      <th>engineSize</th>\n",
       "      <th>paintQuality%</th>\n",
       "      <th>previousOwners</th>\n",
       "      <th>hasDamage</th>\n",
       "      <th>Brand_cleaned</th>\n",
       "      <th>transmission_cleaned</th>\n",
       "      <th>fuelType_cleaned</th>\n",
       "      <th>model_cleaned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69512</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>28421.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VW</td>\n",
       "      <td>SEMI-AUTO</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>GOLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53000</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>4589.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>47.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>YARIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>3624.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>40.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>SEMI-AUTO</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29021</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>9102.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FORD</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>FIESTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>42.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>2 SERIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37194</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>14480.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>53.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MERCEDES</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>C CLASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>52134.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>47.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>SEMI-AUTO</td>\n",
       "      <td>DIESEL</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54886</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>11304.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>AUTOMATIC</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>AYGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>69072.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>60.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>DIESEL</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>16709.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>64.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FORD</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>PETROL</td>\n",
       "      <td>FIESTA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75973 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         year  mileage    tax   mpg  engineSize  paintQuality%  \\\n",
       "carID                                                            \n",
       "69512  2016.0  28421.0    NaN   NaN         2.0           63.0   \n",
       "53000  2019.0   4589.0  145.0  47.9         1.5           50.0   \n",
       "6366   2019.0   3624.0  145.0  40.9         1.5           56.0   \n",
       "29021  2018.0   9102.0  145.0  65.7         1.0           50.0   \n",
       "10062  2019.0   1000.0  145.0  42.8         1.5           97.0   \n",
       "...       ...      ...    ...   ...         ...            ...   \n",
       "37194  2015.0  14480.0  125.0  53.3         2.0           78.0   \n",
       "6265   2013.0  52134.0  200.0  47.9         2.0           38.0   \n",
       "54886  2017.0  11304.0  145.0  67.0         1.0           57.0   \n",
       "860    2015.0  69072.0  125.0  60.1         2.0           74.0   \n",
       "15795  2018.0  16709.0  145.0  64.2         1.1           38.0   \n",
       "\n",
       "       previousOwners  hasDamage Brand_cleaned transmission_cleaned  \\\n",
       "carID                                                                 \n",
       "69512             4.0        0.0            VW            SEMI-AUTO   \n",
       "53000             1.0        0.0        TOYOTA               MANUAL   \n",
       "6366              4.0        0.0          AUDI            SEMI-AUTO   \n",
       "29021             NaN        0.0          FORD               MANUAL   \n",
       "10062             3.0        0.0           BMW               MANUAL   \n",
       "...               ...        ...           ...                  ...   \n",
       "37194             0.0        0.0      MERCEDES               MANUAL   \n",
       "6265              2.0        0.0          AUDI            SEMI-AUTO   \n",
       "54886             3.0        0.0        TOYOTA            AUTOMATIC   \n",
       "860               2.0        0.0          AUDI               MANUAL   \n",
       "15795             1.0        0.0          FORD               MANUAL   \n",
       "\n",
       "      fuelType_cleaned model_cleaned  \n",
       "carID                                 \n",
       "69512           PETROL          GOLF  \n",
       "53000           PETROL         YARIS  \n",
       "6366            PETROL            Q2  \n",
       "29021           PETROL        FIESTA  \n",
       "10062           PETROL      2 SERIES  \n",
       "...                ...           ...  \n",
       "37194           PETROL       C CLASS  \n",
       "6265            DIESEL            Q3  \n",
       "54886           PETROL          AYGO  \n",
       "860             DIESEL            Q3  \n",
       "15795           PETROL        FIESTA  \n",
       "\n",
       "[75973 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Categorical_Correction\n",
    "cat_corrector = Categorical_Correction()\n",
    "X = cat_corrector.fit_transform(X)\n",
    "print(\"Categorical_Correction works\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a042692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Treatment works\n"
     ]
    }
   ],
   "source": [
    "# Outlier_Treatment\n",
    "outlier = Outlier_Treatment()\n",
    "X = outlier.fit_transform(X)\n",
    "print(\"Outlier Treatment works\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0934f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "numeric_features = ['year', 'tax', 'mileage', 'previousOwners', 'engineSize', 'paintQuality%', 'mpg']\n",
    "\n",
    "fig, axes = plt.subplots(ceil(len(numeric_features) / 3 ), 3, figsize = (18,10))\n",
    "\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), numeric_features):\n",
    "    sns.boxplot(x=X[feat], ax=ax, color='skyblue')\n",
    "\n",
    "# Delete empty plots\n",
    "for ax in axes.flatten()[len(numeric_features):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Numerical Variables' Box Plots\", fontsize=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b45b29cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Treatment works\n"
     ]
    }
   ],
   "source": [
    "# Missing_Value_Treatment\n",
    "missing = Missing_Value_Treatment()\n",
    "X = missing.fit_transform(X)\n",
    "print(\"Missing Treatment works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed654e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typecasting works\n"
     ]
    }
   ],
   "source": [
    "# Typecasting\n",
    "type = Typecasting()\n",
    "X = type.fit_transform(X)\n",
    "print(\"Typecasting works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "122310fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Eng works\n"
     ]
    }
   ],
   "source": [
    "# Test Feature_Engineering\n",
    "feature_eng = Feature_Engineering()\n",
    "X = feature_eng.fit_transform(X,y)\n",
    "print(\"Feature Eng works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1233dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                       0\n",
       "mileage                    0\n",
       "tax                        0\n",
       "mpg                        0\n",
       "engineSize                 0\n",
       "paintQuality%           1891\n",
       "previousOwners             0\n",
       "hasDamage                  0\n",
       "Brand_cleaned              0\n",
       "transmission_cleaned       0\n",
       "fuelType_cleaned           0\n",
       "model_cleaned              0\n",
       "carAge                     0\n",
       "AvgUsage                   0\n",
       "carSegment                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4237539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder works\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "code = Encoder()\n",
    "X = code.fit_transform(X,y)\n",
    "print(\"Encoder works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58b1b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler works\n"
     ]
    }
   ],
   "source": [
    "# Scaler\n",
    "scaler = Scaler()\n",
    "X = scaler.fit_transform(X,y)\n",
    "print(\"Scaler works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8167c265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'mileage', 'tax', 'mpg', 'engineSize', 'paintQuality%',\n",
       "       'previousOwners', 'hasDamage', 'carAge', 'AvgUsage', 'carSegment',\n",
       "       'model_cleaned_encoded', 'Brand_cleaned_encoded',\n",
       "       'fuelType_cleaned_DIESEL', 'fuelType_cleaned_ELECTRIC',\n",
       "       'fuelType_cleaned_HYBRID', 'fuelType_cleaned_OTHER',\n",
       "       'fuelType_cleaned_PETROL', 'transmission_cleaned_AUTOMATIC',\n",
       "       'transmission_cleaned_MANUAL', 'transmission_cleaned_OTHER',\n",
       "       'transmission_cleaned_SEMI-AUTO', 'transmission_cleaned_UNKNOWN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5cb92e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test Categorical_Correction\u001b[39;00m\n\u001b[0;32m      2\u001b[0m selection \u001b[38;5;241m=\u001b[39m Feature_Selection()\n\u001b[1;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m selection\u001b[38;5;241m.\u001b[39mfit_transform(X,y)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeat Selection works\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:921\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[1;32mIn[19], line 49\u001b[0m, in \u001b[0;36mFeature_Selection.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     47\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m     48\u001b[0m rfe_lr \u001b[38;5;241m=\u001b[39m RFE(estimator\u001b[38;5;241m=\u001b[39mmodel, n_features_to_select\u001b[38;5;241m=\u001b[39mn_feats)\n\u001b[1;32m---> 49\u001b[0m rfe_lr\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39m X[features], y\u001b[38;5;241m=\u001b[39m y)\n\u001b[0;32m     50\u001b[0m rfe_lr_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(rfe_lr\u001b[38;5;241m.\u001b[39msupport_, index\u001b[38;5;241m=\u001b[39mfeatures)\n\u001b[0;32m     51\u001b[0m rfe_lr_features_list \u001b[38;5;241m=\u001b[39m rfe_lr_features[rfe_lr_features]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# only chooses the features where RFE selected True\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:276\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m Bunch(estimator\u001b[38;5;241m=\u001b[39mBunch(fit\u001b[38;5;241m=\u001b[39mfit_params))\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit)\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:332\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 332\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    335\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    336\u001b[0m     estimator,\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    338\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:601\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    597\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    599\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 601\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    603\u001b[0m     X,\n\u001b[0;32m    604\u001b[0m     y,\n\u001b[0;32m    605\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m    606\u001b[0m     y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    607\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    608\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    609\u001b[0m )\n\u001b[0;32m    611\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1371\u001b[0m     X,\n\u001b[0;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1373\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1374\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1375\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1376\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1377\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1378\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[0;32m   1379\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1380\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1381\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1382\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1383\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1108\u001b[0m         array,\n\u001b[0;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1112\u001b[0m     )\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    121\u001b[0m     X,\n\u001b[0;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    127\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maris\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Test Categorical_Correction\n",
    "selection = Feature_Selection()\n",
    "X = selection.fit_transform(X,y)\n",
    "print(\"Feat Selection works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31b577df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'mileage', 'tax', 'mpg', 'engineSize', 'carAge', 'AvgUsage',\n",
       "       'carSegment', 'model_cleaned_encoded', 'Brand_cleaned_encoded',\n",
       "       'fuelType_cleaned_DIESEL', 'fuelType_cleaned_PETROL',\n",
       "       'transmission_cleaned_MANUAL', 'transmission_cleaned_SEMI-AUTO',\n",
       "       'fuelType_cleaned_ELECTRIC', 'fuelType_cleaned_OTHER'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
