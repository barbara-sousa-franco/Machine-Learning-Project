{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "334eb522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:05.349409Z",
     "start_time": "2025-12-07T17:41:05.293886Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from math import ceil\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, median_absolute_error, root_mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from functions_MARISA import *\n",
    "from Classes import Categorical_Correction, Outlier_Treatment, Missing_Value_Treatment, Typecasting, Feature_Engineering, Encoder, Scaler, Feature_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d1ecd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:07.245163Z",
     "start_time": "2025-12-07T17:41:07.101258Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "665d8ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:08.938950Z",
     "start_time": "2025-12-07T17:41:08.933756Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.set_index('carID', inplace=True)\n",
    "df_test.set_index('carID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2441abb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:15.716543Z",
     "start_time": "2025-12-07T17:41:15.712018Z"
    }
   },
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d69aa7",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9ac63",
   "metadata": {},
   "source": [
    "We start by defining the inconsistent values discussed in the EDA as NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca0536e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:17.080814Z",
     "start_time": "2025-12-07T17:41:17.065905Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.loc[df_train['year']>2020, 'year'] = np.nan\n",
    "df_test.loc[df_test['year']>2020, 'year'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mileage']<0, 'mileage'] = np.nan\n",
    "df_test.loc[df_test['mileage']<0, 'mileage'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['tax']<0, 'tax'] = np.nan\n",
    "df_test.loc[df_test['tax']<0,'tax'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mpg']<=0, 'mpg'] = np.nan\n",
    "df_test.loc[df_test['mpg']<=0, 'mpg'] = np.nan\n",
    "\n",
    "\n",
    "df_train.loc[df_train['previousOwners']< 0, 'previousOwners'] = np.nan\n",
    "df_test.loc[df_test['previousOwners']< 0, 'previousOwners'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['engineSize']<= 0, 'engineSize'] = np.nan\n",
    "df_test.loc[df_test['engineSize']<= 0, 'engineSize'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['mpg'] < 8, 'mpg'] = np.nan\n",
    "df_test.loc[df_test['mpg'] < 8, 'mpg'] = np.nan\n",
    "\n",
    "df_train.loc[df_train['engineSize'] < 1, 'engineSize'] = np.nan\n",
    "df_test.loc[df_test['engineSize'] < 1, 'engineSize'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345109",
   "metadata": {},
   "source": [
    "We proceed to round 'year' and 'previousOwners' to whole numbers using the floor function. Other numerical features are rounded to 2 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80085750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:19.769571Z",
     "start_time": "2025-12-07T17:41:19.758487Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['year'] = np.floor(df_train['year'])\n",
    "df_train['previousOwners'] = np.floor(df_train['previousOwners'])\n",
    "\n",
    "df_test['year'] = np.floor(df_test['year'])\n",
    "df_test['previousOwners'] = np.floor(df_test['previousOwners'])\n",
    "\n",
    "for feat in ['mileage', 'tax', 'mpg', 'engineSize']:\n",
    "    df_train[feat] = df_train[feat].round(2)\n",
    "    df_test[feat] = df_test[feat].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf0b21",
   "metadata": {},
   "source": [
    "We also pre-process the categorical variables in order to have a uniform format for later treatment (inside k-fold CV). We remove leeading and trailing spaces and uppercase all letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "454801c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:21.895291Z",
     "start_time": "2025-12-07T17:41:21.790698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre processing the categorical variables to be easier to find clusters in typos:\n",
    "    # remove spaces (at the beginning and end) and uppercase all letters\n",
    "    # does not replace NaN's\n",
    "df_train['Brand'] = df_train['Brand'].where(df_train['Brand'].isna(), df_train['Brand'].astype(str).str.strip().str.upper())\n",
    "df_test['Brand']  = df_test['Brand'].where(df_test['Brand'].isna(), df_test['Brand'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['model'] = df_train['model'].where(df_train['model'].isna(), df_train['model'].astype(str).str.strip().str.upper())\n",
    "df_test['model']  = df_test['model'].where(df_test['model'].isna(), df_test['model'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['fuelType'] = df_train['fuelType'].where(df_train['fuelType'].isna(), df_train['fuelType'].astype(str).str.strip().str.upper())\n",
    "df_test['fuelType']  = df_test['fuelType'].where(df_test['fuelType'].isna(), df_test['fuelType'].astype(str).str.strip().str.upper())\n",
    "\n",
    "df_train['transmission'] = df_train['transmission'].where(df_train['transmission'].isna(), df_train['transmission'].astype(str).str.strip().str.upper())\n",
    "df_test['transmission']  = df_test['transmission'].where(df_test['transmission'].isna(), df_test['transmission'].astype(str).str.strip().str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1da0ba87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:25.531815Z",
     "start_time": "2025-12-07T17:41:25.527032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_train[\"price\"] == 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3225b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:27.283377Z",
     "start_time": "2025-12-07T17:41:27.269568Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df_train['price']\n",
    "X = df_train.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f9c951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:28.943746Z",
     "start_time": "2025-12-07T17:41:28.930249Z"
    }
   },
   "outputs": [],
   "source": [
    "X.drop('paintQuality%', axis=1, inplace=True)\n",
    "df_test.drop('paintQuality%', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af4260",
   "metadata": {},
   "source": [
    "Notas sobre as classes:\n",
    "\n",
    "- variáveis criadas na inicialização não acabam em _; as ue são criadas dentro dos métodos acabam em _!\n",
    "- criando uma var nos métodos, se ela não começar em self. não será reconhecida por toda a classe, será apenas local!\n",
    "- logo, iniciar com self. para criar novos atributos gerais (assim transform() cconsegue aceder ao atributo criado em fit() por exemplo)\n",
    "- cuidado com data leakage! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543515b8",
   "metadata": {},
   "source": [
    "### Categorical_Correction Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732ce2e",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fdfa392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:34.606521Z",
     "start_time": "2025-12-07T17:41:34.597468Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_huber = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), #NO NEED FOR RANDOM FOREST REG\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', HuberRegressor())\n",
    "])\n",
    "\n",
    "pipeline_huber_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), \n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=HuberRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), \n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "pipeline_mlp_robust = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(RobustScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_robust_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(RobustScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler())),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax2 = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler( feature_range=(-1,1)))),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), transformer = StandardScaler()))\n",
    "])\n",
    "\n",
    "pipeline_mlp_minmax2_logprice = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),\n",
    "    ('outlier treatment', Outlier_Treatment()),\n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()),\n",
    "    ('feature engineering', Feature_Engineering()),\n",
    "    ('encoder', Encoder() ),\n",
    "    ('scaler', Scaler(MinMaxScaler( feature_range=(-1,1)))),\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=MLPRegressor(), func=np.log, inverse_func=np.exp))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820757",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ada50733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:38.206163Z",
     "start_time": "2025-12-07T17:41:38.200439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making an adjusted R2 function:\n",
    "\n",
    "def adjusted_r2_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    n, p = X.shape\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1) # erros\n",
    "\n",
    "adj_r2 = make_scorer(adjusted_r2_scorer, greater_is_better=True) #erros\n",
    "\n",
    "\n",
    "scoring = { 'R2': 'r2', #'AdjR2': adj_r2 -> esta função é difícil de implementar, erros\n",
    "    'MAE': 'neg_mean_absolute_error',\n",
    "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "    'MedAE': 'neg_median_absolute_error',\n",
    "    'RMSE': 'neg_root_mean_squared_error'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b52ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:41:43.099792Z",
     "start_time": "2025-12-07T17:41:43.090213Z"
    }
   },
   "outputs": [],
   "source": [
    "param_distributions_huber = {\n",
    "    'regressor__epsilon': [1.1, 1.2, 1.35, 1.5, 2.0, 2.5, 3.0],\n",
    "    'regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0],\n",
    "    'regressor__fit_intercept': [True],\n",
    "    'regressor__max_iter': [500, 1000, 2000]\n",
    "\n",
    "}\n",
    "\n",
    "param_distributions_huber_logprice = {\n",
    "    'regressor__regressor__epsilon': [1.1, 1.2, 1.35, 1.5, 2.0, 2.5, 3.0],\n",
    "    'regressor__regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0],\n",
    "    'regressor__regressor__fit_intercept': [True],\n",
    "    'regressor__regressor__max_iter': [500, 1000, 2000]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "param_distributions_mlp_adam = {\n",
    "    'regressor__regressor__solver' : ['adam'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16), (200, 100), (400, 200), (100,50,25)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.001, 0.01, 0.1],\n",
    "    'feature selection__rfe_k': list(range(1,15)),\n",
    "    'feature selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "    \n",
    "}\n",
    "\n",
    "param_distributions_mlp_sgd = { \n",
    "    'regressor__regressor__solver' : ['sgd'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16),(100,50), (200,100), (100,50,25), (200,100,50)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate' :  ['constant','invscaling','adaptive'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.01, 0.001, 0.0001, 0.00001],\n",
    "    'regressor__regressor__batch_size' : [100, 200, 500],\n",
    "    'regressor__regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "    'feature selection__rfe_k': list(range(1,23)),\n",
    "    'feature selection__spearman_thr': [0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "param_distributions_mlp_adam_logprice = {\n",
    "    'regressor__regressor__solver' : ['adam'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16), (200, 100), (600, 200), (400,200,100), (300,200,100), (200,100,50), (100,50,25), (600,300,150)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.0001, 0.001, 0.01, 0.1],\n",
    "    'feature selection__rfe_k': list(range(1,23)),\n",
    "    'feature selection__spearman_thr': [0.2, 0.25, 0.3]\n",
    "    \n",
    "}\n",
    "\n",
    "param_distributions_mlp_sgd_logprice = { \n",
    "    'regressor__regressor__solver' : ['sgd'],\n",
    "    'regressor__regressor__hidden_layer_sizes' : [(32,16), (100,50),(200,100), (100,50,25), (200,100,50)],\n",
    "    'regressor__regressor__max_iter' :  [700],\n",
    "    'regressor__regressor__activation' : ['relu', 'tanh', 'logistic'],\n",
    "    'regressor__regressor__learning_rate' :  ['constant','invscaling','adaptive'],\n",
    "    'regressor__regressor__learning_rate_init' : [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'regressor__regressor__batch_size' : [100, 200, 500],\n",
    "    'regressor__regressor__alpha': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "    'feature selection__rfe_k': list(range(1,23)),\n",
    "    'feature selection__spearman_thr': [0.2, 0.25, 0.3],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def random_search (pipeline, param_distributions, n_iter=10) :\n",
    "    return RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,             # 10 random combinations of parameters -> reduced to 2 just for testing\n",
    "        scoring=scoring, # evaluation metrics\n",
    "        refit = 'MAE',\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=random_state),                 # 10-fold CV -> mudei para 5 \n",
    "        verbose=3, # to show iterations\n",
    "        return_train_score=True, # to return train metric results in cv_scores_\n",
    "        random_state=random_state, # defined on top of the nb\n",
    "        n_jobs=-1 \n",
    "    )\n",
    "\n",
    "rs_huber = random_search(\n",
    "    pipeline=pipeline_huber,\n",
    "    param_distributions=param_distributions_huber)\n",
    "\n",
    "rs_huber_logprice = random_search(\n",
    "    pipeline=pipeline_huber_logprice,\n",
    "    param_distributions=param_distributions_huber_logprice)\n",
    "\n",
    "rs_mlp_adam = random_search(pipeline = pipeline_mlp, param_distributions=param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_logprice = random_search(pipeline = pipeline_mlp_logprice, param_distributions=param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "rs_mlp_adam_robust = random_search(pipeline = pipeline_mlp_robust, param_distributions=param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_robust_logprice = random_search(pipeline = pipeline_mlp_robust_logprice, param_distributions=param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "rs_mlp_adam_minmax = random_search(pipeline = pipeline_mlp_minmax, param_distributions=param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_minmax_logprice = random_search(pipeline = pipeline_mlp_minmax_logprice, param_distributions=param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "rs_mlp_adam_minmax2 = random_search(pipeline = pipeline_mlp_minmax2, param_distributions = param_distributions_mlp_adam, n_iter=30)\n",
    "rs_mlp_adam_minmax2_logprice = random_search(pipeline = pipeline_mlp_minmax2_logprice, param_distributions = param_distributions_mlp_adam_logprice, n_iter=30)\n",
    "\n",
    "rs_mlp_sgd = random_search(pipeline = pipeline_mlp, param_distributions=param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_logprice = random_search(pipeline = pipeline_mlp_logprice, param_distributions=param_distributions_mlp_sgd_logprice, n_iter=30)\n",
    "rs_mlp_sgd_robust = random_search(pipeline = pipeline_mlp_robust, param_distributions=param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_robust_logprice = random_search(pipeline = pipeline_mlp_robust_logprice, param_distributions=param_distributions_mlp_sgd_logprice, n_iter=30)\n",
    "rs_mlp_sgd_minmax = random_search(pipeline = pipeline_mlp_minmax, param_distributions=param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_minmax_logprice = random_search(pipeline = pipeline_mlp_minmax_logprice, param_distributions=param_distributions_mlp_sgd_logprice, n_iter=30)\n",
    "rs_mlp_sgd_minmax2 = random_search(pipeline = pipeline_mlp_minmax2, param_distributions = param_distributions_mlp_sgd, n_iter=30)\n",
    "rs_mlp_sgd_minmax2_logprice = random_search(pipeline = pipeline_mlp_minmax2_logprice, param_distributions = param_distributions_mlp_sgd_logprice, n_iter=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1055e41",
   "metadata": {},
   "source": [
    "### Running RnadomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a21c8d5b7814d95f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T06:42:12.295332Z",
     "start_time": "2025-12-07T00:54:43.847709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (600, 200), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1615.5530\n",
      "Overfit: -12.47%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 144\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1770.2732\n",
      "Overfit: -2.20%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 163\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1637.6231\n",
      "Overfit: -10.51%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 151\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1790.5498\n",
      "Overfit: -1.85%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 165\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1652.2762\n",
      "Overfit: -7.39%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 121\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1649.9947\n",
      "Overfit: -6.85%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 150\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1639.3994\n",
      "Overfit: -8.28%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 61\n",
      "Running RandomizedSearchCV with Pipeline with MLP...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1822.7295\n",
      "Overfit: -1.41%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 195\n"
     ]
    }
   ],
   "source": [
    "for rs in [rs_mlp_adam, rs_mlp_sgd, rs_mlp_adam_robust, rs_mlp_sgd_robust, rs_mlp_adam_minmax, rs_mlp_sgd_minmax, rs_mlp_adam_minmax2, rs_mlp_sgd_minmax2]:\n",
    "    print(\"Running RandomizedSearchCV with Pipeline with MLP...\")\n",
    "    rs.fit(X, y)\n",
    "\n",
    "    print(\"\\nRandomizedSearchCV Results:\")\n",
    "    print(f\"Best parameters: {rs.best_params_}\")\n",
    "    print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "    idx = rs.best_index_\n",
    "    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\n",
    "    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "    print(f' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed30b2",
   "metadata": {},
   "source": [
    "SEM LOG:\n",
    "Para adam a melhor opção sem overfit é robust scaler LR = 0.001, (200,100,50), relu. MAE -> 1639. Overfit -> 8.28 Nao vale a pena mexer nos parametros.\n",
    "Para standarscaler dá melhor mas dá overfit com (600,200), experimentar substituir isso por (400,200) e voltar a correr\n",
    "\n",
    "Para sgd o melhor é Standardscaler LR=0.01, lr 0 ADAPTIVE, (32,16), BATCH = 200, relu. MAE -> 1770, overfit -> 2.20\n",
    "\n",
    "\n",
    "\n",
    "COM LOG:\n",
    "Para adam o melhor é standarscaler, 0.001, (200,100,50), tahn. MAE -> 1962. Overfit -> 3% Experimentar aumentar complexidade !!\n",
    "Para sgd o melhor é MinMax com 0.1, adaptive, (200,100) batchsize = 200,, relu. MAE -> 1820. Overfit -> 1.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b011a186fe5dc0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T09:40:11.011888Z",
     "start_time": "2025-12-07T06:42:12.361400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1692.3330\n",
      "Overfit: -3.02%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 68\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 0.001, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1862.1595\n",
      "Overfit: -0.59%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 90\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1724.9555\n",
      "Overfit: -1.77%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 67\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.1, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (200, 100), 'regressor__regressor__batch_size': 200, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1820.8326\n",
      "Overfit: -1.49%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 113\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1771.9289\n",
      "Overfit: -0.96%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 57\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 0.001, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1985.0701\n",
      "Overfit: -0.41%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 101\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100, 50), 'regressor__regressor__activation': 'tanh'}\n",
      "Best CV score: -1741.4855\n",
      "Overfit: -1.83%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 60\n",
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (32, 16), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 0.001, 'regressor__regressor__activation': 'relu'}\n",
      "Best CV score: -1965.8694\n",
      "Overfit: -1.07%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 89\n"
     ]
    }
   ],
   "source": [
    "for rs in [rs_mlp_adam_logprice, rs_mlp_sgd_logprice, rs_mlp_adam_robust_logprice, rs_mlp_sgd_robust_logprice, rs_mlp_adam_minmax_logprice, rs_mlp_sgd_minmax_logprice, rs_mlp_adam_minmax2_logprice, rs_mlp_sgd_minmax2_logprice]:\n",
    "    print(\"Running RandomizedSearchCV with Pipeline with MLP and Log Price...\")\n",
    "    rs.fit(X, y)\n",
    "\n",
    "    print(\"\\nRandomizedSearchCV Results:\")\n",
    "    print(f\"Best parameters: {rs.best_params_}\")\n",
    "    print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "    idx = rs.best_index_\n",
    "    train_mae = rs.cv_results_[\"mean_train_MAE\"][idx]\n",
    "    overfit = (rs.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "    print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "    print(f' Number of iterations until convergence : {rs.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af5dcc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price, StandardScaler...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.0001, 'regressor__regressor__hidden_layer_sizes': (600, 300, 150), 'regressor__regressor__activation': 'relu', 'feature selection__spearman_thr': 0.2, 'feature selection__rfe_k': 14}\n",
      "Best CV score: -1609.5665\n",
      "Overfit: -7.10%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 82\n"
     ]
    }
   ],
   "source": [
    "print(\"Running RandomizedSearchCV with Pipeline with MLP and Log Price, StandardScaler...\")\n",
    "rs_mlp_adam_logprice.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_mlp_adam_logprice.best_params_}\")\n",
    "print(f\"Best CV score: {rs_mlp_adam_logprice.best_score_:.4f}\")\n",
    "idx = rs_mlp_adam_logprice.best_index_\n",
    "train_mae = rs_mlp_adam_logprice.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_mlp_adam_logprice.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_mlp_adam_logprice.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fa52aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP, StandardScaler...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'adam', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.001, 'regressor__regressor__hidden_layer_sizes': (200, 100), 'regressor__regressor__activation': 'relu', 'feature selection__spearman_thr': 0.2, 'feature selection__rfe_k': 20}\n",
      "Best CV score: -1488.7886\n",
      "Overfit: -15.41%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 105\n"
     ]
    }
   ],
   "source": [
    "print(\"Running RandomizedSearchCV with Pipeline with MLP, StandardScaler...\")\n",
    "rs_mlp_adam.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_mlp_adam.best_params_}\")\n",
    "print(f\"Best CV score: {rs_mlp_adam.best_score_:.4f}\")\n",
    "idx = rs_mlp_adam.best_index_\n",
    "train_mae = rs_mlp_adam.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_mlp_adam.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_mlp_adam.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5393aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP, StandardScaler...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (100, 50), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu', 'feature selection__spearman_thr': 0.2, 'feature selection__rfe_k': 11}\n",
      "Best CV score: -1512.9584\n",
      "Overfit: -4.13%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 176\n"
     ]
    }
   ],
   "source": [
    "print(\"Running RandomizedSearchCV with Pipeline with MLP, StandardScaler...\")\n",
    "rs_mlp_sgd.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_mlp_sgd.best_params_}\")\n",
    "print(f\"Best CV score: {rs_mlp_sgd.best_score_:.4f}\")\n",
    "idx = rs_mlp_sgd.best_index_\n",
    "train_mae = rs_mlp_sgd.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_mlp_sgd.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_mlp_sgd.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4550d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV with Pipeline with MLP and Log Price, StandardScaler...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 700, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (200, 100), 'regressor__regressor__batch_size': 500, 'regressor__regressor__alpha': 0.001, 'regressor__regressor__activation': 'relu', 'feature selection__spearman_thr': 0.3, 'feature selection__rfe_k': 21}\n",
      "Best CV score: -1985.1100\n",
      "Overfit: -0.56%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Running RandomizedSearchCV with Pipeline with MLP and Log Price, StandardScaler...\")\n",
    "rs_mlp_sgd_minmax_logprice.fit(X, y)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {rs_mlp_sgd_minmax_logprice.best_params_}\")\n",
    "print(f\"Best CV score: {rs_mlp_sgd_minmax_logprice.best_score_:.4f}\")\n",
    "idx = rs_mlp_sgd_minmax_logprice.best_index_\n",
    "train_mae = rs_mlp_sgd_minmax_logprice.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_mlp_sgd_minmax_logprice.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_mlp_sgd_minmax_logprice.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbdd2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'regressor__regressor__solver': 'sgd', 'regressor__regressor__max_iter': 300, 'regressor__regressor__learning_rate_init': 0.01, 'regressor__regressor__learning_rate': 'adaptive', 'regressor__regressor__hidden_layer_sizes': (200, 100), 'regressor__regressor__batch_size': 100, 'regressor__regressor__alpha': 1e-06, 'regressor__regressor__activation': 'relu', 'feature selection__spearman_thr': 0.2, 'feature selection__rfe_k': 11}\n",
      "Best CV score: -1461.8309\n",
      "Overfit: -6.26%\n",
      "\n",
      "\n",
      " Number of iterations until convergence : 219\n"
     ]
    }
   ],
   "source": [
    "param_distribution_sgd_teste = {'regressor__regressor__solver': ['sgd'], 'regressor__regressor__max_iter': [300], 'regressor__regressor__learning_rate_init': [0.01], 'regressor__regressor__learning_rate': ['adaptive'], 'regressor__regressor__hidden_layer_sizes': [(200,100)], 'regressor__regressor__batch_size': [100], 'regressor__regressor__alpha': [1e-06], 'regressor__regressor__activation': ['relu'], 'feature selection__spearman_thr': [0.2], 'feature selection__rfe_k': [11]}\n",
    "rs_mlp_sgd = random_search(pipeline = pipeline_mlp, param_distributions=param_distribution_sgd_teste, n_iter=1)\n",
    "rs_mlp_sgd.fit(X, y)\n",
    "print(f\"Best parameters: {rs_mlp_sgd.best_params_}\")\n",
    "print(f\"Best CV score: {rs_mlp_sgd.best_score_:.4f}\")\n",
    "idx = rs_mlp_sgd.best_index_\n",
    "train_mae = rs_mlp_sgd.cv_results_[\"mean_train_MAE\"][idx]\n",
    "overfit = (rs_mlp_sgd.best_score_ - train_mae) / abs(train_mae) * 100\n",
    "print(f\"Overfit: {overfit:.2f}%\\n\\n\")\n",
    "print(f' Number of iterations until convergence : {rs_mlp_sgd.best_estimator_.named_steps[\"regressor\"].regressor_.n_iter_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c27001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'mileage',\n",
       " 'mpg',\n",
       " 'engineSize',\n",
       " 'carAge',\n",
       " 'carSegment',\n",
       " 'model_cleaned_encoded',\n",
       " 'Brand_cleaned_encoded',\n",
       " 'fuelType_cleaned_DIESEL',\n",
       " 'transmission_cleaned_MANUAL']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_mlp_adam.best_estimator_.named_steps['feature selection'].selected_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49c915c0b1112c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T17:22:27.796736Z",
     "start_time": "2025-12-07T17:22:27.751721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelo_mlp_teste.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rs_teste.best_estimator_, \"modelo_mlp_teste.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f094e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': ['year',\n",
       "  'mileage',\n",
       "  'tax',\n",
       "  'engineSize',\n",
       "  'carAge',\n",
       "  'AvgUsage',\n",
       "  'carSegment',\n",
       "  'model_cleaned_encoded',\n",
       "  'Brand_cleaned_encoded',\n",
       "  'fuelType_cleaned_DIESEL',\n",
       "  'fuelType_cleaned_PETROL',\n",
       "  'transmission_cleaned_AUTOMATIC',\n",
       "  'transmission_cleaned_MANUAL'],\n",
       " 'mae': 2623.6985387785553}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline = rs_mlp_adam.best_estimator_\n",
    "selector = best_pipeline.named_steps['feature selection']\n",
    "\n",
    "# Máscara de features selecionadas pelo fit no treino completo\n",
    "selected_mask = selector.best_\n",
    "selected_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb663b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 1000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 2000, 'regressor__fit_...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "      <td>{'regressor__max_iter': 500, 'regressor__fit_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <td>2551.6071</td>\n",
       "      <td>2530.1854</td>\n",
       "      <td>2541.2493</td>\n",
       "      <td>2528.0639</td>\n",
       "      <td>2568.8685</td>\n",
       "      <td>2930.3284</td>\n",
       "      <td>2921.043</td>\n",
       "      <td>2530.1849</td>\n",
       "      <td>2527.2113</td>\n",
       "      <td>2568.1785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <td>2556.4507</td>\n",
       "      <td>2535.5577</td>\n",
       "      <td>2546.6487</td>\n",
       "      <td>2533.3778</td>\n",
       "      <td>2574.0082</td>\n",
       "      <td>2933.3653</td>\n",
       "      <td>2924.4678</td>\n",
       "      <td>2535.5573</td>\n",
       "      <td>2532.4386</td>\n",
       "      <td>2574.1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_R2</th>\n",
       "      <td>0.789378</td>\n",
       "      <td>0.796883</td>\n",
       "      <td>0.800425</td>\n",
       "      <td>0.795421</td>\n",
       "      <td>0.7979</td>\n",
       "      <td>0.716265</td>\n",
       "      <td>0.725378</td>\n",
       "      <td>0.796882</td>\n",
       "      <td>0.792719</td>\n",
       "      <td>0.805038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_R2</th>\n",
       "      <td>0.788012</td>\n",
       "      <td>0.795319</td>\n",
       "      <td>0.798866</td>\n",
       "      <td>0.793867</td>\n",
       "      <td>0.796481</td>\n",
       "      <td>0.715462</td>\n",
       "      <td>0.72454</td>\n",
       "      <td>0.795318</td>\n",
       "      <td>0.791204</td>\n",
       "      <td>0.803422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAPE</th>\n",
       "      <td>0.1702</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>0.1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAPE</th>\n",
       "      <td>0.1702</td>\n",
       "      <td>0.1722</td>\n",
       "      <td>0.1757</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.1813</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.1722</td>\n",
       "      <td>0.1687</td>\n",
       "      <td>0.1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MedAE</th>\n",
       "      <td>1662.3472</td>\n",
       "      <td>1662.0865</td>\n",
       "      <td>1699.9093</td>\n",
       "      <td>1649.5007</td>\n",
       "      <td>1734.4398</td>\n",
       "      <td>1927.5141</td>\n",
       "      <td>1963.5399</td>\n",
       "      <td>1662.0101</td>\n",
       "      <td>1632.2679</td>\n",
       "      <td>1749.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MedAE</th>\n",
       "      <td>1664.4162</td>\n",
       "      <td>1661.8458</td>\n",
       "      <td>1699.8727</td>\n",
       "      <td>1650.2333</td>\n",
       "      <td>1737.7184</td>\n",
       "      <td>1930.4911</td>\n",
       "      <td>1967.0569</td>\n",
       "      <td>1661.8527</td>\n",
       "      <td>1632.2834</td>\n",
       "      <td>1752.9066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <td>4468.5152</td>\n",
       "      <td>4388.1669</td>\n",
       "      <td>4349.7284</td>\n",
       "      <td>4403.9355</td>\n",
       "      <td>4377.1677</td>\n",
       "      <td>5186.4866</td>\n",
       "      <td>5102.5168</td>\n",
       "      <td>4388.1715</td>\n",
       "      <td>4432.9214</td>\n",
       "      <td>4299.1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <td>4482.3739</td>\n",
       "      <td>4404.3162</td>\n",
       "      <td>4365.9043</td>\n",
       "      <td>4419.9336</td>\n",
       "      <td>4391.7751</td>\n",
       "      <td>5193.5176</td>\n",
       "      <td>5109.9795</td>\n",
       "      <td>4404.321</td>\n",
       "      <td>4448.4204</td>\n",
       "      <td>4315.9651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  0  ...                                                  9\n",
       "params            {'regressor__max_iter': 2000, 'regressor__fit_...  ...  {'regressor__max_iter': 500, 'regressor__fit_i...\n",
       "mean_train_MAE                                            2551.6071  ...                                          2568.1785\n",
       "mean_test_MAE                                             2556.4507  ...                                          2574.1165\n",
       "mean_train_R2                                              0.789378  ...                                           0.805038\n",
       "mean_test_R2                                               0.788012  ...                                           0.803422\n",
       "mean_train_MAPE                                              0.1702  ...                                             0.1814\n",
       "mean_test_MAPE                                               0.1702  ...                                             0.1813\n",
       "mean_train_MedAE                                          1662.3472  ...                                          1749.0065\n",
       "mean_test_MedAE                                           1664.4162  ...                                          1752.9066\n",
       "mean_train_RMSE                                           4468.5152  ...                                          4299.1549\n",
       "mean_test_RMSE                                            4482.3739  ...                                          4315.9651\n",
       "\n",
       "[11 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = {}\n",
    "df_metrics = {}\n",
    "\n",
    "for rs in random_searches:\n",
    "    results_df[f'results_{rs}'] = pd.DataFrame(rs.cv_results_)\n",
    "\n",
    "\n",
    "    metric_cols_train_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_R2\")]\n",
    "    metric_cols_test_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_R2\")]\n",
    "\n",
    "    metric_cols_train_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_MAE\")]\n",
    "    metric_cols_test_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_MAE\")]\n",
    "    metric_cols_train_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_MAPE\")]\n",
    "    metric_cols_test_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_MAPE\")]\n",
    "    metric_cols_train_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_MedAE\")]\n",
    "    metric_cols_test_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_MedAE\")]\n",
    "    metric_cols_train_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_train_RMSE\")]\n",
    "    metric_cols_test_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"mean_test_RMSE\")]\n",
    "\n",
    "    std_cols_train_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_R2\")]\n",
    "    std_cols_test_R2 = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_R2\")]\n",
    "\n",
    "    std_cols_train_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_MAE\")]\n",
    "    std_cols_test_MAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_MAE\")]\n",
    "    std_cols_train_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_MAPE\")]\n",
    "    std_cols_test_MAPE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_MAPE\")]\n",
    "    std_cols_train_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_MedAE\")]\n",
    "    std_cols_test_MedAE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_MedAE\")]\n",
    "    std_cols_train_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_train_RMSE\")]\n",
    "    std_cols_test_RMSE = [c for c in results_df[f'results_{rs}'].columns if c.startswith(\"std_test_RMSE\")]\n",
    "\n",
    "\n",
    "    df_metrics[f'{rs}'] = results_df[f'results_{rs}'][[\"params\"]+ metric_cols_train_MAE + metric_cols_test_MAE +\n",
    "                        metric_cols_train_R2 + metric_cols_test_R2 + \n",
    "                        metric_cols_train_MAPE + metric_cols_test_MAPE + \n",
    "                        metric_cols_train_MedAE + metric_cols_test_MedAE + \n",
    "                        metric_cols_train_RMSE + metric_cols_test_RMSE ]\n",
    "    df_metrics[f'{rs}'] = df_metrics[f'{rs}'].loc[:,['mean_train_MAE', 'mean_test_MAE',\n",
    "                        'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                        'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                        'mean_train_RMSE', 'mean_test_RMSE' ]] = df_metrics[f'{rs}'].loc[:,\n",
    "                                                                                          ['mean_train_MAE', 'mean_test_MAE',\n",
    "                        'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                        'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                        'mean_train_RMSE', 'mean_test_RMSE' ]] .round(4) * -1\n",
    "                        \n",
    "df_metrics[random_search_huber_standardscaler].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca59d90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <td>2079.6791</td>\n",
       "      <td>1963.3101</td>\n",
       "      <td>1964.0531</td>\n",
       "      <td>2149.1718</td>\n",
       "      <td>7794.1649</td>\n",
       "      <td>2364.6067</td>\n",
       "      <td>2136.3948</td>\n",
       "      <td>2273.3245</td>\n",
       "      <td>2222.3959</td>\n",
       "      <td>2277.5946</td>\n",
       "      <td>2230.2857</td>\n",
       "      <td>3039.7676</td>\n",
       "      <td>2369.0044</td>\n",
       "      <td>2869.7785</td>\n",
       "      <td>7007.7978</td>\n",
       "      <td>1737.1833</td>\n",
       "      <td>2411.2841</td>\n",
       "      <td>2685.4600</td>\n",
       "      <td>1775.2054</td>\n",
       "      <td>2458.9122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <td>2087.4618</td>\n",
       "      <td>1973.2623</td>\n",
       "      <td>1970.4834</td>\n",
       "      <td>2154.5462</td>\n",
       "      <td>7780.8247</td>\n",
       "      <td>2370.8310</td>\n",
       "      <td>2144.6654</td>\n",
       "      <td>2278.0469</td>\n",
       "      <td>2227.0507</td>\n",
       "      <td>2289.4033</td>\n",
       "      <td>2236.2657</td>\n",
       "      <td>3040.9516</td>\n",
       "      <td>2375.0590</td>\n",
       "      <td>2870.3143</td>\n",
       "      <td>7007.9053</td>\n",
       "      <td>1772.5296</td>\n",
       "      <td>2420.0104</td>\n",
       "      <td>2685.3512</td>\n",
       "      <td>1794.1628</td>\n",
       "      <td>2460.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAPE</th>\n",
       "      <td>0.1329</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.1376</td>\n",
       "      <td>0.6744</td>\n",
       "      <td>0.1569</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.2019</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>0.5594</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.1546</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAPE</th>\n",
       "      <td>0.1334</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.6739</td>\n",
       "      <td>0.1569</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1762</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.1634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MedAE</th>\n",
       "      <td>1408.1102</td>\n",
       "      <td>1295.1288</td>\n",
       "      <td>1300.6553</td>\n",
       "      <td>1458.2160</td>\n",
       "      <td>7081.9171</td>\n",
       "      <td>1623.4914</td>\n",
       "      <td>1446.8812</td>\n",
       "      <td>1597.3952</td>\n",
       "      <td>1501.2109</td>\n",
       "      <td>1606.1462</td>\n",
       "      <td>1553.3995</td>\n",
       "      <td>2182.1724</td>\n",
       "      <td>1657.3299</td>\n",
       "      <td>2034.7487</td>\n",
       "      <td>5879.2173</td>\n",
       "      <td>1124.1663</td>\n",
       "      <td>1693.1318</td>\n",
       "      <td>1889.7226</td>\n",
       "      <td>1153.2974</td>\n",
       "      <td>1762.9708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MedAE</th>\n",
       "      <td>1413.4873</td>\n",
       "      <td>1293.9959</td>\n",
       "      <td>1301.2434</td>\n",
       "      <td>1456.5420</td>\n",
       "      <td>7059.7745</td>\n",
       "      <td>1626.0657</td>\n",
       "      <td>1452.2277</td>\n",
       "      <td>1593.0016</td>\n",
       "      <td>1502.8436</td>\n",
       "      <td>1608.2543</td>\n",
       "      <td>1554.8086</td>\n",
       "      <td>2183.4487</td>\n",
       "      <td>1658.3161</td>\n",
       "      <td>2032.9938</td>\n",
       "      <td>5879.6406</td>\n",
       "      <td>1141.3689</td>\n",
       "      <td>1699.3119</td>\n",
       "      <td>1884.8615</td>\n",
       "      <td>1160.2152</td>\n",
       "      <td>1769.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <td>3568.5671</td>\n",
       "      <td>3407.0603</td>\n",
       "      <td>3403.1448</td>\n",
       "      <td>3648.1394</td>\n",
       "      <td>10174.9308</td>\n",
       "      <td>3973.4080</td>\n",
       "      <td>3663.6794</td>\n",
       "      <td>3830.7770</td>\n",
       "      <td>3822.0037</td>\n",
       "      <td>3735.6462</td>\n",
       "      <td>3777.8817</td>\n",
       "      <td>4982.2623</td>\n",
       "      <td>4008.4188</td>\n",
       "      <td>4829.9549</td>\n",
       "      <td>9717.7719</td>\n",
       "      <td>3019.7121</td>\n",
       "      <td>3986.6194</td>\n",
       "      <td>4536.1180</td>\n",
       "      <td>3077.8644</td>\n",
       "      <td>3944.9332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <td>3589.7293</td>\n",
       "      <td>3437.0938</td>\n",
       "      <td>3424.4586</td>\n",
       "      <td>3669.9601</td>\n",
       "      <td>10164.1989</td>\n",
       "      <td>3993.0938</td>\n",
       "      <td>3681.1151</td>\n",
       "      <td>3842.3295</td>\n",
       "      <td>3834.5201</td>\n",
       "      <td>3759.9846</td>\n",
       "      <td>3792.1205</td>\n",
       "      <td>4985.3482</td>\n",
       "      <td>4018.5090</td>\n",
       "      <td>4822.8579</td>\n",
       "      <td>9716.1640</td>\n",
       "      <td>3140.5348</td>\n",
       "      <td>4000.7682</td>\n",
       "      <td>4537.6439</td>\n",
       "      <td>3154.3823</td>\n",
       "      <td>3955.3592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0          1          2          3           4   \\\n",
       "mean_train_MAE    2079.6791  1963.3101  1964.0531  2149.1718   7794.1649   \n",
       "mean_test_MAE     2087.4618  1973.2623  1970.4834  2154.5462   7780.8247   \n",
       "mean_train_MAPE      0.1329     0.1217     0.1220     0.1376      0.6744   \n",
       "mean_test_MAPE       0.1334     0.1223     0.1224     0.1379      0.6739   \n",
       "mean_train_MedAE  1408.1102  1295.1288  1300.6553  1458.2160   7081.9171   \n",
       "mean_test_MedAE   1413.4873  1293.9959  1301.2434  1456.5420   7059.7745   \n",
       "mean_train_RMSE   3568.5671  3407.0603  3403.1448  3648.1394  10174.9308   \n",
       "mean_test_RMSE    3589.7293  3437.0938  3424.4586  3669.9601  10164.1989   \n",
       "\n",
       "                         5          6          7          8          9   \\\n",
       "mean_train_MAE    2364.6067  2136.3948  2273.3245  2222.3959  2277.5946   \n",
       "mean_test_MAE     2370.8310  2144.6654  2278.0469  2227.0507  2289.4033   \n",
       "mean_train_MAPE      0.1569     0.1372     0.1480     0.1411     0.1495   \n",
       "mean_test_MAPE       0.1569     0.1378     0.1483     0.1413     0.1505   \n",
       "mean_train_MedAE  1623.4914  1446.8812  1597.3952  1501.2109  1606.1462   \n",
       "mean_test_MedAE   1626.0657  1452.2277  1593.0016  1502.8436  1608.2543   \n",
       "mean_train_RMSE   3973.4080  3663.6794  3830.7770  3822.0037  3735.6462   \n",
       "mean_test_RMSE    3993.0938  3681.1151  3842.3295  3834.5201  3759.9846   \n",
       "\n",
       "                         10         11         12         13         14  \\\n",
       "mean_train_MAE    2230.2857  3039.7676  2369.0044  2869.7785  7007.7978   \n",
       "mean_test_MAE     2236.2657  3040.9516  2375.0590  2870.3143  7007.9053   \n",
       "mean_train_MAPE      0.1448     0.2019     0.1551     0.1871     0.5594   \n",
       "mean_test_MAPE       0.1452     0.2017     0.1551     0.1875     0.5595   \n",
       "mean_train_MedAE  1553.3995  2182.1724  1657.3299  2034.7487  5879.2173   \n",
       "mean_test_MedAE   1554.8086  2183.4487  1658.3161  2032.9938  5879.6406   \n",
       "mean_train_RMSE   3777.8817  4982.2623  4008.4188  4829.9549  9717.7719   \n",
       "mean_test_RMSE    3792.1205  4985.3482  4018.5090  4822.8579  9716.1640   \n",
       "\n",
       "                         15         16         17         18         19  \n",
       "mean_train_MAE    1737.1833  2411.2841  2685.4600  1775.2054  2458.9122  \n",
       "mean_test_MAE     1772.5296  2420.0104  2685.3512  1794.1628  2460.8917  \n",
       "mean_train_MAPE      0.1075     0.1546     0.1760     0.1094     0.1632  \n",
       "mean_test_MAPE       0.1094     0.1551     0.1762     0.1106     0.1634  \n",
       "mean_train_MedAE  1124.1663  1693.1318  1889.7226  1153.2974  1762.9708  \n",
       "mean_test_MedAE   1141.3689  1699.3119  1884.8615  1160.2152  1769.0095  \n",
       "mean_train_RMSE   3019.7121  3986.6194  4536.1180  3077.8644  3944.9332  \n",
       "mean_test_RMSE    3140.5348  4000.7682  4537.6439  3154.3823  3955.3592  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results_df_mlp_sgd = pd.DataFrame(rs_mlp_sgd.cv_results_)\n",
    "\n",
    "\n",
    "metric_cols_train_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_R2\")]\n",
    "metric_cols_test_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_R2\")]\n",
    "metric_cols_train_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_MAE\")]\n",
    "metric_cols_test_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_MAE\")]\n",
    "metric_cols_train_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_MAPE\")]\n",
    "metric_cols_test_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_MAPE\")]\n",
    "metric_cols_train_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_MedAE\")]\n",
    "metric_cols_test_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_MedAE\")]\n",
    "metric_cols_train_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_train_RMSE\")]\n",
    "metric_cols_test_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"mean_test_RMSE\")]\n",
    "\n",
    "std_cols_train_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_R2\")]\n",
    "std_cols_test_R2 = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_R2\")]\n",
    "\n",
    "std_cols_train_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_MAE\")]\n",
    "std_cols_test_MAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_MAE\")]\n",
    "std_cols_train_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_MAPE\")]\n",
    "std_cols_test_MAPE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_MAPE\")]\n",
    "std_cols_train_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_MedAE\")]\n",
    "std_cols_test_MedAE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_MedAE\")]\n",
    "std_cols_train_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_train_RMSE\")]\n",
    "std_cols_test_RMSE = [c for c in results_df_mlp_sgd.columns if c.startswith(\"std_test_RMSE\")]\n",
    "\n",
    "df_metrics_mlp_sgd = results_df_mlp_sgd[[\"params\"]+ metric_cols_train_MAE + metric_cols_test_MAE +\n",
    "                    metric_cols_train_R2 + metric_cols_test_R2 + \n",
    "                    metric_cols_train_MAPE + metric_cols_test_MAPE + \n",
    "                    metric_cols_train_MedAE + metric_cols_test_MedAE + \n",
    "                    metric_cols_train_RMSE + metric_cols_test_RMSE ]\n",
    "df_metrics_mlp_sgd = df_metrics_mlp_sgd.loc[:,['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] = df_metrics_mlp_sgd.loc[:,\n",
    "                                                                                        ['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] .round(4) * -1\n",
    "                    \n",
    "df_metrics_mlp_sgd.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cad0e2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__solver</th>\n",
       "      <th>param_regressor__regressor__max_iter</th>\n",
       "      <th>param_regressor__regressor__learning_rate_init</th>\n",
       "      <th>param_regressor__regressor__learning_rate</th>\n",
       "      <th>param_regressor__regressor__hidden_layer_sizes</th>\n",
       "      <th>param_regressor__regressor__batch_size</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <th>std_test_RMSE</th>\n",
       "      <th>rank_test_RMSE</th>\n",
       "      <th>split0_train_RMSE</th>\n",
       "      <th>split1_train_RMSE</th>\n",
       "      <th>split2_train_RMSE</th>\n",
       "      <th>split3_train_RMSE</th>\n",
       "      <th>split4_train_RMSE</th>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <th>std_train_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>558.028797</td>\n",
       "      <td>18.976123</td>\n",
       "      <td>16.200441</td>\n",
       "      <td>4.609916</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3589.729294</td>\n",
       "      <td>278.795294</td>\n",
       "      <td>5</td>\n",
       "      <td>-3623.567015</td>\n",
       "      <td>-3573.751681</td>\n",
       "      <td>-3454.721581</td>\n",
       "      <td>-3579.404931</td>\n",
       "      <td>-3611.390420</td>\n",
       "      <td>-3568.567126</td>\n",
       "      <td>59.939917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>377.705706</td>\n",
       "      <td>27.098480</td>\n",
       "      <td>17.470885</td>\n",
       "      <td>2.636715</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3437.093756</td>\n",
       "      <td>259.384596</td>\n",
       "      <td>4</td>\n",
       "      <td>-3469.404762</td>\n",
       "      <td>-3397.131378</td>\n",
       "      <td>-3312.293040</td>\n",
       "      <td>-3394.606579</td>\n",
       "      <td>-3461.865643</td>\n",
       "      <td>-3407.060280</td>\n",
       "      <td>56.789030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.362577</td>\n",
       "      <td>11.492277</td>\n",
       "      <td>14.026476</td>\n",
       "      <td>5.449640</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>constant</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3424.458603</td>\n",
       "      <td>275.950211</td>\n",
       "      <td>3</td>\n",
       "      <td>-3479.374764</td>\n",
       "      <td>-3390.819076</td>\n",
       "      <td>-3327.562300</td>\n",
       "      <td>-3381.370649</td>\n",
       "      <td>-3436.597421</td>\n",
       "      <td>-3403.144842</td>\n",
       "      <td>51.514729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>562.888881</td>\n",
       "      <td>32.192474</td>\n",
       "      <td>16.794674</td>\n",
       "      <td>5.258475</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3669.960146</td>\n",
       "      <td>254.503716</td>\n",
       "      <td>6</td>\n",
       "      <td>-3688.875532</td>\n",
       "      <td>-3667.719420</td>\n",
       "      <td>-3555.317770</td>\n",
       "      <td>-3626.307189</td>\n",
       "      <td>-3702.477223</td>\n",
       "      <td>-3648.139427</td>\n",
       "      <td>53.080899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.484686</td>\n",
       "      <td>14.066121</td>\n",
       "      <td>14.574779</td>\n",
       "      <td>5.498719</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-10164.198898</td>\n",
       "      <td>480.968476</td>\n",
       "      <td>20</td>\n",
       "      <td>-9828.447428</td>\n",
       "      <td>-10983.769140</td>\n",
       "      <td>-10127.229664</td>\n",
       "      <td>-9780.513070</td>\n",
       "      <td>-10154.694848</td>\n",
       "      <td>-10174.930830</td>\n",
       "      <td>431.861187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>742.741857</td>\n",
       "      <td>31.765148</td>\n",
       "      <td>15.924535</td>\n",
       "      <td>5.860255</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>constant</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3993.093789</td>\n",
       "      <td>271.414837</td>\n",
       "      <td>13</td>\n",
       "      <td>-3956.519959</td>\n",
       "      <td>-4006.918559</td>\n",
       "      <td>-3866.622497</td>\n",
       "      <td>-4029.580728</td>\n",
       "      <td>-4007.398314</td>\n",
       "      <td>-3973.408011</td>\n",
       "      <td>58.518414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>195.364304</td>\n",
       "      <td>14.925049</td>\n",
       "      <td>11.659679</td>\n",
       "      <td>1.476357</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3681.115114</td>\n",
       "      <td>272.993292</td>\n",
       "      <td>7</td>\n",
       "      <td>-3745.038658</td>\n",
       "      <td>-3671.663767</td>\n",
       "      <td>-3572.373330</td>\n",
       "      <td>-3612.991916</td>\n",
       "      <td>-3716.329262</td>\n",
       "      <td>-3663.679387</td>\n",
       "      <td>63.813958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>117.630982</td>\n",
       "      <td>11.805049</td>\n",
       "      <td>15.068488</td>\n",
       "      <td>4.435113</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3842.329474</td>\n",
       "      <td>304.426670</td>\n",
       "      <td>11</td>\n",
       "      <td>-3874.723954</td>\n",
       "      <td>-3810.474618</td>\n",
       "      <td>-3801.319241</td>\n",
       "      <td>-3835.471624</td>\n",
       "      <td>-3831.895370</td>\n",
       "      <td>-3830.776961</td>\n",
       "      <td>25.434509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>502.835241</td>\n",
       "      <td>21.121225</td>\n",
       "      <td>12.237389</td>\n",
       "      <td>2.824670</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>constant</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3834.520098</td>\n",
       "      <td>264.962000</td>\n",
       "      <td>10</td>\n",
       "      <td>-3894.702757</td>\n",
       "      <td>-3809.171791</td>\n",
       "      <td>-3751.647028</td>\n",
       "      <td>-3782.561085</td>\n",
       "      <td>-3871.936021</td>\n",
       "      <td>-3822.003737</td>\n",
       "      <td>53.756541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70.559527</td>\n",
       "      <td>8.421112</td>\n",
       "      <td>14.510177</td>\n",
       "      <td>5.434039</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3759.984618</td>\n",
       "      <td>217.862472</td>\n",
       "      <td>8</td>\n",
       "      <td>-3853.443162</td>\n",
       "      <td>-3758.214891</td>\n",
       "      <td>-3592.847103</td>\n",
       "      <td>-3690.804405</td>\n",
       "      <td>-3782.921461</td>\n",
       "      <td>-3735.646204</td>\n",
       "      <td>88.343408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>119.007178</td>\n",
       "      <td>14.859804</td>\n",
       "      <td>15.567739</td>\n",
       "      <td>5.415075</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-3792.120532</td>\n",
       "      <td>241.681687</td>\n",
       "      <td>9</td>\n",
       "      <td>-3919.490306</td>\n",
       "      <td>-3777.277536</td>\n",
       "      <td>-3683.860172</td>\n",
       "      <td>-3692.595831</td>\n",
       "      <td>-3816.184893</td>\n",
       "      <td>-3777.881747</td>\n",
       "      <td>86.757391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2062.089798</td>\n",
       "      <td>66.676741</td>\n",
       "      <td>12.017694</td>\n",
       "      <td>1.858810</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>constant</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-4985.348199</td>\n",
       "      <td>250.840935</td>\n",
       "      <td>18</td>\n",
       "      <td>-5021.883327</td>\n",
       "      <td>-4989.153576</td>\n",
       "      <td>-4926.294135</td>\n",
       "      <td>-4928.484502</td>\n",
       "      <td>-5045.496121</td>\n",
       "      <td>-4982.262332</td>\n",
       "      <td>48.250013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>76.815181</td>\n",
       "      <td>2.112804</td>\n",
       "      <td>13.010936</td>\n",
       "      <td>5.664640</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-4018.508965</td>\n",
       "      <td>308.762894</td>\n",
       "      <td>15</td>\n",
       "      <td>-4112.329323</td>\n",
       "      <td>-3873.059847</td>\n",
       "      <td>-4019.545043</td>\n",
       "      <td>-4017.510703</td>\n",
       "      <td>-4019.648940</td>\n",
       "      <td>-4008.418771</td>\n",
       "      <td>76.748961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>102.682580</td>\n",
       "      <td>5.330166</td>\n",
       "      <td>13.010984</td>\n",
       "      <td>5.228511</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-4822.857883</td>\n",
       "      <td>407.744154</td>\n",
       "      <td>17</td>\n",
       "      <td>-4535.058602</td>\n",
       "      <td>-4695.274182</td>\n",
       "      <td>-4840.506602</td>\n",
       "      <td>-4936.863420</td>\n",
       "      <td>-5142.071763</td>\n",
       "      <td>-4829.954914</td>\n",
       "      <td>206.910816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>92.377069</td>\n",
       "      <td>6.062701</td>\n",
       "      <td>14.577364</td>\n",
       "      <td>4.299658</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-9716.164033</td>\n",
       "      <td>200.087059</td>\n",
       "      <td>19</td>\n",
       "      <td>-9764.540348</td>\n",
       "      <td>-9723.633636</td>\n",
       "      <td>-9641.518478</td>\n",
       "      <td>-9688.663497</td>\n",
       "      <td>-9770.503693</td>\n",
       "      <td>-9717.771930</td>\n",
       "      <td>48.299427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>187.055395</td>\n",
       "      <td>18.112049</td>\n",
       "      <td>13.829041</td>\n",
       "      <td>5.512543</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3140.534828</td>\n",
       "      <td>256.483755</td>\n",
       "      <td>1</td>\n",
       "      <td>-3034.162892</td>\n",
       "      <td>-2988.991477</td>\n",
       "      <td>-2972.533508</td>\n",
       "      <td>-3040.844582</td>\n",
       "      <td>-3062.027954</td>\n",
       "      <td>-3019.712083</td>\n",
       "      <td>33.513027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>88.325405</td>\n",
       "      <td>2.962579</td>\n",
       "      <td>16.692462</td>\n",
       "      <td>3.844482</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>invscaling</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-4000.768172</td>\n",
       "      <td>266.618100</td>\n",
       "      <td>14</td>\n",
       "      <td>-4026.215663</td>\n",
       "      <td>-4181.243002</td>\n",
       "      <td>-3840.481320</td>\n",
       "      <td>-3849.992309</td>\n",
       "      <td>-4035.164729</td>\n",
       "      <td>-3986.619404</td>\n",
       "      <td>127.926666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>238.861156</td>\n",
       "      <td>22.098575</td>\n",
       "      <td>16.470852</td>\n",
       "      <td>3.634199</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-4537.643874</td>\n",
       "      <td>343.910177</td>\n",
       "      <td>16</td>\n",
       "      <td>-4576.559998</td>\n",
       "      <td>-4635.115021</td>\n",
       "      <td>-4521.681841</td>\n",
       "      <td>-4508.241726</td>\n",
       "      <td>-4438.991334</td>\n",
       "      <td>-4536.117984</td>\n",
       "      <td>66.113605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>459.071625</td>\n",
       "      <td>41.116077</td>\n",
       "      <td>15.115410</td>\n",
       "      <td>6.413855</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>constant</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3154.382267</td>\n",
       "      <td>239.660370</td>\n",
       "      <td>2</td>\n",
       "      <td>-3109.561561</td>\n",
       "      <td>-3058.253402</td>\n",
       "      <td>-3017.594393</td>\n",
       "      <td>-3075.214638</td>\n",
       "      <td>-3128.698123</td>\n",
       "      <td>-3077.864424</td>\n",
       "      <td>39.020158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>160.759178</td>\n",
       "      <td>13.574618</td>\n",
       "      <td>14.555457</td>\n",
       "      <td>6.235444</td>\n",
       "      <td>sgd</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>constant</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3955.359209</td>\n",
       "      <td>200.895114</td>\n",
       "      <td>12</td>\n",
       "      <td>-4118.835587</td>\n",
       "      <td>-3887.341703</td>\n",
       "      <td>-3795.147922</td>\n",
       "      <td>-3936.758375</td>\n",
       "      <td>-3986.582641</td>\n",
       "      <td>-3944.933245</td>\n",
       "      <td>107.513677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      558.028797     18.976123        16.200441        4.609916   \n",
       "1      377.705706     27.098480        17.470885        2.636715   \n",
       "2      195.362577     11.492277        14.026476        5.449640   \n",
       "3      562.888881     32.192474        16.794674        5.258475   \n",
       "4       80.484686     14.066121        14.574779        5.498719   \n",
       "5      742.741857     31.765148        15.924535        5.860255   \n",
       "6      195.364304     14.925049        11.659679        1.476357   \n",
       "7      117.630982     11.805049        15.068488        4.435113   \n",
       "8      502.835241     21.121225        12.237389        2.824670   \n",
       "9       70.559527      8.421112        14.510177        5.434039   \n",
       "10     119.007178     14.859804        15.567739        5.415075   \n",
       "11    2062.089798     66.676741        12.017694        1.858810   \n",
       "12      76.815181      2.112804        13.010936        5.664640   \n",
       "13     102.682580      5.330166        13.010984        5.228511   \n",
       "14      92.377069      6.062701        14.577364        4.299658   \n",
       "15     187.055395     18.112049        13.829041        5.512543   \n",
       "16      88.325405      2.962579        16.692462        3.844482   \n",
       "17     238.861156     22.098575        16.470852        3.634199   \n",
       "18     459.071625     41.116077        15.115410        6.413855   \n",
       "19     160.759178     13.574618        14.555457        6.235444   \n",
       "\n",
       "   param_regressor__regressor__solver  param_regressor__regressor__max_iter  \\\n",
       "0                                 sgd                                   700   \n",
       "1                                 sgd                                   700   \n",
       "2                                 sgd                                   700   \n",
       "3                                 sgd                                   700   \n",
       "4                                 sgd                                   700   \n",
       "5                                 sgd                                   700   \n",
       "6                                 sgd                                   700   \n",
       "7                                 sgd                                   700   \n",
       "8                                 sgd                                   700   \n",
       "9                                 sgd                                   700   \n",
       "10                                sgd                                   700   \n",
       "11                                sgd                                   700   \n",
       "12                                sgd                                   700   \n",
       "13                                sgd                                   700   \n",
       "14                                sgd                                   700   \n",
       "15                                sgd                                   700   \n",
       "16                                sgd                                   700   \n",
       "17                                sgd                                   700   \n",
       "18                                sgd                                   700   \n",
       "19                                sgd                                   700   \n",
       "\n",
       "    param_regressor__regressor__learning_rate_init  \\\n",
       "0                                          0.00010   \n",
       "1                                          0.00010   \n",
       "2                                          0.00100   \n",
       "3                                          0.00100   \n",
       "4                                          0.00010   \n",
       "5                                          0.00010   \n",
       "6                                          0.00010   \n",
       "7                                          0.01000   \n",
       "8                                          0.00010   \n",
       "9                                          0.01000   \n",
       "10                                         0.01000   \n",
       "11                                         0.00001   \n",
       "12                                         0.01000   \n",
       "13                                         0.00010   \n",
       "14                                         0.01000   \n",
       "15                                         0.01000   \n",
       "16                                         0.00100   \n",
       "17                                         0.00001   \n",
       "18                                         0.00100   \n",
       "19                                         0.00001   \n",
       "\n",
       "   param_regressor__regressor__learning_rate  \\\n",
       "0                                   adaptive   \n",
       "1                                   adaptive   \n",
       "2                                   constant   \n",
       "3                                   adaptive   \n",
       "4                                 invscaling   \n",
       "5                                   constant   \n",
       "6                                   adaptive   \n",
       "7                                 invscaling   \n",
       "8                                   constant   \n",
       "9                                 invscaling   \n",
       "10                                invscaling   \n",
       "11                                  constant   \n",
       "12                                invscaling   \n",
       "13                                invscaling   \n",
       "14                                invscaling   \n",
       "15                                  adaptive   \n",
       "16                                invscaling   \n",
       "17                                  adaptive   \n",
       "18                                  constant   \n",
       "19                                  constant   \n",
       "\n",
       "   param_regressor__regressor__hidden_layer_sizes  \\\n",
       "0                                      (200, 100)   \n",
       "1                                      (200, 100)   \n",
       "2                                   (100, 50, 25)   \n",
       "3                                      (200, 100)   \n",
       "4                                        (32, 16)   \n",
       "5                                      (200, 100)   \n",
       "6                                        (32, 16)   \n",
       "7                                      (200, 100)   \n",
       "8                                   (100, 50, 25)   \n",
       "9                                        (32, 16)   \n",
       "10                                     (200, 100)   \n",
       "11                                     (200, 100)   \n",
       "12                                       (32, 16)   \n",
       "13                                  (100, 50, 25)   \n",
       "14                                  (100, 50, 25)   \n",
       "15                                       (32, 16)   \n",
       "16                                  (100, 50, 25)   \n",
       "17                                       (32, 16)   \n",
       "18                                  (100, 50, 25)   \n",
       "19                                       (32, 16)   \n",
       "\n",
       "    param_regressor__regressor__batch_size  ...  mean_test_RMSE std_test_RMSE  \\\n",
       "0                                      200  ...    -3589.729294    278.795294   \n",
       "1                                      100  ...    -3437.093756    259.384596   \n",
       "2                                      500  ...    -3424.458603    275.950211   \n",
       "3                                      200  ...    -3669.960146    254.503716   \n",
       "4                                      500  ...   -10164.198898    480.968476   \n",
       "5                                      100  ...    -3993.093789    271.414837   \n",
       "6                                      200  ...    -3681.115114    272.993292   \n",
       "7                                      500  ...    -3842.329474    304.426670   \n",
       "8                                      100  ...    -3834.520098    264.962000   \n",
       "9                                      500  ...    -3759.984618    217.862472   \n",
       "10                                     500  ...    -3792.120532    241.681687   \n",
       "11                                     200  ...    -4985.348199    250.840935   \n",
       "12                                     500  ...    -4018.508965    308.762894   \n",
       "13                                     100  ...    -4822.857883    407.744154   \n",
       "14                                     500  ...    -9716.164033    200.087059   \n",
       "15                                     200  ...    -3140.534828    256.483755   \n",
       "16                                     200  ...    -4000.768172    266.618100   \n",
       "17                                     500  ...    -4537.643874    343.910177   \n",
       "18                                     100  ...    -3154.382267    239.660370   \n",
       "19                                     200  ...    -3955.359209    200.895114   \n",
       "\n",
       "   rank_test_RMSE  split0_train_RMSE  split1_train_RMSE  split2_train_RMSE  \\\n",
       "0               5       -3623.567015       -3573.751681       -3454.721581   \n",
       "1               4       -3469.404762       -3397.131378       -3312.293040   \n",
       "2               3       -3479.374764       -3390.819076       -3327.562300   \n",
       "3               6       -3688.875532       -3667.719420       -3555.317770   \n",
       "4              20       -9828.447428      -10983.769140      -10127.229664   \n",
       "5              13       -3956.519959       -4006.918559       -3866.622497   \n",
       "6               7       -3745.038658       -3671.663767       -3572.373330   \n",
       "7              11       -3874.723954       -3810.474618       -3801.319241   \n",
       "8              10       -3894.702757       -3809.171791       -3751.647028   \n",
       "9               8       -3853.443162       -3758.214891       -3592.847103   \n",
       "10              9       -3919.490306       -3777.277536       -3683.860172   \n",
       "11             18       -5021.883327       -4989.153576       -4926.294135   \n",
       "12             15       -4112.329323       -3873.059847       -4019.545043   \n",
       "13             17       -4535.058602       -4695.274182       -4840.506602   \n",
       "14             19       -9764.540348       -9723.633636       -9641.518478   \n",
       "15              1       -3034.162892       -2988.991477       -2972.533508   \n",
       "16             14       -4026.215663       -4181.243002       -3840.481320   \n",
       "17             16       -4576.559998       -4635.115021       -4521.681841   \n",
       "18              2       -3109.561561       -3058.253402       -3017.594393   \n",
       "19             12       -4118.835587       -3887.341703       -3795.147922   \n",
       "\n",
       "    split3_train_RMSE  split4_train_RMSE  mean_train_RMSE  std_train_RMSE  \n",
       "0        -3579.404931       -3611.390420     -3568.567126       59.939917  \n",
       "1        -3394.606579       -3461.865643     -3407.060280       56.789030  \n",
       "2        -3381.370649       -3436.597421     -3403.144842       51.514729  \n",
       "3        -3626.307189       -3702.477223     -3648.139427       53.080899  \n",
       "4        -9780.513070      -10154.694848    -10174.930830      431.861187  \n",
       "5        -4029.580728       -4007.398314     -3973.408011       58.518414  \n",
       "6        -3612.991916       -3716.329262     -3663.679387       63.813958  \n",
       "7        -3835.471624       -3831.895370     -3830.776961       25.434509  \n",
       "8        -3782.561085       -3871.936021     -3822.003737       53.756541  \n",
       "9        -3690.804405       -3782.921461     -3735.646204       88.343408  \n",
       "10       -3692.595831       -3816.184893     -3777.881747       86.757391  \n",
       "11       -4928.484502       -5045.496121     -4982.262332       48.250013  \n",
       "12       -4017.510703       -4019.648940     -4008.418771       76.748961  \n",
       "13       -4936.863420       -5142.071763     -4829.954914      206.910816  \n",
       "14       -9688.663497       -9770.503693     -9717.771930       48.299427  \n",
       "15       -3040.844582       -3062.027954     -3019.712083       33.513027  \n",
       "16       -3849.992309       -4035.164729     -3986.619404      127.926666  \n",
       "17       -4508.241726       -4438.991334     -4536.117984       66.113605  \n",
       "18       -3075.214638       -3128.698123     -3077.864424       39.020158  \n",
       "19       -3936.758375       -3986.582641     -3944.933245      107.513677  \n",
       "\n",
       "[20 rows x 88 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_mlp_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab0c5d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T15:23:31.343097Z",
     "start_time": "2025-12-07T15:23:31.173341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_train_MAE</th>\n",
       "      <td>8573.5668</td>\n",
       "      <td>1867.2584</td>\n",
       "      <td>1703.7763</td>\n",
       "      <td>1849.8854</td>\n",
       "      <td>2033.9338</td>\n",
       "      <td>1938.7958</td>\n",
       "      <td>4857.2161</td>\n",
       "      <td>1786.0109</td>\n",
       "      <td>2110.3977</td>\n",
       "      <td>2222.6412</td>\n",
       "      <td>...</td>\n",
       "      <td>1942.8541</td>\n",
       "      <td>2179.0444</td>\n",
       "      <td>7847.9487</td>\n",
       "      <td>1998.9471</td>\n",
       "      <td>2426.9009</td>\n",
       "      <td>1843.6284</td>\n",
       "      <td>1817.4186</td>\n",
       "      <td>1942.5476</td>\n",
       "      <td>2267.2738</td>\n",
       "      <td>2582.9861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAE</th>\n",
       "      <td>8582.8583</td>\n",
       "      <td>1880.7814</td>\n",
       "      <td>1730.6002</td>\n",
       "      <td>1866.9190</td>\n",
       "      <td>2054.4437</td>\n",
       "      <td>1944.4531</td>\n",
       "      <td>4879.1424</td>\n",
       "      <td>1806.1612</td>\n",
       "      <td>2126.5235</td>\n",
       "      <td>2240.6096</td>\n",
       "      <td>...</td>\n",
       "      <td>1952.7789</td>\n",
       "      <td>2191.6547</td>\n",
       "      <td>7823.7612</td>\n",
       "      <td>2018.5442</td>\n",
       "      <td>2439.5984</td>\n",
       "      <td>1854.1340</td>\n",
       "      <td>1832.3496</td>\n",
       "      <td>1952.0300</td>\n",
       "      <td>2281.6827</td>\n",
       "      <td>2592.9147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MAPE</th>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0.3381</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.1294</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1187</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.5582</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MAPE</th>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.1132</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1249</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>0.3396</td>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.1314</td>\n",
       "      <td>0.5553</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1138</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>0.1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_MedAE</th>\n",
       "      <td>7451.6377</td>\n",
       "      <td>1211.7983</td>\n",
       "      <td>1074.8931</td>\n",
       "      <td>1194.8168</td>\n",
       "      <td>1340.9759</td>\n",
       "      <td>1272.5525</td>\n",
       "      <td>3587.6200</td>\n",
       "      <td>1123.1760</td>\n",
       "      <td>1410.1433</td>\n",
       "      <td>1577.5639</td>\n",
       "      <td>...</td>\n",
       "      <td>1287.2240</td>\n",
       "      <td>1463.4586</td>\n",
       "      <td>6259.7930</td>\n",
       "      <td>1312.7797</td>\n",
       "      <td>1650.2162</td>\n",
       "      <td>1217.2612</td>\n",
       "      <td>1224.6145</td>\n",
       "      <td>1288.9807</td>\n",
       "      <td>1521.3138</td>\n",
       "      <td>1853.6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_MedAE</th>\n",
       "      <td>7462.4228</td>\n",
       "      <td>1210.5698</td>\n",
       "      <td>1084.9631</td>\n",
       "      <td>1199.8402</td>\n",
       "      <td>1353.0299</td>\n",
       "      <td>1277.3985</td>\n",
       "      <td>3594.2029</td>\n",
       "      <td>1137.1749</td>\n",
       "      <td>1417.2485</td>\n",
       "      <td>1579.9552</td>\n",
       "      <td>...</td>\n",
       "      <td>1294.2725</td>\n",
       "      <td>1466.5851</td>\n",
       "      <td>6215.4041</td>\n",
       "      <td>1326.0760</td>\n",
       "      <td>1657.9516</td>\n",
       "      <td>1221.5847</td>\n",
       "      <td>1228.0979</td>\n",
       "      <td>1291.7564</td>\n",
       "      <td>1533.3525</td>\n",
       "      <td>1850.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <td>11290.8565</td>\n",
       "      <td>3212.2763</td>\n",
       "      <td>3069.6271</td>\n",
       "      <td>3295.4380</td>\n",
       "      <td>3511.6391</td>\n",
       "      <td>3389.6338</td>\n",
       "      <td>7346.3079</td>\n",
       "      <td>3191.5867</td>\n",
       "      <td>3623.6747</td>\n",
       "      <td>3511.3041</td>\n",
       "      <td>...</td>\n",
       "      <td>3324.1276</td>\n",
       "      <td>3637.2132</td>\n",
       "      <td>10915.7451</td>\n",
       "      <td>3504.0384</td>\n",
       "      <td>3994.7135</td>\n",
       "      <td>3163.1008</td>\n",
       "      <td>3061.8432</td>\n",
       "      <td>3394.7750</td>\n",
       "      <td>3748.3830</td>\n",
       "      <td>4096.5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <td>11318.3623</td>\n",
       "      <td>3241.1425</td>\n",
       "      <td>3157.1591</td>\n",
       "      <td>3343.5355</td>\n",
       "      <td>3562.8610</td>\n",
       "      <td>3403.8915</td>\n",
       "      <td>7328.3117</td>\n",
       "      <td>3245.0632</td>\n",
       "      <td>3674.2785</td>\n",
       "      <td>3521.2131</td>\n",
       "      <td>...</td>\n",
       "      <td>3345.4689</td>\n",
       "      <td>3708.1282</td>\n",
       "      <td>10889.5912</td>\n",
       "      <td>3541.2970</td>\n",
       "      <td>4042.6891</td>\n",
       "      <td>3197.2080</td>\n",
       "      <td>3113.7006</td>\n",
       "      <td>3420.5129</td>\n",
       "      <td>3784.8278</td>\n",
       "      <td>4146.2132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0          1          2          3          4   \\\n",
       "mean_train_MAE     8573.5668  1867.2584  1703.7763  1849.8854  2033.9338   \n",
       "mean_test_MAE      8582.8583  1880.7814  1730.6002  1866.9190  2054.4437   \n",
       "mean_train_MAPE       0.6825     0.1123     0.1026     0.1114     0.1237   \n",
       "mean_test_MAPE        0.6830     0.1132     0.1046     0.1126     0.1249   \n",
       "mean_train_MedAE   7451.6377  1211.7983  1074.8931  1194.8168  1340.9759   \n",
       "mean_test_MedAE    7462.4228  1210.5698  1084.9631  1199.8402  1353.0299   \n",
       "mean_train_RMSE   11290.8565  3212.2763  3069.6271  3295.4380  3511.6391   \n",
       "mean_test_RMSE    11318.3623  3241.1425  3157.1591  3343.5355  3562.8610   \n",
       "\n",
       "                         5          6          7          8          9   ...  \\\n",
       "mean_train_MAE    1938.7958  4857.2161  1786.0109  2110.3977  2222.6412  ...   \n",
       "mean_test_MAE     1944.4531  4879.1424  1806.1612  2126.5235  2240.6096  ...   \n",
       "mean_train_MAPE      0.1175     0.3381     0.1061     0.1294     0.1385  ...   \n",
       "mean_test_MAPE       0.1181     0.3396     0.1078     0.1307     0.1398  ...   \n",
       "mean_train_MedAE  1272.5525  3587.6200  1123.1760  1410.1433  1577.5639  ...   \n",
       "mean_test_MedAE   1277.3985  3594.2029  1137.1749  1417.2485  1579.9552  ...   \n",
       "mean_train_RMSE   3389.6338  7346.3079  3191.5867  3623.6747  3511.3041  ...   \n",
       "mean_test_RMSE    3403.8915  7328.3117  3245.0632  3674.2785  3521.2131  ...   \n",
       "\n",
       "                         20         21          22         23         24  \\\n",
       "mean_train_MAE    1942.8541  2179.0444   7847.9487  1998.9471  2426.9009   \n",
       "mean_test_MAE     1952.7789  2191.6547   7823.7612  2018.5442  2439.5984   \n",
       "mean_train_MAPE      0.1187     0.1307      0.5582     0.1237     0.1498   \n",
       "mean_test_MAPE       0.1195     0.1314      0.5553     0.1250     0.1503   \n",
       "mean_train_MedAE  1287.2240  1463.4586   6259.7930  1312.7797  1650.2162   \n",
       "mean_test_MedAE   1294.2725  1466.5851   6215.4041  1326.0760  1657.9516   \n",
       "mean_train_RMSE   3324.1276  3637.2132  10915.7451  3504.0384  3994.7135   \n",
       "mean_test_RMSE    3345.4689  3708.1282  10889.5912  3541.2970  4042.6891   \n",
       "\n",
       "                         25         26         27         28         29  \n",
       "mean_train_MAE    1843.6284  1817.4186  1942.5476  2267.2738  2582.9861  \n",
       "mean_test_MAE     1854.1340  1832.3496  1952.0300  2281.6827  2592.9147  \n",
       "mean_train_MAPE      0.1128     0.1130     0.1183     0.1366     0.1632  \n",
       "mean_test_MAPE       0.1138     0.1142     0.1189     0.1377     0.1636  \n",
       "mean_train_MedAE  1217.2612  1224.6145  1288.9807  1521.3138  1853.6557  \n",
       "mean_test_MedAE   1221.5847  1228.0979  1291.7564  1533.3525  1850.2157  \n",
       "mean_train_RMSE   3163.1008  3061.8432  3394.7750  3748.3830  4096.5398  \n",
       "mean_test_RMSE    3197.2080  3113.7006  3420.5129  3784.8278  4146.2132  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results_df_mlp_adam_logprice = pd.DataFrame(rs_mlp_adam_logprice.cv_results_)\n",
    "\n",
    "\n",
    "metric_cols_train_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_R2\")]\n",
    "metric_cols_test_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_R2\")]\n",
    "metric_cols_train_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_MAE\")]\n",
    "metric_cols_test_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_MAE\")]\n",
    "metric_cols_train_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_MAPE\")]\n",
    "metric_cols_test_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_MAPE\")]\n",
    "metric_cols_train_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_MedAE\")]\n",
    "metric_cols_test_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_MedAE\")]\n",
    "metric_cols_train_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_train_RMSE\")]\n",
    "metric_cols_test_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"mean_test_RMSE\")]\n",
    "\n",
    "std_cols_train_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_R2\")]\n",
    "std_cols_test_R2 = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_R2\")]\n",
    "\n",
    "std_cols_train_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_MAE\")]\n",
    "std_cols_test_MAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_MAE\")]\n",
    "std_cols_train_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_MAPE\")]\n",
    "std_cols_test_MAPE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_MAPE\")]\n",
    "std_cols_train_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_MedAE\")]\n",
    "std_cols_test_MedAE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_MedAE\")]\n",
    "std_cols_train_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_train_RMSE\")]\n",
    "std_cols_test_RMSE = [c for c in results_df_mlp_adam_logprice.columns if c.startswith(\"std_test_RMSE\")]\n",
    "\n",
    "df_metrics_mlp_adam_logprice = results_df_mlp_adam_logprice[[\"params\"]+ metric_cols_train_MAE + metric_cols_test_MAE +\n",
    "                    metric_cols_train_R2 + metric_cols_test_R2 + \n",
    "                    metric_cols_train_MAPE + metric_cols_test_MAPE + \n",
    "                    metric_cols_train_MedAE + metric_cols_test_MedAE + \n",
    "                    metric_cols_train_RMSE + metric_cols_test_RMSE ]\n",
    "df_metrics_mlp_adam_logprice = df_metrics_mlp_adam_logprice.loc[:,['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] = df_metrics_mlp_adam_logprice.loc[:,\n",
    "                                                                                        ['mean_train_MAE', 'mean_test_MAE',\n",
    "                    'mean_train_MAPE', 'mean_test_MAPE', \n",
    "                    'mean_train_MedAE', 'mean_test_MedAE', \n",
    "                    'mean_train_RMSE', 'mean_test_RMSE' ]] .round(4) * -1\n",
    "                    \n",
    "df_metrics_mlp_adam_logprice.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef90043a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T15:23:47.510893Z",
     "start_time": "2025-12-07T15:23:47.475719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_scaler__scaler</th>\n",
       "      <th>param_regressor__regressor__solver</th>\n",
       "      <th>param_regressor__regressor__max_iter</th>\n",
       "      <th>param_regressor__regressor__learning_rate_init</th>\n",
       "      <th>param_regressor__regressor__hidden_layer_sizes</th>\n",
       "      <th>param_regressor__regressor__activation</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_RMSE</th>\n",
       "      <th>std_test_RMSE</th>\n",
       "      <th>rank_test_RMSE</th>\n",
       "      <th>split0_train_RMSE</th>\n",
       "      <th>split1_train_RMSE</th>\n",
       "      <th>split2_train_RMSE</th>\n",
       "      <th>split3_train_RMSE</th>\n",
       "      <th>split4_train_RMSE</th>\n",
       "      <th>mean_train_RMSE</th>\n",
       "      <th>std_train_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>434.694617</td>\n",
       "      <td>7.121700</td>\n",
       "      <td>19.343342</td>\n",
       "      <td>4.576002</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(600, 300, 150)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-11318.362257</td>\n",
       "      <td>1143.837331</td>\n",
       "      <td>30</td>\n",
       "      <td>-10055.103841</td>\n",
       "      <td>-12330.886963</td>\n",
       "      <td>-9684.060297</td>\n",
       "      <td>-11780.511890</td>\n",
       "      <td>-12603.719672</td>\n",
       "      <td>-11290.856533</td>\n",
       "      <td>1196.153347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.257864</td>\n",
       "      <td>11.737984</td>\n",
       "      <td>11.889528</td>\n",
       "      <td>0.964390</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3241.142535</td>\n",
       "      <td>214.283346</td>\n",
       "      <td>6</td>\n",
       "      <td>-3292.523233</td>\n",
       "      <td>-3188.549050</td>\n",
       "      <td>-3093.582715</td>\n",
       "      <td>-3249.319974</td>\n",
       "      <td>-3237.406730</td>\n",
       "      <td>-3212.276341</td>\n",
       "      <td>67.962478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1282.908450</td>\n",
       "      <td>208.451343</td>\n",
       "      <td>18.675480</td>\n",
       "      <td>6.584770</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-3157.159089</td>\n",
       "      <td>317.796155</td>\n",
       "      <td>3</td>\n",
       "      <td>-2944.645230</td>\n",
       "      <td>-3176.838894</td>\n",
       "      <td>-3005.813822</td>\n",
       "      <td>-3179.785187</td>\n",
       "      <td>-3041.052616</td>\n",
       "      <td>-3069.627150</td>\n",
       "      <td>93.955603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.071285</td>\n",
       "      <td>7.141914</td>\n",
       "      <td>19.582847</td>\n",
       "      <td>3.020914</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-3343.535522</td>\n",
       "      <td>316.509341</td>\n",
       "      <td>9</td>\n",
       "      <td>-3390.087314</td>\n",
       "      <td>-3434.904652</td>\n",
       "      <td>-3285.445435</td>\n",
       "      <td>-3223.964929</td>\n",
       "      <td>-3142.787485</td>\n",
       "      <td>-3295.437963</td>\n",
       "      <td>106.695708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400.769610</td>\n",
       "      <td>101.299953</td>\n",
       "      <td>15.667760</td>\n",
       "      <td>5.098244</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-3562.861048</td>\n",
       "      <td>402.799285</td>\n",
       "      <td>19</td>\n",
       "      <td>-3842.714943</td>\n",
       "      <td>-3492.428679</td>\n",
       "      <td>-3715.048430</td>\n",
       "      <td>-3344.121019</td>\n",
       "      <td>-3163.882302</td>\n",
       "      <td>-3511.639075</td>\n",
       "      <td>245.092924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>257.433797</td>\n",
       "      <td>74.590608</td>\n",
       "      <td>15.762641</td>\n",
       "      <td>3.897662</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-3403.891544</td>\n",
       "      <td>322.071185</td>\n",
       "      <td>12</td>\n",
       "      <td>-3511.329079</td>\n",
       "      <td>-3449.995122</td>\n",
       "      <td>-3326.427436</td>\n",
       "      <td>-3438.874752</td>\n",
       "      <td>-3221.542503</td>\n",
       "      <td>-3389.633778</td>\n",
       "      <td>103.080192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>446.587622</td>\n",
       "      <td>232.110540</td>\n",
       "      <td>16.161217</td>\n",
       "      <td>5.261830</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-7328.311654</td>\n",
       "      <td>3212.029467</td>\n",
       "      <td>27</td>\n",
       "      <td>-3519.329448</td>\n",
       "      <td>-9817.107121</td>\n",
       "      <td>-9956.440310</td>\n",
       "      <td>-3510.374388</td>\n",
       "      <td>-9928.288380</td>\n",
       "      <td>-7346.307929</td>\n",
       "      <td>3128.719008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>165.672887</td>\n",
       "      <td>21.660510</td>\n",
       "      <td>13.443306</td>\n",
       "      <td>5.009503</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-3245.063198</td>\n",
       "      <td>410.659949</td>\n",
       "      <td>7</td>\n",
       "      <td>-3259.877138</td>\n",
       "      <td>-3289.468072</td>\n",
       "      <td>-3275.479371</td>\n",
       "      <td>-3250.492418</td>\n",
       "      <td>-2882.616338</td>\n",
       "      <td>-3191.586668</td>\n",
       "      <td>155.057986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87.254988</td>\n",
       "      <td>6.430920</td>\n",
       "      <td>13.418715</td>\n",
       "      <td>5.419186</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3674.278535</td>\n",
       "      <td>251.193078</td>\n",
       "      <td>20</td>\n",
       "      <td>-3671.117199</td>\n",
       "      <td>-3655.210679</td>\n",
       "      <td>-3571.421774</td>\n",
       "      <td>-3598.275438</td>\n",
       "      <td>-3622.348589</td>\n",
       "      <td>-3623.674736</td>\n",
       "      <td>36.393840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>278.254169</td>\n",
       "      <td>19.506122</td>\n",
       "      <td>13.528002</td>\n",
       "      <td>4.960626</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3521.213088</td>\n",
       "      <td>456.763346</td>\n",
       "      <td>16</td>\n",
       "      <td>-3159.903043</td>\n",
       "      <td>-3323.633185</td>\n",
       "      <td>-3903.753083</td>\n",
       "      <td>-3157.431655</td>\n",
       "      <td>-4011.799346</td>\n",
       "      <td>-3511.304063</td>\n",
       "      <td>371.063466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>157.599323</td>\n",
       "      <td>19.842822</td>\n",
       "      <td>12.275299</td>\n",
       "      <td>2.151750</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3221.234599</td>\n",
       "      <td>387.122561</td>\n",
       "      <td>5</td>\n",
       "      <td>-3022.548098</td>\n",
       "      <td>-3216.208001</td>\n",
       "      <td>-3163.371122</td>\n",
       "      <td>-3309.169862</td>\n",
       "      <td>-2804.918054</td>\n",
       "      <td>-3103.243027</td>\n",
       "      <td>175.665475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>289.370951</td>\n",
       "      <td>29.472246</td>\n",
       "      <td>13.579268</td>\n",
       "      <td>4.176719</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-3539.554403</td>\n",
       "      <td>356.873295</td>\n",
       "      <td>17</td>\n",
       "      <td>-3538.875771</td>\n",
       "      <td>-3468.129365</td>\n",
       "      <td>-3611.172980</td>\n",
       "      <td>-3548.417604</td>\n",
       "      <td>-3457.738205</td>\n",
       "      <td>-3524.866785</td>\n",
       "      <td>56.435863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>105.912334</td>\n",
       "      <td>13.866038</td>\n",
       "      <td>12.372564</td>\n",
       "      <td>3.039127</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-10053.479395</td>\n",
       "      <td>419.406558</td>\n",
       "      <td>28</td>\n",
       "      <td>-9784.738914</td>\n",
       "      <td>-9971.712644</td>\n",
       "      <td>-9844.024479</td>\n",
       "      <td>-10615.899145</td>\n",
       "      <td>-10054.130579</td>\n",
       "      <td>-10054.101152</td>\n",
       "      <td>296.342058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2805.699045</td>\n",
       "      <td>311.672281</td>\n",
       "      <td>20.231675</td>\n",
       "      <td>4.607414</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(600, 300, 150)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-2920.155364</td>\n",
       "      <td>359.824842</td>\n",
       "      <td>1</td>\n",
       "      <td>-2984.965573</td>\n",
       "      <td>-2853.675284</td>\n",
       "      <td>-2757.323342</td>\n",
       "      <td>-2743.641645</td>\n",
       "      <td>-2337.021356</td>\n",
       "      <td>-2735.325440</td>\n",
       "      <td>217.037366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>122.352709</td>\n",
       "      <td>15.998584</td>\n",
       "      <td>10.007660</td>\n",
       "      <td>0.785963</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3949.316474</td>\n",
       "      <td>470.375976</td>\n",
       "      <td>23</td>\n",
       "      <td>-4651.914622</td>\n",
       "      <td>-3501.112240</td>\n",
       "      <td>-3327.719225</td>\n",
       "      <td>-3952.725468</td>\n",
       "      <td>-3839.144473</td>\n",
       "      <td>-3854.523206</td>\n",
       "      <td>457.850414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>163.660203</td>\n",
       "      <td>18.080192</td>\n",
       "      <td>15.800288</td>\n",
       "      <td>4.597713</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-3415.344138</td>\n",
       "      <td>322.843816</td>\n",
       "      <td>13</td>\n",
       "      <td>-3309.615069</td>\n",
       "      <td>-3453.944629</td>\n",
       "      <td>-3307.729306</td>\n",
       "      <td>-3426.388013</td>\n",
       "      <td>-3480.670703</td>\n",
       "      <td>-3395.669544</td>\n",
       "      <td>73.080319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>975.862060</td>\n",
       "      <td>454.602108</td>\n",
       "      <td>17.127209</td>\n",
       "      <td>5.827359</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-4898.522817</td>\n",
       "      <td>2601.273202</td>\n",
       "      <td>26</td>\n",
       "      <td>-4507.753810</td>\n",
       "      <td>-9997.580437</td>\n",
       "      <td>-3229.898817</td>\n",
       "      <td>-3178.113232</td>\n",
       "      <td>-3137.976506</td>\n",
       "      <td>-4810.264560</td>\n",
       "      <td>2644.155061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>206.396605</td>\n",
       "      <td>47.495993</td>\n",
       "      <td>13.544542</td>\n",
       "      <td>4.377057</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3285.548847</td>\n",
       "      <td>407.939975</td>\n",
       "      <td>8</td>\n",
       "      <td>-2867.379849</td>\n",
       "      <td>-3319.380099</td>\n",
       "      <td>-3116.416688</td>\n",
       "      <td>-3436.552764</td>\n",
       "      <td>-3277.343522</td>\n",
       "      <td>-3203.414584</td>\n",
       "      <td>196.852674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>359.046680</td>\n",
       "      <td>91.939077</td>\n",
       "      <td>17.175161</td>\n",
       "      <td>5.427960</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-3452.398554</td>\n",
       "      <td>442.618343</td>\n",
       "      <td>15</td>\n",
       "      <td>-3323.353677</td>\n",
       "      <td>-3343.015373</td>\n",
       "      <td>-3376.282595</td>\n",
       "      <td>-3974.812054</td>\n",
       "      <td>-3055.373635</td>\n",
       "      <td>-3414.567467</td>\n",
       "      <td>302.588696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>338.903240</td>\n",
       "      <td>51.815047</td>\n",
       "      <td>15.789478</td>\n",
       "      <td>3.978469</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3385.301014</td>\n",
       "      <td>262.547925</td>\n",
       "      <td>11</td>\n",
       "      <td>-3309.453950</td>\n",
       "      <td>-3420.467207</td>\n",
       "      <td>-3226.811831</td>\n",
       "      <td>-3254.389481</td>\n",
       "      <td>-3269.738740</td>\n",
       "      <td>-3296.172242</td>\n",
       "      <td>67.647673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>111.835452</td>\n",
       "      <td>9.122546</td>\n",
       "      <td>11.877839</td>\n",
       "      <td>3.948862</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-3345.468883</td>\n",
       "      <td>281.558061</td>\n",
       "      <td>10</td>\n",
       "      <td>-3424.284620</td>\n",
       "      <td>-3297.987198</td>\n",
       "      <td>-3312.364955</td>\n",
       "      <td>-3323.899092</td>\n",
       "      <td>-3262.101938</td>\n",
       "      <td>-3324.127561</td>\n",
       "      <td>54.222401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>91.283952</td>\n",
       "      <td>6.459945</td>\n",
       "      <td>14.283394</td>\n",
       "      <td>3.842987</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3708.128226</td>\n",
       "      <td>423.651216</td>\n",
       "      <td>21</td>\n",
       "      <td>-4026.254985</td>\n",
       "      <td>-3704.765004</td>\n",
       "      <td>-3688.334593</td>\n",
       "      <td>-3583.367262</td>\n",
       "      <td>-3183.343937</td>\n",
       "      <td>-3637.213156</td>\n",
       "      <td>271.081850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>291.341999</td>\n",
       "      <td>17.615092</td>\n",
       "      <td>14.232816</td>\n",
       "      <td>5.013046</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-10889.591215</td>\n",
       "      <td>1082.834230</td>\n",
       "      <td>29</td>\n",
       "      <td>-9789.772647</td>\n",
       "      <td>-10996.978663</td>\n",
       "      <td>-10998.219809</td>\n",
       "      <td>-9842.842072</td>\n",
       "      <td>-12950.912498</td>\n",
       "      <td>-10915.745138</td>\n",
       "      <td>1146.668407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>325.526556</td>\n",
       "      <td>51.600073</td>\n",
       "      <td>12.982654</td>\n",
       "      <td>3.134963</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(300, 200, 100)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-3541.297033</td>\n",
       "      <td>452.074040</td>\n",
       "      <td>18</td>\n",
       "      <td>-3515.254175</td>\n",
       "      <td>-3461.126574</td>\n",
       "      <td>-3861.453667</td>\n",
       "      <td>-3379.305378</td>\n",
       "      <td>-3303.051969</td>\n",
       "      <td>-3504.038353</td>\n",
       "      <td>192.700163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>680.552624</td>\n",
       "      <td>77.052690</td>\n",
       "      <td>14.764534</td>\n",
       "      <td>2.895003</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(600, 200)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-4042.689083</td>\n",
       "      <td>361.266374</td>\n",
       "      <td>24</td>\n",
       "      <td>-3898.521775</td>\n",
       "      <td>-4136.217645</td>\n",
       "      <td>-3774.664736</td>\n",
       "      <td>-4494.850431</td>\n",
       "      <td>-3669.313085</td>\n",
       "      <td>-3994.713535</td>\n",
       "      <td>294.523381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3130.935869</td>\n",
       "      <td>436.097796</td>\n",
       "      <td>14.849011</td>\n",
       "      <td>4.970852</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(600, 300, 150)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-3197.208020</td>\n",
       "      <td>248.575787</td>\n",
       "      <td>4</td>\n",
       "      <td>-3219.445737</td>\n",
       "      <td>-3168.538097</td>\n",
       "      <td>-3045.782086</td>\n",
       "      <td>-3190.368330</td>\n",
       "      <td>-3191.369897</td>\n",
       "      <td>-3163.100829</td>\n",
       "      <td>60.845143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>382.478663</td>\n",
       "      <td>45.494693</td>\n",
       "      <td>16.437148</td>\n",
       "      <td>4.491974</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(200, 100)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-3113.700588</td>\n",
       "      <td>252.766262</td>\n",
       "      <td>2</td>\n",
       "      <td>-3179.892050</td>\n",
       "      <td>-3057.248958</td>\n",
       "      <td>-3038.179800</td>\n",
       "      <td>-3013.425760</td>\n",
       "      <td>-3020.469623</td>\n",
       "      <td>-3061.843238</td>\n",
       "      <td>60.946888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>284.551120</td>\n",
       "      <td>30.969272</td>\n",
       "      <td>14.130046</td>\n",
       "      <td>4.726945</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(200, 100, 50)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>...</td>\n",
       "      <td>-3420.512913</td>\n",
       "      <td>348.187516</td>\n",
       "      <td>14</td>\n",
       "      <td>-3361.718500</td>\n",
       "      <td>-3456.410997</td>\n",
       "      <td>-3355.462645</td>\n",
       "      <td>-3475.300469</td>\n",
       "      <td>-3324.982331</td>\n",
       "      <td>-3394.774988</td>\n",
       "      <td>59.653260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>91.289858</td>\n",
       "      <td>10.180296</td>\n",
       "      <td>18.866226</td>\n",
       "      <td>2.796089</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.100</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>-3784.827768</td>\n",
       "      <td>654.468729</td>\n",
       "      <td>22</td>\n",
       "      <td>-3514.105027</td>\n",
       "      <td>-3341.251456</td>\n",
       "      <td>-3212.133420</td>\n",
       "      <td>-4876.676804</td>\n",
       "      <td>-3797.748389</td>\n",
       "      <td>-3748.383019</td>\n",
       "      <td>597.275435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>907.996204</td>\n",
       "      <td>202.069811</td>\n",
       "      <td>16.147163</td>\n",
       "      <td>6.820276</td>\n",
       "      <td>MinMaxScaler(feature_range=(-1, 1))</td>\n",
       "      <td>adam</td>\n",
       "      <td>700</td>\n",
       "      <td>0.010</td>\n",
       "      <td>(600, 300, 150)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>-4146.213160</td>\n",
       "      <td>410.917096</td>\n",
       "      <td>25</td>\n",
       "      <td>-4382.550568</td>\n",
       "      <td>-3755.779241</td>\n",
       "      <td>-4265.112567</td>\n",
       "      <td>-4270.418417</td>\n",
       "      <td>-3808.838187</td>\n",
       "      <td>-4096.539796</td>\n",
       "      <td>260.515906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      434.694617      7.121700        19.343342        4.576002   \n",
       "1       84.257864     11.737984        11.889528        0.964390   \n",
       "2     1282.908450    208.451343        18.675480        6.584770   \n",
       "3      103.071285      7.141914        19.582847        3.020914   \n",
       "4      400.769610    101.299953        15.667760        5.098244   \n",
       "5      257.433797     74.590608        15.762641        3.897662   \n",
       "6      446.587622    232.110540        16.161217        5.261830   \n",
       "7      165.672887     21.660510        13.443306        5.009503   \n",
       "8       87.254988      6.430920        13.418715        5.419186   \n",
       "9      278.254169     19.506122        13.528002        4.960626   \n",
       "10     157.599323     19.842822        12.275299        2.151750   \n",
       "11     289.370951     29.472246        13.579268        4.176719   \n",
       "12     105.912334     13.866038        12.372564        3.039127   \n",
       "13    2805.699045    311.672281        20.231675        4.607414   \n",
       "14     122.352709     15.998584        10.007660        0.785963   \n",
       "15     163.660203     18.080192        15.800288        4.597713   \n",
       "16     975.862060    454.602108        17.127209        5.827359   \n",
       "17     206.396605     47.495993        13.544542        4.377057   \n",
       "18     359.046680     91.939077        17.175161        5.427960   \n",
       "19     338.903240     51.815047        15.789478        3.978469   \n",
       "20     111.835452      9.122546        11.877839        3.948862   \n",
       "21      91.283952      6.459945        14.283394        3.842987   \n",
       "22     291.341999     17.615092        14.232816        5.013046   \n",
       "23     325.526556     51.600073        12.982654        3.134963   \n",
       "24     680.552624     77.052690        14.764534        2.895003   \n",
       "25    3130.935869    436.097796        14.849011        4.970852   \n",
       "26     382.478663     45.494693        16.437148        4.491974   \n",
       "27     284.551120     30.969272        14.130046        4.726945   \n",
       "28      91.289858     10.180296        18.866226        2.796089   \n",
       "29     907.996204    202.069811        16.147163        6.820276   \n",
       "\n",
       "                   param_scaler__scaler param_regressor__regressor__solver  \\\n",
       "0                        MinMaxScaler()                               adam   \n",
       "1   MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "2                        RobustScaler()                               adam   \n",
       "3                      StandardScaler()                               adam   \n",
       "4                        RobustScaler()                               adam   \n",
       "5                        RobustScaler()                               adam   \n",
       "6                        RobustScaler()                               adam   \n",
       "7                        RobustScaler()                               adam   \n",
       "8                        MinMaxScaler()                               adam   \n",
       "9   MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "10                     StandardScaler()                               adam   \n",
       "11  MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "12  MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "13                       RobustScaler()                               adam   \n",
       "14                     StandardScaler()                               adam   \n",
       "15                     StandardScaler()                               adam   \n",
       "16                     StandardScaler()                               adam   \n",
       "17                       RobustScaler()                               adam   \n",
       "18                       MinMaxScaler()                               adam   \n",
       "19                     StandardScaler()                               adam   \n",
       "20  MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "21                       RobustScaler()                               adam   \n",
       "22                       MinMaxScaler()                               adam   \n",
       "23                     StandardScaler()                               adam   \n",
       "24                       MinMaxScaler()                               adam   \n",
       "25  MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "26  MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "27                       RobustScaler()                               adam   \n",
       "28  MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "29  MinMaxScaler(feature_range=(-1, 1))                               adam   \n",
       "\n",
       "    param_regressor__regressor__max_iter  \\\n",
       "0                                    700   \n",
       "1                                    700   \n",
       "2                                    700   \n",
       "3                                    700   \n",
       "4                                    700   \n",
       "5                                    700   \n",
       "6                                    700   \n",
       "7                                    700   \n",
       "8                                    700   \n",
       "9                                    700   \n",
       "10                                   700   \n",
       "11                                   700   \n",
       "12                                   700   \n",
       "13                                   700   \n",
       "14                                   700   \n",
       "15                                   700   \n",
       "16                                   700   \n",
       "17                                   700   \n",
       "18                                   700   \n",
       "19                                   700   \n",
       "20                                   700   \n",
       "21                                   700   \n",
       "22                                   700   \n",
       "23                                   700   \n",
       "24                                   700   \n",
       "25                                   700   \n",
       "26                                   700   \n",
       "27                                   700   \n",
       "28                                   700   \n",
       "29                                   700   \n",
       "\n",
       "    param_regressor__regressor__learning_rate_init  \\\n",
       "0                                            0.100   \n",
       "1                                            0.010   \n",
       "2                                            0.001   \n",
       "3                                            0.010   \n",
       "4                                            0.010   \n",
       "5                                            0.001   \n",
       "6                                            0.010   \n",
       "7                                            0.010   \n",
       "8                                            0.100   \n",
       "9                                            0.010   \n",
       "10                                           0.001   \n",
       "11                                           0.001   \n",
       "12                                           0.100   \n",
       "13                                           0.001   \n",
       "14                                           0.100   \n",
       "15                                           0.010   \n",
       "16                                           0.010   \n",
       "17                                           0.001   \n",
       "18                                           0.010   \n",
       "19                                           0.001   \n",
       "20                                           0.001   \n",
       "21                                           0.010   \n",
       "22                                           0.100   \n",
       "23                                           0.010   \n",
       "24                                           0.100   \n",
       "25                                           0.001   \n",
       "26                                           0.010   \n",
       "27                                           0.001   \n",
       "28                                           0.100   \n",
       "29                                           0.010   \n",
       "\n",
       "   param_regressor__regressor__hidden_layer_sizes  \\\n",
       "0                                 (600, 300, 150)   \n",
       "1                                        (32, 16)   \n",
       "2                                      (600, 200)   \n",
       "3                                        (32, 16)   \n",
       "4                                      (600, 200)   \n",
       "5                                      (200, 100)   \n",
       "6                                      (600, 200)   \n",
       "7                                   (100, 50, 25)   \n",
       "8                                        (32, 16)   \n",
       "9                                      (600, 200)   \n",
       "10                                  (100, 50, 25)   \n",
       "11                                 (200, 100, 50)   \n",
       "12                                 (200, 100, 50)   \n",
       "13                                (600, 300, 150)   \n",
       "14                                  (100, 50, 25)   \n",
       "15                                  (100, 50, 25)   \n",
       "16                                     (600, 200)   \n",
       "17                                     (200, 100)   \n",
       "18                                     (200, 100)   \n",
       "19                                     (600, 200)   \n",
       "20                                       (32, 16)   \n",
       "21                                     (200, 100)   \n",
       "22                                     (600, 200)   \n",
       "23                                (300, 200, 100)   \n",
       "24                                     (600, 200)   \n",
       "25                                (600, 300, 150)   \n",
       "26                                     (200, 100)   \n",
       "27                                 (200, 100, 50)   \n",
       "28                                       (32, 16)   \n",
       "29                                (600, 300, 150)   \n",
       "\n",
       "   param_regressor__regressor__activation  ... mean_test_RMSE  std_test_RMSE  \\\n",
       "0                                    tanh  ...  -11318.362257    1143.837331   \n",
       "1                                    relu  ...   -3241.142535     214.283346   \n",
       "2                                    tanh  ...   -3157.159089     317.796155   \n",
       "3                                logistic  ...   -3343.535522     316.509341   \n",
       "4                                    tanh  ...   -3562.861048     402.799285   \n",
       "5                                logistic  ...   -3403.891544     322.071185   \n",
       "6                                logistic  ...   -7328.311654    3212.029467   \n",
       "7                                logistic  ...   -3245.063198     410.659949   \n",
       "8                                    relu  ...   -3674.278535     251.193078   \n",
       "9                                    relu  ...   -3521.213088     456.763346   \n",
       "10                                   relu  ...   -3221.234599     387.122561   \n",
       "11                               logistic  ...   -3539.554403     356.873295   \n",
       "12                               logistic  ...  -10053.479395     419.406558   \n",
       "13                                   tanh  ...   -2920.155364     359.824842   \n",
       "14                                   relu  ...   -3949.316474     470.375976   \n",
       "15                                   tanh  ...   -3415.344138     322.843816   \n",
       "16                               logistic  ...   -4898.522817    2601.273202   \n",
       "17                                   relu  ...   -3285.548847     407.939975   \n",
       "18                                   tanh  ...   -3452.398554     442.618343   \n",
       "19                                   relu  ...   -3385.301014     262.547925   \n",
       "20                                   tanh  ...   -3345.468883     281.558061   \n",
       "21                                   relu  ...   -3708.128226     423.651216   \n",
       "22                                   tanh  ...  -10889.591215    1082.834230   \n",
       "23                                   tanh  ...   -3541.297033     452.074040   \n",
       "24                                   relu  ...   -4042.689083     361.266374   \n",
       "25                               logistic  ...   -3197.208020     248.575787   \n",
       "26                               logistic  ...   -3113.700588     252.766262   \n",
       "27                               logistic  ...   -3420.512913     348.187516   \n",
       "28                                   relu  ...   -3784.827768     654.468729   \n",
       "29                                   tanh  ...   -4146.213160     410.917096   \n",
       "\n",
       "    rank_test_RMSE  split0_train_RMSE  split1_train_RMSE  split2_train_RMSE  \\\n",
       "0               30      -10055.103841      -12330.886963       -9684.060297   \n",
       "1                6       -3292.523233       -3188.549050       -3093.582715   \n",
       "2                3       -2944.645230       -3176.838894       -3005.813822   \n",
       "3                9       -3390.087314       -3434.904652       -3285.445435   \n",
       "4               19       -3842.714943       -3492.428679       -3715.048430   \n",
       "5               12       -3511.329079       -3449.995122       -3326.427436   \n",
       "6               27       -3519.329448       -9817.107121       -9956.440310   \n",
       "7                7       -3259.877138       -3289.468072       -3275.479371   \n",
       "8               20       -3671.117199       -3655.210679       -3571.421774   \n",
       "9               16       -3159.903043       -3323.633185       -3903.753083   \n",
       "10               5       -3022.548098       -3216.208001       -3163.371122   \n",
       "11              17       -3538.875771       -3468.129365       -3611.172980   \n",
       "12              28       -9784.738914       -9971.712644       -9844.024479   \n",
       "13               1       -2984.965573       -2853.675284       -2757.323342   \n",
       "14              23       -4651.914622       -3501.112240       -3327.719225   \n",
       "15              13       -3309.615069       -3453.944629       -3307.729306   \n",
       "16              26       -4507.753810       -9997.580437       -3229.898817   \n",
       "17               8       -2867.379849       -3319.380099       -3116.416688   \n",
       "18              15       -3323.353677       -3343.015373       -3376.282595   \n",
       "19              11       -3309.453950       -3420.467207       -3226.811831   \n",
       "20              10       -3424.284620       -3297.987198       -3312.364955   \n",
       "21              21       -4026.254985       -3704.765004       -3688.334593   \n",
       "22              29       -9789.772647      -10996.978663      -10998.219809   \n",
       "23              18       -3515.254175       -3461.126574       -3861.453667   \n",
       "24              24       -3898.521775       -4136.217645       -3774.664736   \n",
       "25               4       -3219.445737       -3168.538097       -3045.782086   \n",
       "26               2       -3179.892050       -3057.248958       -3038.179800   \n",
       "27              14       -3361.718500       -3456.410997       -3355.462645   \n",
       "28              22       -3514.105027       -3341.251456       -3212.133420   \n",
       "29              25       -4382.550568       -3755.779241       -4265.112567   \n",
       "\n",
       "    split3_train_RMSE  split4_train_RMSE  mean_train_RMSE  std_train_RMSE  \n",
       "0       -11780.511890      -12603.719672    -11290.856533     1196.153347  \n",
       "1        -3249.319974       -3237.406730     -3212.276341       67.962478  \n",
       "2        -3179.785187       -3041.052616     -3069.627150       93.955603  \n",
       "3        -3223.964929       -3142.787485     -3295.437963      106.695708  \n",
       "4        -3344.121019       -3163.882302     -3511.639075      245.092924  \n",
       "5        -3438.874752       -3221.542503     -3389.633778      103.080192  \n",
       "6        -3510.374388       -9928.288380     -7346.307929     3128.719008  \n",
       "7        -3250.492418       -2882.616338     -3191.586668      155.057986  \n",
       "8        -3598.275438       -3622.348589     -3623.674736       36.393840  \n",
       "9        -3157.431655       -4011.799346     -3511.304063      371.063466  \n",
       "10       -3309.169862       -2804.918054     -3103.243027      175.665475  \n",
       "11       -3548.417604       -3457.738205     -3524.866785       56.435863  \n",
       "12      -10615.899145      -10054.130579    -10054.101152      296.342058  \n",
       "13       -2743.641645       -2337.021356     -2735.325440      217.037366  \n",
       "14       -3952.725468       -3839.144473     -3854.523206      457.850414  \n",
       "15       -3426.388013       -3480.670703     -3395.669544       73.080319  \n",
       "16       -3178.113232       -3137.976506     -4810.264560     2644.155061  \n",
       "17       -3436.552764       -3277.343522     -3203.414584      196.852674  \n",
       "18       -3974.812054       -3055.373635     -3414.567467      302.588696  \n",
       "19       -3254.389481       -3269.738740     -3296.172242       67.647673  \n",
       "20       -3323.899092       -3262.101938     -3324.127561       54.222401  \n",
       "21       -3583.367262       -3183.343937     -3637.213156      271.081850  \n",
       "22       -9842.842072      -12950.912498    -10915.745138     1146.668407  \n",
       "23       -3379.305378       -3303.051969     -3504.038353      192.700163  \n",
       "24       -4494.850431       -3669.313085     -3994.713535      294.523381  \n",
       "25       -3190.368330       -3191.369897     -3163.100829       60.845143  \n",
       "26       -3013.425760       -3020.469623     -3061.843238       60.946888  \n",
       "27       -3475.300469       -3324.982331     -3394.774988       59.653260  \n",
       "28       -4876.676804       -3797.748389     -3748.383019      597.275435  \n",
       "29       -4270.418417       -3808.838187     -4096.539796      260.515906  \n",
       "\n",
       "[30 rows x 86 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_mlp_adam_logprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__max_iter': 2000, 'regressor__fit_intercept': True, 'regressor__epsilon': 2.0, 'regressor__alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# best estimator usa o MAE que foi escolhido no refit da randomized search\n",
    "\n",
    "# y_prev_MAE = random_search.best_estimator_.predict(df_test)\n",
    "\n",
    "#--------------\n",
    "\n",
    "# MAS se quisermos escolher nós próprias o modelo, escolhemos os parâmetros e colocamos na pipeline diretamente:\n",
    "\n",
    "model_params = random_search_huber_standardscaler.cv_results_['params'][1]\n",
    "\n",
    "print(model_params)\n",
    "\n",
    "test_pipeline = pipeline_huber_standardscaler.set_params(**model_params)\n",
    "\n",
    "test_pipeline.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "y_pred = test_pipeline.predict(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "080750b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89856</td>\n",
       "      <td>12395.172688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106581</td>\n",
       "      <td>22889.303863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80886</td>\n",
       "      <td>16059.964318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100174</td>\n",
       "      <td>18685.122790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81376</td>\n",
       "      <td>21292.119165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32562</th>\n",
       "      <td>105775</td>\n",
       "      <td>17533.123908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32563</th>\n",
       "      <td>81363</td>\n",
       "      <td>32517.830227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32564</th>\n",
       "      <td>76833</td>\n",
       "      <td>30390.936661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32565</th>\n",
       "      <td>91768</td>\n",
       "      <td>20886.943567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32566</th>\n",
       "      <td>99627</td>\n",
       "      <td>13234.943995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32567 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        carID         price\n",
       "0       89856  12395.172688\n",
       "1      106581  22889.303863\n",
       "2       80886  16059.964318\n",
       "3      100174  18685.122790\n",
       "4       81376  21292.119165\n",
       "...       ...           ...\n",
       "32562  105775  17533.123908\n",
       "32563   81363  32517.830227\n",
       "32564   76833  30390.936661\n",
       "32565   91768  20886.943567\n",
       "32566   99627  13234.943995\n",
       "\n",
       "[32567 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.DataFrame({'carID' : df_test.index, 'price' : y_pred})\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18a05bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.to_csv(\"finetuning_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b590d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions_mlp = [{\n",
    "    'regressor__hidden_layer_sizes' : [(10,), (50,), (100,), (50,50), (100,50), (100,100)],\n",
    "    'regressor__max_iter' : [200, 500, 1000],\n",
    "    'regressor__activation' : ['relu', 'tanh', 'logistic', 'sigmoid'],\n",
    "    'regressor__solver' : ['adam'],\n",
    "    'regressor__learning_rate_init' : [0.001, 0.01, 0.1],\n",
    "    \n",
    "},\n",
    "{ \n",
    "    'regressor__hidden_layer_sizes' : [(10,), (50,), (100,), (50,50), (100,50), (100,100)],\n",
    "    'regressor__max_iter' : [200, 500, 1000],\n",
    "    'regressor__activation' : ['relu', 'tanh', 'logistic', 'sigmoid'],\n",
    "    'regressor__solver' : ['sgd'],\n",
    "    'regressor__learning_rate' :  ['constant','invscaling','adaptive'],\n",
    "    'regressor__learning_rate_init' : [0.001, 0.01, 0.1],\n",
    "    'regressor__batch_size' : [50, 100, 200]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b59445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a58393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(    \n",
    "    solver='adam',\n",
    "    hidden_layer_sizes=(600,400,200,100),\n",
    "    max_iter=1000,\n",
    "    activation='relu',\n",
    "    learning_rate_init=0.01,\n",
    "    random_state=random_state)\n",
    "\n",
    "\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('categorical treatment', Categorical_Correction()),  \n",
    "    ('outlier treatment', Outlier_Treatment()),                   \n",
    "    ('missing value treatment', Missing_Value_Treatment()),\n",
    "    ('typecasting', Typecasting()), \n",
    "    ('feature engineering', Feature_Engineering()), \n",
    "    ('encoder', Encoder() ), \n",
    "    ('scaler', Scaler()), #NO NEED FOR RANDOM FOREST REG\n",
    "    ('feature selection', Feature_Selection()),\n",
    "    ('regressor', TransformedTargetRegressor(regressor=mlp, transformer = StandardScaler()) )\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "530f70a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 1751.4008\n",
      "Validation MAE: 1753.6437\n",
      "Overfitting: -0.13%\n"
     ]
    }
   ],
   "source": [
    "pipeline_mlp.fit(X_train, y_train)\n",
    "y_train_pred = pipeline_mlp.predict(X_train)\n",
    "y_val_pred = pipeline_mlp.predict(X_val)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "overfit = (train_mae - val_mae) / train_mae * 100\n",
    "print(f\"Train MAE: {train_mae:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Overfitting: {overfit:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5af92",
   "metadata": {},
   "source": [
    "adam, logistic, LR = 0.1, size = (13,)\n",
    "n_iter = 342\n",
    "Train MAE: 1965.7185\n",
    "Validation MAE: 1935.6366\n",
    "\n",
    "\n",
    "adam, logistic, LR =0.01, size = (80,60)\n",
    "n_iter = 846\n",
    "Train MAE: 1516.8650\n",
    "Validation MAE: 1644.8813\n",
    "Overfitting: -8.44%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adam, relu, LR = (10,5)\n",
    "n_iter = 134\n",
    "Train MAE: 1960.8814\n",
    "Validation MAE: 1946.3452\n",
    "Overfitting: 0.74%\n",
    "\n",
    "adam, relu, LR = (400,200)\n",
    "n_iter = 135\n",
    "Train MAE: 1486.1078\n",
    "Validation MAE: 1626.7325\n",
    "Overfitting: -9.46%\n",
    "melhores resultados para lr = 0.01 , (600,400,200) sem overfit\n",
    "\n",
    "\n",
    "\n",
    "adam, tanh, LR = 0.01\n",
    "n_iter = 562\n",
    "Train MAE: 1607.2187\n",
    "Validation MAE: 1725.7870\n",
    "Overfitting: -7.38%\n",
    "\n",
    "\n",
    "\n",
    "sgd, logistic, 400,200, LR = 0.1, adaptive, batch = 500\n",
    "Train MAE: 1709.2573\n",
    "Validation MAE: 1722.8373\n",
    "Overfitting: -0.79%\n",
    "bons resultados para batch = 100, LR = 0.1, (400,200,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f43d0740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_mlp.named_steps['regressor'].regressor_.n_iter_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbe29e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "losses = pipeline_mlp.named_steps['regressor'].regressor_.loss_curve_\n",
    "iterations = range(pipeline_mlp.named_steps['regressor'].regressor_.n_iter_)\n",
    "sns.lineplot(x = iterations, y = losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
